description,url,name,task_code,category,url_doc,input_type,bp_workshop_name,bp_workshop_description
Sparse to Sparse transform for polynomial interactions,no url,Sparse to Sparse transform for polynomial interactions,SPOLY,Numeric Preprocessing,no documentation retrieved,NUM,Sparse Interaction Machine,Sparse to Sparse transform for polynomial interactions
Bin numerical values into non-uniform bins using decision trees,no url,Bin numerical values into non-uniform bins using decision trees,BINNING,Numeric Preprocessing,no documentation retrieved,NUM,Binning of numerical variables,Bin numerical values into non-uniform bins using decision trees
One-Hot (or dummy-variable) transformation of categorical features,no url,One-Hot (or dummy-variable) transformation of categorical features,PDM3,Numeric Preprocessing,no documentation retrieved,NUM,One-Hot Encoding,One-Hot (or dummy-variable) transformation of categorical features
"Median Value-Based Numeric Imputation (V2 with quick median algorithm)
    
    For a numeric feature, impute rows of missing values with median value (V2).
    
    Impute missing values on numeric variables with their median and create
    indicator variables to identify records that were imputed. A quick median algorithm (based on
    np.partition) is implemented to compute median feature value.
    
    **Imputation strategy:**
    
    A numeric feature is imputed with the median value if there are enough finite values in the feature
    samples used to train a numeric imputation task (e.g., `> t`, default: 50) and there are rows with
    NaN or infinite values in the samples to be imputed.
    
    After imputation, the imputed numeric features will be scaled if the argument `S` is set to
    True. The feature will use scaled rounding (i.e., rounding to a logarithmic scale).
    
    **Imputation indicator:**
    
    The indicator column (0, 1) is added to indicate imputed rows if the numeric feature is imputed with
    : 1) the median value and with at least one row with NaN and 2) at least two unique values.
    
    **Example:**
    
    An imputation task is initialized with t=2.
    
    Input numeric features of this task:
    
    feature0, feature1, feature2, feature3
    
    1, 2, np.nan, np.nan
    
    2, 3, np.nan, 18
    
    3, 2, np.nan, 16
    
    4, 1, 13, 14
    
    20, 1, 45, 46
    
    Output numeric features of this task:
    
    feature0, feature1, feature2, feature2-mi, feature3, feature3-mi
    
    1, 2, 45, 1, 18, 1
    
    2, 3, 45, 1, 18, 0
    
    3, 2, 45, 1, 16, 0
    
    4, 1, 13, 0, 14, 0
    
    20, 1, 45, 0, 46, 0
    
    In the imputation output, median value imputation is run on feature2 and feature3. The feature2-mi
    is the indicator column for the imputation on feature2. The feature3-mi is the indicator column for
    the imputation on feature3.
    
    Parameters
    ----------
    threshold (t): int (default='50')
        Minimum number of required finite elements in a column
        to impute the data onto NaNs and INFs.
        ``values: [1, 99999]``
    scale_small (s): bool (default='False')
        True if small values (range of the numeric variable is <= 1) are to be scaled.
        ``values: [False, True]``
    
    References
    ----------
    .. [1] [1] Acuna, Edgar, and Caroline Rodriguez.
       ""The treatment of missing values and its effect on classifier accuracy.""
       Classification, Clustering, and Data Mining Applications.
       Springer Berlin Heidelberg, 2004. 639-647.
       `[link]
       <https://link.springer.com/chapter/10.1007/978-3-642-17103-1_60>`__
    .. [2] [2] Feelders, Ad.
       ""Handling missing data in trees: Surrogate splits or statistical imputation?""
       Principles of Data Mining and Knowledge Discovery. Springer Berlin Heidelberg, 1999. 329-334.
       `[link]
       <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.7991&rep=rep1&type=pdf>`__
    
    See Also
    --------",no url,Median Value-Based Numeric Imputation (V2 with quick median algorithm),PNI2,Numeric Preprocessing,no documentation retrieved,NUM,Missing Values Imputed (quick median),Impute missing values on numeric variables with their median and create indicator variables to mark imputed records 
Greedy Search for ratios.  Adds pairs of ratios to a linear model until the model stops improving.  Then adds those ratios to the main dataset.,no url,Greedy Search for ratios,RATIO3,Numeric Preprocessing,no documentation retrieved,NUM,Search for ratios,Greedy Search for ratios.  Adds pairs of ratios to a linear model until the model stops improving.  Then adds those ratios to the main dataset.
Normalize features by scaling samples individually to unit norm. Based on scikit-learn Normalize,https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer,Normalize features by scaling samples individually to unit norm,NORM,Numeric Preprocessing,"













Normalizer — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.preprocessing
Normalizer









Normalizer#


class sklearn.preprocessing.Normalizer(norm='l2', *, copy=True)[source]#
Normalize samples individually to unit norm.
Each sample (i.e. each row of the data matrix) with at least one
non zero component is rescaled independently of other samples so
that its norm (l1, l2 or inf) equals one.
This transformer is able to work both with dense numpy arrays and
scipy.sparse matrix (use CSR format if you want to avoid the burden of
a copy / conversion).
Scaling inputs to unit norms is a common operation for text
classification or clustering for instance. For instance the dot
product of two l2-normalized TF-IDF vectors is the cosine similarity
of the vectors and is the base similarity metric for the Vector
Space Model commonly used by the Information Retrieval community.
For an example visualization, refer to Compare Normalizer with other
scalers.
Read more in the User Guide.

Parameters:

norm{‘l1’, ‘l2’, ‘max’}, default=’l2’The norm to use to normalize each non zero sample. If norm=’max’
is used, values will be rescaled by the maximum of the absolute
values.

copybool, default=TrueSet to False to perform inplace row normalization and avoid a
copy (if the input is already a numpy array or a scipy.sparse
CSR matrix).



Attributes:

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

normalizeEquivalent function without the estimator API.



Notes
This estimator is stateless and does not need to be fitted.
However, we recommend to call fit_transform instead of
transform, as parameter validation is only performed in
fit.
Examples
>>> from sklearn.preprocessing import Normalizer
>>> X = [[4, 1, 2, 2],
...      [1, 3, 9, 3],
...      [5, 7, 5, 1]]
>>> transformer = Normalizer().fit(X)  # fit does nothing.
>>> transformer
Normalizer()
>>> transformer.transform(X)
array([[0.8, 0.2, 0.4, 0.4],
       [0.1, 0.3, 0.9, 0.3],
       [0.5, 0.7, 0.5, 0.1]])




fit(X, y=None)[source]#
Only validates estimator’s parameters.
This method allows to: (i) validate the estimator’s parameters and
(ii) be consistent with the scikit-learn transformer API.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data to estimate the normalization parameters.

yIgnoredNot used, present here for API consistency by convention.



Returns:

selfobjectFitted transformer.







fit_transform(X, y=None, **fit_params)[source]#
Fit to data, then transform it.
Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.

Parameters:

Xarray-like of shape (n_samples, n_features)Input samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs),                 default=NoneTarget values (None for unsupervised transformations).

**fit_paramsdictAdditional fit parameters.



Returns:

X_newndarray array of shape (n_samples, n_features_new)Transformed array.







get_feature_names_out(input_features=None)[source]#
Get output feature names for transformation.

Parameters:

input_featuresarray-like of str or None, default=NoneInput features.

If input_features is None, then feature_names_in_ is
used as feature names in. If feature_names_in_ is not defined,
then the following input feature names are generated:
[""x0"", ""x1"", ..., ""x(n_features_in_ - 1)""].
If input_features is an array-like, then input_features must
match feature_names_in_ if feature_names_in_ is defined.




Returns:

feature_names_outndarray of str objectsSame as input features.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







set_output(*, transform=None)[source]#
Set output container.
See Introducing the set_output API
for an example on how to use the API.

Parameters:

transform{“default”, “pandas”, “polars”}, default=NoneConfigure output of transform and fit_transform.

""default"": Default output format of a transformer
""pandas"": DataFrame output
""polars"": Polars output
None: Transform configuration is unchanged


Added in version 1.4: ""polars"" option was added.




Returns:

selfestimator instanceEstimator instance.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_transform_request(*, copy: bool | None | str = '$UNCHANGED$') → Normalizer[source]#
Request metadata passed to the transform method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to transform if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to transform.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

copystr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for copy parameter in transform.



Returns:

selfobjectThe updated object.







transform(X, copy=None)[source]#
Scale each non zero row of X to unit norm.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data to normalize, row by row. scipy.sparse matrices should be
in CSR format to avoid an un-necessary copy.

copybool, default=NoneCopy the input X or not.



Returns:

X_tr{ndarray, sparse matrix} of shape (n_samples, n_features)Transformed array.







Gallery examples#

Scalable learning with polynomial kernel approximation
Scalable learning with polynomial kernel approximation

Compare the effect of different scalers on data with outliers
Compare the effect of different scalers on data with outliers

Clustering text documents using k-means
Clustering text documents using k-means










previous
MultiLabelBinarizer




next
OneHotEncoder










 On this page
  


Normalizer
fit
fit_transform
get_feature_names_out
get_metadata_routing
get_params
set_output
set_params
set_transform_request
transform


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Normalizer,Normalize features by scaling samples individually to unit norm. Based on scikit-learn Normalize
Convert numeric features into piece-wise constant spline base expansion. Missing values are inputed with the median prior to creating splines,no url,Convert numeric features into piece-wise constant spline base expansion,GS,Numeric Preprocessing,no documentation retrieved,NUM,Constant Splines,Convert numeric features into piece-wise constant spline base expansion. Missing values are inputed with the median prior to creating splines
"Greedy Search for differences between pairs of features, using a proprietary DataRobot-developed methodology",no url,"Greedy Search for differences between pairs of features, using a proprietary DataRobot-developed methodology",DIFF3,Numeric Preprocessing,no documentation retrieved,NUM,Search for differences,"Greedy Search for differences between pairs of features, using a proprietary DataRobot-developed methodology"
Impute missing/disguised missing values on numeric variables with their median and create indicator variables to mark records with data quality issues,no url,Impute missing/disguised missing values on numeric variables with their median and create indicator variables to mark records with data quality issues,NDC,Data Quality,no documentation retrieved,NUM,Numeric Data Cleansing,Impute missing/disguised missing values on numeric variables with their median and create indicator variables to mark records with data quality issues
"Arbitrary or median value-based numeric imputation (V4 with memory usage optimization)
    
    For a numeric feature, impute rows of missing values with an arbitrary (default: -9999) or
    median value. It also outputs the extra features (0, 1) indicating imputed rows.
    
    This is effective for tree-based models, as they can learn a split between the arbitrary value
    (-9999) and the rest of the data (which ideally will not overlap this value). More advanced
    tree-based models usually use a method called ""surrogate splits."" For models that don't support this
    method, arbitrary-value imputation is a method that yields very similar results.
    
    **Imputation strategy:**
    
    A numeric feature is imputed with the arbitrary value (default: -9999) if it:
    
    - has enough rows with finite values (e.g., `> t`, default: 10).
    
    - has large number of rows with NaN (e.g., `> min_cna`, default: 5).
    
    - is not configured as a monotonic-constrained feature.
    
    Other numeric features will be imputed with the median value, if necessary.
    After imputation, the imputed numeric features will be scaled if the argument `S` is set to
    True.
    
    **Imputation indicator:**
    
    The indicator column (0, 1) is added to indicate imputed rows if the numeric feature:
    
    - is imputed with the median value.
    
    - has at least one row with nan.
    
    - has at least one unique value.
    
    **Example:**
    
    An imputation task is initialized with t=2 and min_cna=2.
    
    Input numeric features of this task:
    
    feature0,feature1,feature2,feature3
    
    1.0, 2.0, NaN, NaN,
    
    2.0, 3.0, NaN, 18.0
    
    3.0, 2.0, NaN, 16.0
    
    4.0, 1.0, NaN, 14.0
    
    5.0, 4.0, 2.0, 15.0
    
    20.0, 1.0, 45.0, 46.0
    
    Output numeric features of this task:
    
    feature0, feature1, feature2, feature3, feature3-mi
    
    1.0, 2.0, -9999.0, 16.0, 1.0
    
    2.0, 3.0, -9999.0, 18.0, 0.0
    
    3.0, 2.0, -9999.0, 16.0, 0.0
    
    4.0, 1.0, -9999.0, 14.0, 0.0
    
    5.0, 4.0, 2.0, 15.0, 0.0
    
    20.0, 1.0, 45.0, 46.0, 0.0
    
    - Arbitrary value imputation is run on the feature 2 (num of nan rows >= min_cna).
    
    - Median value imputation is run on the feature 3.
    
    - The feature3-mi is the indicator column for the imputation on feature 3.
    
    **Implementation**
        - Uses custom numpy logic
    
    Parameters
    ----------
    threshold (t): int (default='10')
        Minimum number of finite elements required in a column for it to be considered for imputation
        (with arbitrary or median value)
        ``values: [0, 99999]``
    scale_small (s): bool (default='False')
        True if small values (the numeric variable is in a range of (0, 0.1]) are to be scaled
        ``values: [False, True]``
    arbimp (m): int (default='-9999')
        Value to be used for imputing
        ``values: [-99999, 99999]``
    min_count_na (min_cna): int (default='5')
        Minimum number of missing values required for arbitrary imputation
        ``values: [0, 99999]``
    mono_up (mono_up): string
        ID of the featurelist specifying the set of features to apply as monotonically increasing
        in relation to the target
    mono_down (mono_down): string
        ID of the featurelist specifying the set of features to apply as monotonically decreasing
        in relation to the target
    
    References
    ----------
    .. [1] [1] Feelders, Ad.
       ""Handling missing data in trees: Surrogate splits or statistical imputation?""
       Principles of Data Mining and Knowledge Discovery. Springer Berlin Heidelberg, 1999. 329-334.
       `[link]
       <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.7991&rep=rep1&type=pdf>`__
    
    See Also
    --------",no url,Arbitrary or median value-based numeric imputation (V4 with memory usage optimization),PNIA4,Data Quality,no documentation retrieved,NUM,Missing Values Imputed (arbitrary or quick median),Impute missing values on numeric variables with arbitrary number
Truncated Singular Value Decomposition,http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD,Truncated Singular Value Decomposition,SVD2,Dimensionality Reducer,"













TruncatedSVD — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.decomposition
TruncatedSVD









TruncatedSVD#


class sklearn.decomposition.TruncatedSVD(n_components=2, *, algorithm='randomized', n_iter=5, n_oversamples=10, power_iteration_normalizer='auto', random_state=None, tol=0.0)[source]#
Dimensionality reduction using truncated SVD (aka LSA).
This transformer performs linear dimensionality reduction by means of
truncated singular value decomposition (SVD). Contrary to PCA, this
estimator does not center the data before computing the singular value
decomposition. This means it can work with sparse matrices
efficiently.
In particular, truncated SVD works on term count/tf-idf matrices as
returned by the vectorizers in sklearn.feature_extraction.text. In
that context, it is known as latent semantic analysis (LSA).
This estimator supports two algorithms: a fast randomized SVD solver, and
a “naive” algorithm that uses ARPACK as an eigensolver on X * X.T or
X.T * X, whichever is more efficient.
Read more in the User Guide.

Parameters:

n_componentsint, default=2Desired dimensionality of output data.
If algorithm=’arpack’, must be strictly less than the number of features.
If algorithm=’randomized’, must be less than or equal to the number of features.
The default value is useful for visualisation. For LSA, a value of
100 is recommended.

algorithm{‘arpack’, ‘randomized’}, default=’randomized’SVD solver to use. Either “arpack” for the ARPACK wrapper in SciPy
(scipy.sparse.linalg.svds), or “randomized” for the randomized
algorithm due to Halko (2009).

n_iterint, default=5Number of iterations for randomized SVD solver. Not used by ARPACK. The
default is larger than the default in
randomized_svd to handle sparse
matrices that may have large slowly decaying spectrum.

n_oversamplesint, default=10Number of oversamples for randomized SVD solver. Not used by ARPACK.
See randomized_svd for a complete
description.

Added in version 1.1.


power_iteration_normalizer{‘auto’, ‘QR’, ‘LU’, ‘none’}, default=’auto’Power iteration normalizer for randomized SVD solver.
Not used by ARPACK. See randomized_svd
for more details.

Added in version 1.1.


random_stateint, RandomState instance or None, default=NoneUsed during randomized svd. Pass an int for reproducible results across
multiple function calls.
See Glossary.

tolfloat, default=0.0Tolerance for ARPACK. 0 means machine precision. Ignored by randomized
SVD solver.



Attributes:

components_ndarray of shape (n_components, n_features)The right singular vectors of the input data.

explained_variance_ndarray of shape (n_components,)The variance of the training samples transformed by a projection to
each component.

explained_variance_ratio_ndarray of shape (n_components,)Percentage of variance explained by each of the selected components.

singular_values_ndarray of shape (n_components,)The singular values corresponding to each of the selected components.
The singular values are equal to the 2-norms of the n_components
variables in the lower-dimensional space.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

DictionaryLearningFind a dictionary that sparsely encodes data.

FactorAnalysisA simple linear generative model with Gaussian latent variables.

IncrementalPCAIncremental principal components analysis.

KernelPCAKernel Principal component analysis.

NMFNon-Negative Matrix Factorization.

PCAPrincipal component analysis.



Notes
SVD suffers from a problem called “sign indeterminacy”, which means the
sign of the components_ and the output from transform depend on the
algorithm and random state. To work around this, fit instances of this
class to data once, then keep the instance around to do transformations.
References
Halko, et al. (2009). “Finding structure with randomness:
Stochastic algorithms for constructing approximate matrix decompositions”
Examples
>>> from sklearn.decomposition import TruncatedSVD
>>> from scipy.sparse import csr_matrix
>>> import numpy as np
>>> np.random.seed(0)
>>> X_dense = np.random.rand(100, 100)
>>> X_dense[:, 2 * np.arange(50)] = 0
>>> X = csr_matrix(X_dense)
>>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)
>>> svd.fit(X)
TruncatedSVD(n_components=5, n_iter=7, random_state=42)
>>> print(svd.explained_variance_ratio_)
[0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]
>>> print(svd.explained_variance_ratio_.sum())
0.2102...
>>> print(svd.singular_values_)
[35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]




fit(X, y=None)[source]#
Fit model on training data X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training data.

yIgnoredNot used, present here for API consistency by convention.



Returns:

selfobjectReturns the transformer object.







fit_transform(X, y=None)[source]#
Fit model to X and perform dimensionality reduction on X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training data.

yIgnoredNot used, present here for API consistency by convention.



Returns:

X_newndarray of shape (n_samples, n_components)Reduced version of X. This will always be a dense array.







get_feature_names_out(input_features=None)[source]#
Get output feature names for transformation.
The feature names out will prefixed by the lowercased class name. For
example, if the transformer outputs 3 features, then the feature names
out are: [""class_name0"", ""class_name1"", ""class_name2""].

Parameters:

input_featuresarray-like of str or None, default=NoneOnly used to validate feature names with the names seen in fit.



Returns:

feature_names_outndarray of str objectsTransformed feature names.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







inverse_transform(X)[source]#
Transform X back to its original space.
Returns an array X_original whose transform would be X.

Parameters:

Xarray-like of shape (n_samples, n_components)New data.



Returns:

X_originalndarray of shape (n_samples, n_features)Note that this is always a dense array.







set_output(*, transform=None)[source]#
Set output container.
See Introducing the set_output API
for an example on how to use the API.

Parameters:

transform{“default”, “pandas”, “polars”}, default=NoneConfigure output of transform and fit_transform.

""default"": Default output format of a transformer
""pandas"": DataFrame output
""polars"": Polars output
None: Transform configuration is unchanged


Added in version 1.4: ""polars"" option was added.




Returns:

selfestimator instanceEstimator instance.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







transform(X)[source]#
Perform dimensionality reduction on X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)New data.



Returns:

X_newndarray of shape (n_samples, n_components)Reduced version of X. This will always be a dense array.







Gallery examples#

Hashing feature transformation using Totally Random Trees
Hashing feature transformation using Totally Random Trees

Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…
Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...

Clustering text documents using k-means
Clustering text documents using k-means










previous
SparsePCA




next
dict_learning










 On this page
  


TruncatedSVD
fit
fit_transform
get_feature_names_out
get_metadata_routing
get_params
inverse_transform
set_output
set_params
transform


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Truncated Singular Value Decomposition,Truncated Singular Value Decomposition
Truncated Singular Value Decomposition,http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD,Truncated Singular Value Decomposition,SVD,Dimensionality Reducer,"













TruncatedSVD — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.decomposition
TruncatedSVD









TruncatedSVD#


class sklearn.decomposition.TruncatedSVD(n_components=2, *, algorithm='randomized', n_iter=5, n_oversamples=10, power_iteration_normalizer='auto', random_state=None, tol=0.0)[source]#
Dimensionality reduction using truncated SVD (aka LSA).
This transformer performs linear dimensionality reduction by means of
truncated singular value decomposition (SVD). Contrary to PCA, this
estimator does not center the data before computing the singular value
decomposition. This means it can work with sparse matrices
efficiently.
In particular, truncated SVD works on term count/tf-idf matrices as
returned by the vectorizers in sklearn.feature_extraction.text. In
that context, it is known as latent semantic analysis (LSA).
This estimator supports two algorithms: a fast randomized SVD solver, and
a “naive” algorithm that uses ARPACK as an eigensolver on X * X.T or
X.T * X, whichever is more efficient.
Read more in the User Guide.

Parameters:

n_componentsint, default=2Desired dimensionality of output data.
If algorithm=’arpack’, must be strictly less than the number of features.
If algorithm=’randomized’, must be less than or equal to the number of features.
The default value is useful for visualisation. For LSA, a value of
100 is recommended.

algorithm{‘arpack’, ‘randomized’}, default=’randomized’SVD solver to use. Either “arpack” for the ARPACK wrapper in SciPy
(scipy.sparse.linalg.svds), or “randomized” for the randomized
algorithm due to Halko (2009).

n_iterint, default=5Number of iterations for randomized SVD solver. Not used by ARPACK. The
default is larger than the default in
randomized_svd to handle sparse
matrices that may have large slowly decaying spectrum.

n_oversamplesint, default=10Number of oversamples for randomized SVD solver. Not used by ARPACK.
See randomized_svd for a complete
description.

Added in version 1.1.


power_iteration_normalizer{‘auto’, ‘QR’, ‘LU’, ‘none’}, default=’auto’Power iteration normalizer for randomized SVD solver.
Not used by ARPACK. See randomized_svd
for more details.

Added in version 1.1.


random_stateint, RandomState instance or None, default=NoneUsed during randomized svd. Pass an int for reproducible results across
multiple function calls.
See Glossary.

tolfloat, default=0.0Tolerance for ARPACK. 0 means machine precision. Ignored by randomized
SVD solver.



Attributes:

components_ndarray of shape (n_components, n_features)The right singular vectors of the input data.

explained_variance_ndarray of shape (n_components,)The variance of the training samples transformed by a projection to
each component.

explained_variance_ratio_ndarray of shape (n_components,)Percentage of variance explained by each of the selected components.

singular_values_ndarray of shape (n_components,)The singular values corresponding to each of the selected components.
The singular values are equal to the 2-norms of the n_components
variables in the lower-dimensional space.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

DictionaryLearningFind a dictionary that sparsely encodes data.

FactorAnalysisA simple linear generative model with Gaussian latent variables.

IncrementalPCAIncremental principal components analysis.

KernelPCAKernel Principal component analysis.

NMFNon-Negative Matrix Factorization.

PCAPrincipal component analysis.



Notes
SVD suffers from a problem called “sign indeterminacy”, which means the
sign of the components_ and the output from transform depend on the
algorithm and random state. To work around this, fit instances of this
class to data once, then keep the instance around to do transformations.
References
Halko, et al. (2009). “Finding structure with randomness:
Stochastic algorithms for constructing approximate matrix decompositions”
Examples
>>> from sklearn.decomposition import TruncatedSVD
>>> from scipy.sparse import csr_matrix
>>> import numpy as np
>>> np.random.seed(0)
>>> X_dense = np.random.rand(100, 100)
>>> X_dense[:, 2 * np.arange(50)] = 0
>>> X = csr_matrix(X_dense)
>>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)
>>> svd.fit(X)
TruncatedSVD(n_components=5, n_iter=7, random_state=42)
>>> print(svd.explained_variance_ratio_)
[0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]
>>> print(svd.explained_variance_ratio_.sum())
0.2102...
>>> print(svd.singular_values_)
[35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]




fit(X, y=None)[source]#
Fit model on training data X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training data.

yIgnoredNot used, present here for API consistency by convention.



Returns:

selfobjectReturns the transformer object.







fit_transform(X, y=None)[source]#
Fit model to X and perform dimensionality reduction on X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training data.

yIgnoredNot used, present here for API consistency by convention.



Returns:

X_newndarray of shape (n_samples, n_components)Reduced version of X. This will always be a dense array.







get_feature_names_out(input_features=None)[source]#
Get output feature names for transformation.
The feature names out will prefixed by the lowercased class name. For
example, if the transformer outputs 3 features, then the feature names
out are: [""class_name0"", ""class_name1"", ""class_name2""].

Parameters:

input_featuresarray-like of str or None, default=NoneOnly used to validate feature names with the names seen in fit.



Returns:

feature_names_outndarray of str objectsTransformed feature names.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







inverse_transform(X)[source]#
Transform X back to its original space.
Returns an array X_original whose transform would be X.

Parameters:

Xarray-like of shape (n_samples, n_components)New data.



Returns:

X_originalndarray of shape (n_samples, n_features)Note that this is always a dense array.







set_output(*, transform=None)[source]#
Set output container.
See Introducing the set_output API
for an example on how to use the API.

Parameters:

transform{“default”, “pandas”, “polars”}, default=NoneConfigure output of transform and fit_transform.

""default"": Default output format of a transformer
""pandas"": DataFrame output
""polars"": Polars output
None: Transform configuration is unchanged


Added in version 1.4: ""polars"" option was added.




Returns:

selfestimator instanceEstimator instance.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







transform(X)[source]#
Perform dimensionality reduction on X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)New data.



Returns:

X_newndarray of shape (n_samples, n_components)Reduced version of X. This will always be a dense array.







Gallery examples#

Hashing feature transformation using Totally Random Trees
Hashing feature transformation using Totally Random Trees

Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…
Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...

Clustering text documents using k-means
Clustering text documents using k-means










previous
SparsePCA




next
dict_learning










 On this page
  


TruncatedSVD
fit
fit_transform
get_feature_names_out
get_metadata_routing
get_params
inverse_transform
set_output
set_params
transform


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Truncated Singular Value Decomposition,Truncated Singular Value Decomposition
Principal component analysis used for dimensionality reduction by keeping only the most significant singular vectors,no url,Principal component analysis used for dimensionality reduction by keeping only the most significant singular vectors,PPCA,Dimensionality Reducer,no documentation retrieved,NUM,Partial Principal Components Analysis,Principal component analysis used for dimensionality reduction by keeping only the most significant singular vectors
"Standardize task based on mean removal and scaling to standard deviation
    
    Standardize features by removing the mean and scaling to unit standard deviation. It is based on
    scikit-learn estimator **sklearn.preprocessing.StandardScaler**.
    
    Centering and scaling happen independently on each feature based on
    relevant statistics from samples in the training set.
    
    Standardization of a dataset is a common requirement for many
    machine learning estimators. These estimators might behave badly if the
    distribution of individual feature does not align with standard,
    normally distributed data.
    
    For instance, many elements used in the objective function of a
    learning algorithm (such as the RBF kernel of Support Vector Machines
    or the L1 and L2 regularizers of linear models) assume that all
    features are centered around 0 and have variance in the same order.
    If variance of one feature has a larger order of magnitude than others,
    it might dominate the objective function and make the estimator
    unable to learn from other features correctly as expected.
    
    Parameters
    ----------
    force_mean (fm): bool (default='False')
        When True, sparse input features are converted to arrays.
        ``values: [False, True]``
    
    References
    ----------
    .. [1] Marquardt, Donald W.
       ""Comment: You should standardize the predictor variables in your regression models.""
       Journal of the American Statistical Association 75.369 (1980): 87-91.
       `[link]
       <https://doi.org/10.1080/01621459.1980.10477430>`__
    
    See Also
    --------
    Source:
        `sklearn preprocessing StandardScaler
        <http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html>`_
    
    Source:
        `sklearn StandardScaler user guide
        <http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler>`_",no url,Standardize task based on mean removal and scaling to standard deviation,ST,Numeric Preprocessing,no documentation retrieved,NUM,Standardize,Standardize features by removing the mean and scaling to unit variance. Based on scikit-learn StandardScaler
Transform on the link function scale.,no url,Transform on the link function scale,LINK,Numeric Preprocessing,no documentation retrieved,NUM,Transform on the link function scale,Transform on the link function scale.
"Greedy Search for useful transformations of the input variables, using a proprietary DataRobot-developed methodology",no url,"Greedy Search for useful transformations of the input variables, using a proprietary DataRobot-developed methodology",BTRANSF6T,Numeric Preprocessing,no documentation retrieved,NUM,Transparent Search for best transformation,"Greedy Search for useful transformations of the input variables, using a proprietary DataRobot-developed methodology"
Log transform preprocessor.,no url,Log transform preprocessor,LOGT,Numeric Preprocessing,no documentation retrieved,NUM,Log Transformer,Log transform preprocessor.
"Greedy Search for useful transformations of the input variables, using a proprietary DataRobot-developed methodology",no url,"Greedy Search for useful transformations of the input variables, using a proprietary DataRobot-developed methodology",BTRANSF6,Numeric Preprocessing,no documentation retrieved,NUM,Search for best transformation including Smooth Ridit,"Greedy Search for useful transformations of the input variables, using a proprietary DataRobot-developed methodology"
"Standardize task based on median removal and scaling to standard deviation or mean absolute deviation
    
    Standardize features by removing the median and scaling to unit
    standard deviation or the mean absolute deviation.
    
    Centering and scaling happen independently on each feature based on
    relevant statistics from samples in the training set.
    
    Standardization of a dataset is a common requirement for many
    machine learning estimators. These estimators might behave badly if the
    distribution of individual feature does not align with standard,
    normally distributed data.
    
    For instance, many elements used in the objective function of a
    learning algorithm (such as the RBF kernel of Support Vector Machines
    or the L1 and L2 regularizers of linear models) assume that all
    features are centered around 0 and have variance in the same order.
    If variance of one feature has a larger order of magnitude than others,
    it might dominate the objective function and make the estimator
    unable to learn from other features correctly as expected.
    
    Parameters
    ----------
    scale_type (st): string (default='std')
        Type of scaling, either standard deviation (std, the default) or mean absolute deviation (mad).
        ``values: ['std','mad']``
    sparsity_threshold (st): float (default=0.25)
        Threshold of sparsity conversion. If sparsity level is higher than the threshold, the matrix is
        converted to a sparse format.
        ``values: [0, 1]``
    
    References
    ----------
    .. [1] Marquardt, Donald W.
       ""Comment: You should standardize the predictor variables in your regression models.""
       Journal of the American Statistical Association 75.369 (1980): 87-91.
       `[link]
       <https://doi.org/10.1080/01621459.1980.10477430>`__
    
    See Also
    --------",no url,Standardize task based on median removal and scaling to standard deviation or mean absolute deviation,RST,Numeric Preprocessing,no documentation retrieved,NUM,Standardize,Standardize features by removing the median and scaling to unit variance or mean absolute deviation.
"Ridit transformer
    
    For a numeric feature, transform it to a ridit score based on percentile rank. The percentile score
    will be further adjusted to an interval between -1 and 1. The transformer can be configured
    to skip binary feature and date/time derived features. If the sparsity is higher than the
    sparsity_threshold, data will be centered to the median and the output will be a sparse matrix.
    
    The ridit transform is an extension of Bross' (1958) RIDIT scoring method,
    which suggests the use of Ridit analysis for data that are ordered
    but not on an interval scale, such as injury categories.
    Bross' (1958) RIDIT's procedure is as follows: from a reference
    population with the same categories (of injury, for example), determine
    a ""ridit"" or score for each category. This category score
    is the percentile rank of an item in the reference population and
    is equal to the number of items in all lower categories plus
    one-half the number of items in the subject category, all divided by
    the population size. By definition, the mean of Bross ridit calculated for the
    reference population will always be 0.5.
    
    The ridit transform extends the Bross ridit method by applying the method
    to numerical values and normalizing the score such that the mean calculated
    for the reference population will always be 0 and the score will be
    in the interval [-1,1].
    
    Intuitively, the ridit transform can be interpreted to be an
    adjusted percentile score.
    
    Ridit transform is not smooth and variable mapping is not continuous at the bin boundaries.
    DataRobot developed a ""smooth"" version of Ridit mapping, where the mapping inside of each bin is set
    to linearly increase to reach the other bin's starting value. The middle point in the bin
    corresponds to the value of the original Ridit algorithm, and the mean of the distribution is still
    equal to 0.5. By using such approach, the mapping is continuous and predictions are consistent for
    the data values close to the boundary values.
    
    Parameters
    ----------
    sparsity_threshold (spt): float
        If sparsity level is higher than the parameter, the matrix is converted
        to a sparse format.
        ``values: [0, 1]``
    skip_bins (skb): bool
        If True, ridit transform will skip binary columns.
        ``values: [False,True]``
    skip_date_features (skd): bool
        If True, ridit transform will skip extracted features from the date column.
        ``values: [False,True]``
    
    References
    ----------
    .. [1] Bross, I. D. J. [1958].
       ""How to Use Ridit Analysis.""
       Biornetrics- 14, pg. 18-38.
       `[link]
       <https://doi.org/10.2307/2527727>`__
    .. [2] Brockett, Patrick L., and Arnold Levine.
       ""On a characterization of ridits.""
       The Annals of Statistics (1977): 1245-1248.
       `[link]
       <https://projecteuclid.org/download/pdf_1/euclid.aos/1176344010>`__
    
    See Also
    --------",no url,Ridit transformer,RDT5,Numeric Preprocessing,no documentation retrieved,NUM,Smooth Ridit Transform,Smooth Ridit transform: Convert all numeric and ordinal features into the same scale (values between -1 and 1) based on their cumulative smoothed empirical distribution.
Fit each feature to a Uniform distribution.,no url,Fit each feature to a Uniform distribution,UNIF3,Numeric Preprocessing,no documentation retrieved,NUM,Impose Uniform Transform,Fit each feature to a Uniform distribution.
Ridit transform: Convert all numeric and ordinal features into the same scale based on their cumulative empirical distribution.,no url,Ridit transform: Convert all numeric and ordinal features into the same scale based on their cumulative empirical distribution,SRDT3,Numeric Preprocessing,no documentation retrieved,NUM,Ridit Transform,Ridit transform: Convert all numeric and ordinal features into the same scale based on their cumulative empirical distribution.
"Buhlmann Credibility Estimates from categorical features with high cardinality.  This transformer calculates credibility estimates using a proprietary, DataRobot-developed methodology",no url,Buhlmann Credibility Estimates from categorical features with high cardinality,CRED1,Categorical Preprocessing,no documentation retrieved,CAT,Buhlmann credibility estimates for high cardinality features,"Buhlmann Credibility Estimates from categorical features with high cardinality.  This transformer calculates credibility estimates using a proprietary, DataRobot-developed methodology"
One-Hot (or dummy-variable) transformation of categorical features,no url,One-Hot (or dummy-variable) transformation of categorical features,PDM3,Categorical Preprocessing,no documentation retrieved,CAT,One-Hot Encoding,One-Hot (or dummy-variable) transformation of categorical features
Dense embedding of categorical features. Transform categorical features into a dense vector of a fixed size,no url,Dense embedding of categorical features,CATEMB,Categorical Preprocessing,no documentation retrieved,CAT,Categorical Embedding,Dense embedding of categorical features. Transform categorical features into a dense vector of a fixed size
L2 Regularization Credibility Estimator,no url,L2 Regularization Credibility Estimator,CRED1b1,Categorical Preprocessing,no documentation retrieved,CAT,Univariate credibility estimates with L2,L2 Regularization Credibility Estimator
Create a count matrix from categorical features,no url,Create a count matrix from categorical features,PCCAT,Categorical Preprocessing,no documentation retrieved,CAT,Category Count,Create a count matrix from categorical features
"Ordinal scale converter of categorical features
    
    For a categorical feature, convert categorical levels to an ordinal scale.
    The ordinal scale is 0 to (unique values of categorical_var) - 1.
    Rare categories (=other) and missing values are encoded as -1 and -2, respectively.
    Mapping is based on the lexicographic ordering of the categorical values,
    the frequency of the levels, the response, or is done randomly.
    
    Ordinal encoding is effective for tree-based models, as it usually performs as well as
    one-hot encoding but requires fewer computational resources (memory and cpu).
    
    Ordinal encoding, however, does not work for linear methods.
    
    Parameters
    ----------
    card_max (cmax) : multi (default=None)
      Maximum number of categorical feature levels allowed. If None, a feature
      with any number of levels is allowed.
      ``values: {'int': [1, 9999999], 'select': None}``
    min_support (ms): int (default=5)
      Minimum number of levels required for a category to be represented on the ordinal scale.
      If a category level count is below the minimum, it will be grouped with other small cardinality
      levels or encoded as a missing value, depending on the value of ``other_category``.
      ``values: [1, 99999]``
    other_category (o): bool (default=True)
      If True, small cardinality values are mapped to a dedicated value (-1), otherwise they are encoded
      as missing values (-2).
      ``values: [False, True]``
    random_scale (r): bool (default=True)
      Applies if ``method`` is ``None``. If ``random_scale`` is True, random ordering is used for the
      ordinal scale. If it is False, lexicographical ordering is used.
      ``values: [False, True]``
    seed (s): int (default='1234')
      The RNG seed.
      ``values: [0, 99999]``
    offset (os): int (default='0')
      Shift the ordinal scale of ordinal encoder
      ``values: [0, 99999]``
    method (m): select (default='None')
      Method used in the encoding. ``None``: uses random_scale. ``random``: random ordering of levels,
      ``lex``: lexicographical ordering by category level names, ``freq``: frequency ordering from
      least frequent to most frequent, ``resp``: response ordering.
      ``values: ['None', 'random', 'lex', 'freq', 'resp']``
    add_cols_metadata ('acm'): select (default=False)
      If specified, add -cols to metadata.
      ``values: [False, True]``
    add_maps_metadata ('amm'): select (default=False)
      If specified, add -maps to metadata.
      ``values: [False, True]``
    
    References
    ----------
    
    See Also
    --------",no url,Ordinal scale converter of categorical features,ORDCAT2,Categorical Preprocessing,no documentation retrieved,CAT,Ordinal encoding of categorical variables,"Ordinal transformation of categorical features. Recodes categorical features as integers based on either the lexicographic ordering of the categorical values, the frequency of the categorical values, the response or randomly"
Computes NLTK Sentiment for text features,no url,Computes NLTK Sentiment for text features,NLTK_SENTIMENT,Text Preprocessing,no documentation retrieved,TXT,NLTK Sentiment Featurizer,Computes NLTK Sentiment for text features
"Stemming method used to remove morphological affixes from words, leaving only the word stem.",no url,"Stemming method used to remove morphological affixes from words, leaving only the word stem",STEMMER,Text Preprocessing,no documentation retrieved,TXT,Stemming Featurizer,"Stemming method used to remove morphological affixes from words, leaving only the word stem."
Convert raw text fields into a vector. Based on Sentence-TinyBERT embeddings.,https://arxiv.org/abs/1909.10351,Convert raw text fields into a vector,SENTENCE_TINY_BERT,Text Preprocessing,"


 [1909.10351] TinyBERT: Distilling BERT for Natural Language Understanding





























  








Skip to main content





We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate





 > cs > arXiv:1909.10351
  





Help | Advanced Search




All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text




Search















open search






GO



open navigation menu


quick links

Login
Help Pages
About












Computer Science > Computation and Language


arXiv:1909.10351 (cs)
    




  [Submitted on 23 Sep 2019 (v1), last revised 16 Oct 2020 (this version, v5)]
Title:TinyBERT: Distilling BERT for Natural Language Understanding
Authors:Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu View a PDF of the paper titled TinyBERT: Distilling BERT for Natural Language Understanding, by Xiaoqi Jiao and 6 other authors
View PDF

Abstract:Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture he general-domain as well as the task-specific knowledge in BERT.
TinyBERT with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT with 4 layers is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only about 28% parameters and about 31% inference time of them. Moreover, TinyBERT with 6 layers performs on-par with its teacher BERTBASE.
    


 
Comments:
Findings of EMNLP 2020; results have been updated; code and model: this https URL


Subjects:

Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

Cite as:
arXiv:1909.10351 [cs.CL]


 
(or 
arXiv:1909.10351v5 [cs.CL] for this version)
          


 
 https://doi.org/10.48550/arXiv.1909.10351


Focus to learn more



                  arXiv-issued DOI via DataCite







Submission history From: Yichun Yin [view email]       [v1]
        Mon, 23 Sep 2019 13:05:35 UTC (1,272 KB)
[v2]
        Tue, 24 Sep 2019 12:39:36 UTC (1,274 KB)
[v3]
        Tue, 3 Dec 2019 01:29:39 UTC (3,110 KB)
[v4]
        Wed, 4 Dec 2019 01:50:34 UTC (3,110 KB)
[v5]
        Fri, 16 Oct 2020 02:12:46 UTC (875 KB)



 

Full-text links:
Access Paper:


View a PDF of the paper titled TinyBERT: Distilling BERT for Natural Language Understanding, by Xiaoqi Jiao and 6 other authorsView PDFTeX SourceOther Formats
view license

 
    Current browse context: cs.CL


< prev

  |   
next >


new
 | 
recent
 | 2019-09

    Change to browse by:
    
cs
cs.AI
cs.LG




References & Citations

NASA ADSGoogle Scholar
Semantic Scholar





 3 blog links (what is this?)
        


DBLP - CS Bibliography

listing | bibtex 

Yichun YinLifeng ShangXin JiangXiao ChenFang Wang …


a
export BibTeX citation
Loading...




BibTeX formatted citation
×


loading...


Data provided by: 




Bookmark





 




Bibliographic Tools

Bibliographic and Citation Tools






Bibliographic Explorer Toggle



Bibliographic Explorer (What is the Explorer?)







Connected Papers Toggle



Connected Papers (What is Connected Papers?)






Litmaps Toggle



Litmaps (What is Litmaps?)







scite.ai Toggle



scite Smart Citations (What are Smart Citations?)









Code, Data, Media

Code, Data and Media Associated with this Article






alphaXiv Toggle



alphaXiv (What is alphaXiv?)







Links to Code Toggle



CatalyzeX Code Finder for Papers (What is CatalyzeX?)







DagsHub Toggle



DagsHub (What is DagsHub?)







GotitPub Toggle



Gotit.pub (What is GotitPub?)







Huggingface Toggle



Hugging Face (What is Huggingface?)







Links to Code Toggle



Papers with Code (What is Papers with Code?)







ScienceCast Toggle



ScienceCast (What is ScienceCast?)













Demos

Demos






Replicate Toggle



Replicate (What is Replicate?)







Spaces Toggle



Hugging Face Spaces (What is Spaces?)







Spaces Toggle



TXYZ.AI (What is TXYZ.AI?)








Related Papers

Recommenders and Search Tools






Link to Influence Flower



Influence Flower (What are Influence Flowers?)







Core recommender toggle



CORE Recommender (What is CORE?)





Author
Venue
Institution
Topic













        About arXivLabs
      



arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.










Which authors of this paper are endorsers? |
    Disable MathJax (What is MathJax?)
    












About
Help





contact arXivClick here to contact arXiv
 Contact


subscribe to arXiv mailingsClick here to subscribe
 Subscribe











Copyright
Privacy Policy




Web Accessibility Assistance


arXiv Operational Status 
                    Get status notifications via
                    email
                    or slack





 






",TXT,Pretrained Sentence TinyBERT-Pruned Featurizer,Convert raw text fields into a vector. Based on Sentence-TinyBERT embeddings.
Computes Named Entity Recognition for text features,no url,Computes Named Entity Recognition for text features,SPACY_NAMED_ENTITY_RECOGNITION,Text Preprocessing,no documentation retrieved,TXT,SpaCy Named Entity Recognition Detector,Computes Named Entity Recognition for text features
Convert raw text fields into a vector. Based on Sentence Roberta embeddings.,no url,Convert raw text fields into a vector,SENTENCE_ROBERTA,Text Preprocessing,no documentation retrieved,TXT,Pretrained Sentence Roberta Featurizer,Convert raw text fields into a vector. Based on Sentence Roberta embeddings.
Text encoding based on Keras BPE Tokenizer class,https://pypi.org/project/bpemb/,Text encoding based on Keras BPE Tokenizer class,KERAS_BPE_TOKENIZER_V2,Text Preprocessing,"








bpemb · PyPI










 

















Skip to main content

Switch to mobile version    







Warning

Some features may not work without JavaScript. Please try enabling it if you encounter problems.


 












Search PyPI



Search



 


Help
Sponsors
Log in
Register




Menu      




Help
Sponsors
Log in
Register



 




Search PyPI



Search








        bpemb 0.3.6
      


pip install bpemb


Copy PIP instructions






Latest version


Released: 
  Oct 1, 2024
 





 
Byte-pair embeddings in 275 languages
 







Navigation





Project description                




Release history                




Download files                






Verified details    

These details have been verified by PyPI
Maintainers






            noutenki
          




Unverified details
These details have not been verified by PyPI
Project links



Homepage
      



Meta



License: MIT License (MIT)
      



Author: Benjamin Heinzerling





Classifiers


License



            OSI Approved :: MIT License
          




Operating System



            OS Independent
          




Programming Language



            Python :: 3
          















Project description              




Project details              




Release history            




Download files              




Project description

BPEmb
BPEmb is a collection of pre-trained subword embeddings in 275 languages, based on Byte-Pair Encoding (BPE) and trained on Wikipedia. Its intended use is as input for neural models in natural language processing.
Website ・
Usage ・
Download ・
MultiBPEmb ・
Paper (pdf) ・
Citing BPEmb
Usage
Install BPEmb with pip:
pip install bpemb

Embeddings and SentencePiece models will be downloaded automatically the first time you use them.
>>> from bpemb import BPEmb
# load English BPEmb model with default vocabulary size (10k) and 50-dimensional embeddings
>>> bpemb_en = BPEmb(lang=""en"", dim=50)
downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model
downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d50.w2v.bin.tar.gz

You can do two main things with BPEmb. The first is subword segmentation:
# apply English BPE subword segmentation model
>>> bpemb_en.encode(""Stratford"")
['▁strat', 'ford']
# load Chinese BPEmb model with vocabulary size 100k and default (100-dim) embeddings
>>> bpemb_zh = BPEmb(lang=""zh"", vs=100000)
# apply Chinese BPE subword segmentation model
>>> bpemb_zh.encode(""这是一个中文句子"")  # ""This is a Chinese sentence.""
['▁这是一个', '中文', '句子']  # [""This is a"", ""Chinese"", ""sentence""]

If / how a word gets split depends on the vocabulary size. Generally, a smaller vocabulary size will yield a segmentation into many subwords, while a large vocabulary size will result in frequent words not being split:



vocabulary size
segmentation




1000
['▁str', 'at', 'f', 'ord']


3000
['▁str', 'at', 'ford']


5000
['▁str', 'at', 'ford']


10000
['▁strat', 'ford']


25000
['▁stratford']


50000
['▁stratford']


100000
['▁stratford']


200000
['▁stratford']



The second purpose of BPEmb is to provide pretrained subword embeddings:
# Embeddings are wrapped in a gensim KeyedVectors object
>>> type(bpemb_zh.emb)
gensim.models.keyedvectors.Word2VecKeyedVectors
# You can use BPEmb objects like gensim KeyedVectors
>>> bpemb_en.most_similar(""ford"")
[('bury', 0.8745079040527344),
 ('ton', 0.8725000619888306),
 ('well', 0.871537446975708),
 ('ston', 0.8701574206352234),
 ('worth', 0.8672043085098267),
 ('field', 0.859795331954956),
 ('ley', 0.8591548204421997),
 ('ington', 0.8126075267791748),
 ('bridge', 0.8099068999290466),
 ('brook', 0.7979353070259094)]
>>> type(bpemb_en.vectors)
numpy.ndarray
>>> bpemb_en.vectors.shape
(10000, 50)
>>> bpemb_zh.vectors.shape
(100000, 100)

To use subword embeddings in your neural network, either encode your input into subword IDs:
>>> ids = bpemb_zh.encode_ids(""这是一个中文句子"")
[25950, 695, 20199]
>>> bpemb_zh.vectors[ids].shape
(3, 100)

Or use the embed method:
# apply Chinese subword segmentation and perform embedding lookup
>>> bpemb_zh.embed(""这是一个中文句子"").shape
(3, 100)

Downloads for each language
ab (Abkhazian) ・
ace (Achinese) ・
ady (Adyghe) ・
af (Afrikaans) ・
ak (Akan) ・
als (Alemannic) ・
am (Amharic) ・
an (Aragonese) ・
ang (Old English) ・
ar (Arabic) ・
arc (Official Aramaic) ・
arz (Egyptian Arabic) ・
as (Assamese) ・
ast (Asturian) ・
atj (Atikamekw) ・
av (Avaric) ・
ay (Aymara) ・
az (Azerbaijani) ・
azb (South Azerbaijani)
ba (Bashkir) ・
bar (Bavarian) ・
bcl (Central Bikol) ・
be (Belarusian) ・
bg (Bulgarian) ・
bi (Bislama) ・
bjn (Banjar) ・
bm (Bambara) ・
bn (Bengali) ・
bo (Tibetan) ・
bpy (Bishnupriya) ・
br (Breton) ・
bs (Bosnian) ・
bug (Buginese) ・
bxr (Russia Buriat)
ca (Catalan) ・
cdo (Min Dong Chinese) ・
ce (Chechen) ・
ceb (Cebuano) ・
ch (Chamorro) ・
chr (Cherokee) ・
chy (Cheyenne) ・
ckb (Central Kurdish) ・
co (Corsican) ・
cr (Cree) ・
crh (Crimean Tatar) ・
cs (Czech) ・
csb (Kashubian) ・
cu (Church Slavic) ・
cv (Chuvash) ・
cy (Welsh)
da (Danish) ・
de (German) ・
din (Dinka) ・
diq (Dimli) ・
dsb (Lower Sorbian) ・
dty (Dotyali) ・
dv (Dhivehi) ・
dz (Dzongkha)
ee (Ewe) ・
el (Modern Greek) ・
en (English) ・
eo (Esperanto) ・
es (Spanish) ・
et (Estonian) ・
eu (Basque) ・
ext (Extremaduran)
fa (Persian) ・
ff (Fulah) ・
fi (Finnish) ・
fj (Fijian) ・
fo (Faroese) ・
fr (French) ・
frp (Arpitan) ・
frr (Northern Frisian) ・
fur (Friulian) ・
fy (Western Frisian)
ga (Irish) ・
gag (Gagauz) ・
gan (Gan Chinese) ・
gd (Scottish Gaelic) ・
gl (Galician) ・
glk (Gilaki) ・
gn (Guarani) ・
gom (Goan Konkani) ・
got (Gothic) ・
gu (Gujarati) ・
gv (Manx)
ha (Hausa) ・
hak (Hakka Chinese) ・
haw (Hawaiian) ・
he (Hebrew) ・
hi (Hindi) ・
hif (Fiji Hindi) ・
hr (Croatian) ・
hsb (Upper Sorbian) ・
ht (Haitian) ・
hu (Hungarian) ・
hy (Armenian)
ia (Interlingua) ・
id (Indonesian) ・
ie (Interlingue) ・
ig (Igbo) ・
ik (Inupiaq) ・
ilo (Iloko) ・
io (Ido) ・
is (Icelandic) ・
it (Italian) ・
iu (Inuktitut)
ja (Japanese) ・
jam (Jamaican Creole English) ・
jbo (Lojban) ・
jv (Javanese)
ka (Georgian) ・
kaa (Kara-Kalpak) ・
kab (Kabyle) ・
kbd (Kabardian) ・
kbp (Kabiyè) ・
kg (Kongo) ・
ki (Kikuyu) ・
kk (Kazakh) ・
kl (Kalaallisut) ・
km (Central Khmer) ・
kn (Kannada) ・
ko (Korean) ・
koi (Komi-Permyak) ・
krc (Karachay-Balkar) ・
ks (Kashmiri) ・
ksh (Kölsch) ・
ku (Kurdish) ・
kv (Komi) ・
kw (Cornish) ・
ky (Kirghiz)
la (Latin) ・
lad (Ladino) ・
lb (Luxembourgish) ・
lbe (Lak) ・
lez (Lezghian) ・
lg (Ganda) ・
li (Limburgan) ・
lij (Ligurian) ・
lmo (Lombard) ・
ln (Lingala) ・
lo (Lao) ・
lrc (Northern Luri) ・
lt (Lithuanian) ・
ltg (Latgalian) ・
lv (Latvian)
mai (Maithili) ・
mdf (Moksha) ・
mg (Malagasy) ・
mh (Marshallese) ・
mhr (Eastern Mari) ・
mi (Maori) ・
min (Minangkabau) ・
mk (Macedonian) ・
ml (Malayalam) ・
mn (Mongolian) ・
mr (Marathi) ・
mrj (Western Mari) ・
ms (Malay) ・
mt (Maltese) ・
mwl (Mirandese) ・
my (Burmese) ・
myv (Erzya) ・
mzn (Mazanderani)
na (Nauru) ・
nap (Neapolitan) ・
nds (Low German) ・
ne (Nepali) ・
new (Newari) ・
ng (Ndonga) ・
nl (Dutch) ・
nn (Norwegian Nynorsk) ・
no (Norwegian) ・
nov (Novial) ・
nrm (Narom) ・
nso (Pedi) ・
nv (Navajo) ・
ny (Nyanja)
oc (Occitan) ・
olo (Livvi) ・
om (Oromo) ・
or (Oriya) ・
os (Ossetian)
pa (Panjabi) ・
pag (Pangasinan) ・
pam (Pampanga) ・
pap (Papiamento) ・
pcd (Picard) ・
pdc (Pennsylvania German) ・
pfl (Pfaelzisch) ・
pi (Pali) ・
pih (Pitcairn-Norfolk) ・
pl (Polish) ・
pms (Piemontese) ・
pnb (Western Panjabi) ・
pnt (Pontic) ・
ps (Pushto) ・
pt (Portuguese)
qu (Quechua)
rm (Romansh) ・
rmy (Vlax Romani) ・
rn (Rundi) ・
ro (Romanian) ・
ru (Russian) ・
rue (Rusyn) ・
rw (Kinyarwanda)
sa (Sanskrit) ・
sah (Yakut) ・
sc (Sardinian) ・
scn (Sicilian) ・
sco (Scots) ・
sd (Sindhi) ・
se (Northern Sami) ・
sg (Sango) ・
sh (Serbo-Croatian) ・
si (Sinhala) ・
sk (Slovak) ・
sl (Slovenian) ・
sm (Samoan) ・
sn (Shona) ・
so (Somali) ・
sq (Albanian) ・
sr (Serbian) ・
srn (Sranan Tongo) ・
ss (Swati) ・
st (Southern Sotho) ・
stq (Saterfriesisch) ・
su (Sundanese) ・
sv (Swedish) ・
sw (Swahili) ・
szl (Silesian)
ta (Tamil) ・
tcy (Tulu) ・
te (Telugu) ・
tet (Tetum) ・
tg (Tajik) ・
th (Thai) ・
ti (Tigrinya) ・
tk (Turkmen) ・
tl (Tagalog) ・
tn (Tswana) ・
to (Tonga) ・
tpi (Tok Pisin) ・
tr (Turkish) ・
ts (Tsonga) ・
tt (Tatar) ・
tum (Tumbuka) ・
tw (Twi) ・
ty (Tahitian) ・
tyv (Tuvinian)
udm (Udmurt) ・
ug (Uighur) ・
uk (Ukrainian) ・
ur (Urdu) ・
uz (Uzbek)
ve (Venda) ・
vec (Venetian) ・
vep (Veps) ・
vi (Vietnamese) ・
vls (Vlaams) ・
vo (Volapük)
wa (Walloon) ・
war (Waray) ・
wo (Wolof) ・
wuu (Wu Chinese)
xal (Kalmyk) ・
xh (Xhosa) ・
xmf (Mingrelian)
yi (Yiddish) ・
yo (Yoruba)
za (Zhuang) ・
zea (Zeeuws) ・
zh (Chinese) ・
zu (Zulu)
MultiBPEmb
multi (multilingual)
Citing BPEmb
If you use BPEmb in academic work, please cite:
@InProceedings{heinzerling2018bpemb,
  author = {Benjamin Heinzerling and Michael Strube},
  title = ""{BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages}"",
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year = {2018},
  month = {May 7-12, 2018},
  address = {Miyazaki, Japan},
  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {979-10-95546-00-9},
  language = {english}
  }




Project details


Verified details    

These details have been verified by PyPI
Maintainers






            noutenki
          




Unverified details
These details have not been verified by PyPI
Project links



Homepage
      



Meta



License: MIT License (MIT)
      



Author: Benjamin Heinzerling





Classifiers


License



            OSI Approved :: MIT License
          




Operating System



            OS Independent
          




Programming Language



            Python :: 3
          





 



Release history

Release notifications |
              RSS feed 





This version







                  0.3.6
                


  Oct 1, 2024













                  0.3.5
                


  Mar 16, 2024













                  0.3.4
                


  Sep 23, 2022













                  0.3.3
                


  Apr 12, 2021













                  0.3.2
                


  Jul 17, 2020













                  0.3.0
                


  Jun 13, 2019













                  0.2.12
                


  Apr 30, 2019













                  0.2.11
                


  Feb 25, 2019













                  0.2.10
                


  Feb 9, 2019













                  0.2.9
                


  Dec 1, 2018













                  0.2.8
                


  Dec 1, 2018













                  0.2.7
                


  Nov 19, 2018







Download files
Download the file for your platform. If you're not sure which to choose, learn more about installing packages.

Source Distribution            






          bpemb-0.3.6.tar.gz
        
        (24.3 kB
        view hashes)
        
          Uploaded 
  Oct 1, 2024

Source




Built Distribution            






          bpemb-0.3.6-py3-none-any.whl
        
        (20.0 kB
        view hashes)
        
          Uploaded 
  Oct 1, 2024

Python 3








Close



Hashes for bpemb-0.3.6.tar.gz      

Hashes for bpemb-0.3.6.tar.gz


Algorithm
Hash digest





SHA256
a33fa1dcdfaf3d4cb3eaebac430b6f23a684a888e1761f5a026ce3868153ee2d


Copy              



MD5
81868482da2b6e1a7de66c0c3d65c26a


Copy              



BLAKE2b-256
761304c4da4daf77a5cfa5dc911a3de91a394ca6236331799d8c9957bdc85185


Copy              






Close






Close



Hashes for bpemb-0.3.6-py3-none-any.whl      

Hashes for bpemb-0.3.6-py3-none-any.whl


Algorithm
Hash digest





SHA256
6eabc133bbd0a7dbeb52b2cfed55ca5cacbb38b236ebb1f504b279a2d835e8b7


Copy              



MD5
499c2f704bafc7a87aa832dba3338e65


Copy              



BLAKE2b-256
c5f3e878025903d935de64a92acceb0c2af0c225d0fd17d3fe9502c61c86e504


Copy              






Close














Help


Installing packages
Uploading packages
User guide
Project name retention
FAQs




About PyPI


PyPI Blog
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors




Contributing to PyPI


Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits




Using PyPI


Code of conduct
Report security issue
Privacy Notice
Terms of Use
Acceptable Use Policy






Status:
all systems operational


Developed and maintained by the Python community, for the Python community.          
Donate today!


          ""PyPI"", ""Python Package Index"", and the blocks logos are registered trademarks of the Python Software Foundation.


          © 2024 Python Software Foundation
Site map




Switch to desktop version        







              English
            



              español
            



              français
            



              日本語
            



              português (Brasil)
            



              українська
            



              Ελληνικά
            



              Deutsch
            



              中文 (简体)
            



              中文 (繁體)
            



              русский
            



              עברית
            



              Esperanto
            





Supported by



AWS

            Cloud computing and Security Sponsor
          



Datadog

            Monitoring
          



Fastly

            CDN
          



Google

            Download Analytics
          



Microsoft

            PSF Sponsor
          



Pingdom

            Monitoring
          



Sentry

            Error logging
          



StatusPage

            Status page
          



",TXT,Keras Byte-Pair Encoding (BPE) of text variables,Text encoding based on Keras BPE Tokenizer class
Convert raw text fields into a document-term matrix. Based on scikit-learn TfidfVectorizer.,http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer,Convert raw text fields into a document-term matrix,PTM3,Text Preprocessing,"













TfidfVectorizer — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.feature_extraction
TfidfVectorizer









TfidfVectorizer#


class sklearn.feature_extraction.text.TfidfVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]#
Convert a collection of raw documents to a matrix of TF-IDF features.
Equivalent to CountVectorizer followed by
TfidfTransformer.
For an example of usage, see
Classification of text documents using sparse features.
For an efficiency comparison of the different feature extractors, see
FeatureHasher and DictVectorizer Comparison.
For an example of document clustering and comparison with
HashingVectorizer, see
Clustering text documents using k-means.
Read more in the User Guide.

Parameters:

input{‘filename’, ‘file’, ‘content’}, default=’content’
If 'filename', the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.
If 'file', the sequence items must have a ‘read’ method (file-like
object) that is called to fetch the bytes in memory.
If 'content', the input is expected to be a sequence of items that
can be of type string or byte.


encodingstr, default=’utf-8’If bytes or files are given to analyze, this encoding is used to
decode.

decode_error{‘strict’, ‘ignore’, ‘replace’}, default=’strict’Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given encoding. By default, it is
‘strict’, meaning that a UnicodeDecodeError will be raised. Other
values are ‘ignore’ and ‘replace’.

strip_accents{‘ascii’, ‘unicode’} or callable, default=NoneRemove accents and perform other character normalization
during the preprocessing step.
‘ascii’ is a fast method that only works on characters that have
a direct ASCII mapping.
‘unicode’ is a slightly slower method that works on any characters.
None (default) means no character normalization is performed.
Both ‘ascii’ and ‘unicode’ use NFKD normalization from
unicodedata.normalize.

lowercasebool, default=TrueConvert all characters to lowercase before tokenizing.

preprocessorcallable, default=NoneOverride the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.
Only applies if analyzer is not callable.

tokenizercallable, default=NoneOverride the string tokenization step while preserving the
preprocessing and n-grams generation steps.
Only applies if analyzer == 'word'.

analyzer{‘word’, ‘char’, ‘char_wb’} or callable, default=’word’Whether the feature should be made of word or character n-grams.
Option ‘char_wb’ creates character n-grams only from text inside
word boundaries; n-grams at the edges of words are padded with space.
If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.

Changed in version 0.21: Since v0.21, if input is 'filename' or 'file', the data
is first read from the file and then passed to the given callable
analyzer.


stop_words{‘english’}, list, default=NoneIf a string, it is passed to _check_stop_list and the appropriate stop
list is returned. ‘english’ is currently the only supported string
value.
There are several known issues with ‘english’ and you should
consider an alternative (see Using stop words).
If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.
Only applies if analyzer == 'word'.
If None, no stop words will be used. In this case, setting max_df
to a higher value, such as in the range (0.7, 1.0), can automatically detect
and filter stop words based on intra corpus document frequency of terms.

token_patternstr, default=r”(?u)\b\w\w+\b”Regular expression denoting what constitutes a “token”, only used
if analyzer == 'word'. The default regexp selects tokens of 2
or more alphanumeric characters (punctuation is completely ignored
and always treated as a token separator).
If there is a capturing group in token_pattern then the
captured group content, not the entire match, becomes the token.
At most one capturing group is permitted.

ngram_rangetuple (min_n, max_n), default=(1, 1)The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n <= n <= max_n
will be used. For example an ngram_range of (1, 1) means only
unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means
only bigrams.
Only applies if analyzer is not callable.

max_dffloat or int, default=1.0When building the vocabulary ignore terms that have a document
frequency strictly higher than the given threshold (corpus-specific
stop words).
If float in range [0.0, 1.0], the parameter represents a proportion of
documents, integer absolute counts.
This parameter is ignored if vocabulary is not None.

min_dffloat or int, default=1When building the vocabulary ignore terms that have a document
frequency strictly lower than the given threshold. This value is also
called cut-off in the literature.
If float in range of [0.0, 1.0], the parameter represents a proportion
of documents, integer absolute counts.
This parameter is ignored if vocabulary is not None.

max_featuresint, default=NoneIf not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.
Otherwise, all features are used.
This parameter is ignored if vocabulary is not None.

vocabularyMapping or iterable, default=NoneEither a Mapping (e.g., a dict) where keys are terms and values are
indices in the feature matrix, or an iterable over terms. If not
given, a vocabulary is determined from the input documents.

binarybool, default=FalseIf True, all non-zero term counts are set to 1. This does not mean
outputs will have only 0/1 values, only that the tf term in tf-idf
is binary. (Set binary to True, use_idf to False and
norm to None to get 0/1 outputs).

dtypedtype, default=float64Type of the matrix returned by fit_transform() or transform().

norm{‘l1’, ‘l2’} or None, default=’l2’Each output row will have unit norm, either:

‘l2’: Sum of squares of vector elements is 1. The cosine
similarity between two vectors is their dot product when l2 norm has
been applied.
‘l1’: Sum of absolute values of vector elements is 1.
See normalize.
None: No normalization.


use_idfbool, default=TrueEnable inverse-document-frequency reweighting. If False, idf(t) = 1.

smooth_idfbool, default=TrueSmooth idf weights by adding one to document frequencies, as if an
extra document was seen containing every term in the collection
exactly once. Prevents zero divisions.

sublinear_tfbool, default=FalseApply sublinear tf scaling, i.e. replace tf with 1 + log(tf).



Attributes:

vocabulary_dictA mapping of terms to feature indices.

fixed_vocabulary_boolTrue if a fixed vocabulary of term to indices mapping
is provided by the user.

idf_array of shape (n_features,)Inverse document frequency vector, only defined if use_idf=True.





See also

CountVectorizerTransforms text into a sparse matrix of n-gram counts.

TfidfTransformerPerforms the TF-IDF transformation from a provided matrix of counts.



Examples
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> corpus = [
...     'This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',
... ]
>>> vectorizer = TfidfVectorizer()
>>> X = vectorizer.fit_transform(corpus)
>>> vectorizer.get_feature_names_out()
array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',
       'this'], ...)
>>> print(X.shape)
(4, 9)




build_analyzer()[source]#
Return a callable to process input data.
The callable handles preprocessing, tokenization, and n-grams generation.

Returns:

analyzer: callableA function to handle preprocessing, tokenization
and n-grams generation.







build_preprocessor()[source]#
Return a function to preprocess the text before tokenization.

Returns:

preprocessor: callableA function to preprocess the text before tokenization.







build_tokenizer()[source]#
Return a function that splits a string into a sequence of tokens.

Returns:

tokenizer: callableA function to split a string into a sequence of tokens.







decode(doc)[source]#
Decode the input into a string of unicode symbols.
The decoding strategy depends on the vectorizer parameters.

Parameters:

docbytes or strThe string to decode.



Returns:

doc: strA string of unicode symbols.







fit(raw_documents, y=None)[source]#
Learn vocabulary and idf from training set.

Parameters:

raw_documentsiterableAn iterable which generates either str, unicode or file objects.

yNoneThis parameter is not needed to compute tfidf.



Returns:

selfobjectFitted vectorizer.







fit_transform(raw_documents, y=None)[source]#
Learn vocabulary and idf, return document-term matrix.
This is equivalent to fit followed by transform, but more efficiently
implemented.

Parameters:

raw_documentsiterableAn iterable which generates either str, unicode or file objects.

yNoneThis parameter is ignored.



Returns:

Xsparse matrix of (n_samples, n_features)Tf-idf-weighted document-term matrix.







get_feature_names_out(input_features=None)[source]#
Get output feature names for transformation.

Parameters:

input_featuresarray-like of str or None, default=NoneNot used, present here for API consistency by convention.



Returns:

feature_names_outndarray of str objectsTransformed feature names.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







get_stop_words()[source]#
Build or fetch the effective stop words list.

Returns:

stop_words: list or NoneA list of stop words.







property idf_#
Inverse document frequency vector, only defined if use_idf=True.

Returns:

ndarray of shape (n_features,)






inverse_transform(X)[source]#
Return terms per document with nonzero entries in X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Document-term matrix.



Returns:

X_invlist of arrays of shape (n_samples,)List of arrays of terms.







set_fit_request(*, raw_documents: bool | None | str = '$UNCHANGED$') → TfidfVectorizer[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

raw_documentsstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for raw_documents parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_transform_request(*, raw_documents: bool | None | str = '$UNCHANGED$') → TfidfVectorizer[source]#
Request metadata passed to the transform method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to transform if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to transform.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

raw_documentsstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for raw_documents parameter in transform.



Returns:

selfobjectThe updated object.







transform(raw_documents)[source]#
Transform documents to document-term matrix.
Uses the vocabulary and document frequencies (df) learned by fit (or
fit_transform).

Parameters:

raw_documentsiterableAn iterable which generates either str, unicode or file objects.



Returns:

Xsparse matrix of (n_samples, n_features)Tf-idf-weighted document-term matrix.







Gallery examples#

Biclustering documents with the Spectral Co-clustering algorithm
Biclustering documents with the Spectral Co-clustering algorithm

Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation

Sample pipeline for text feature extraction and evaluation
Sample pipeline for text feature extraction and evaluation

Column Transformer with Heterogeneous Data Sources
Column Transformer with Heterogeneous Data Sources

Classification of text documents using sparse features
Classification of text documents using sparse features

Clustering text documents using k-means
Clustering text documents using k-means

FeatureHasher and DictVectorizer Comparison
FeatureHasher and DictVectorizer Comparison










previous
TfidfTransformer




next
sklearn.feature_selection










 On this page
  


TfidfVectorizer
build_analyzer
build_preprocessor
build_tokenizer
decode
fit
fit_transform
get_feature_names_out
get_metadata_routing
get_params
get_stop_words
idf_
inverse_transform
set_fit_request
set_params
set_transform_request
transform


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",TXT,Matrix of word-grams occurrences,Convert raw text fields into a document-term matrix. Based on scikit-learn TfidfVectorizer.
"POS Tagging Featurizer
    
    Text POS-Tagging Featurizer for Composable ML.
    
    Part-of-speech (POS) tagging is a popular Natural Language Processing process that
    categorizes words in text (a corpus) as a particular part of speech,
    depending on the definition of the word and its context.
    
    Key processing steps:
    1. Given the parameter selection (after converting to lowercase), tokenization
    is performed against the input text using SpaCy's English language tokenizer. DataRobot does not
    currently support non-English POS-tagging.
    2. The SpaCy pre-trained model matches each token to its respective POS tag ``(verb, noun, adjective, adverb, etc.)``.
    3. POS-tagged tokens are then returned to the original sentence structure and added back to
    the input data.
    4. Input data column names that have been POS-tagged are given the suffix ``pos_tagged``.
    For example: ``text_col --> text_col_pos_tagged``
    
    Example of POS-tagged featurizer results:
    
    Input:
    ``the quick brown foxes jumping over the lazier dogs.``
    Output:
    ``the_DET quick_ADJ brown_ADJ foxes_NOUN jumping_VERB over_ADP the_DET lazier_ADJ dogs_NOUN``
    
    Parameters
    ----------
    lowercase (lc): select (default=`True`)
        When `True`, converts input text to lowercase before POS tagging begins. Default is True.
        ``values: [True, False]``
    
    References
    ----------
    .. [1] [1] Steven Bird, Edward Loper
       ""Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit""
       O'Reilly Media, Beijing, 2009, ISBN 978-0-596-51649-9
    
    See Also
    --------
    Source:
        `SpaCy Library
        <https://spacy.io/api/tagger>`_",no url,POS Tagging Featurizer,POS_TAGGING,Text Preprocessing,no documentation retrieved,TXT,POSTagging Featurizer,"Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context."
"Lemmatization Featurizer
    
    Text Lemmatization Featurizer for Composable ML.
    
    Lemmatization is the process of reducing text to a common base form from either its inflection
    or derviation form. Unlike stemming, which may reduce words to an invalid root, lemmatization's
    outputs are valid linguistic forms. For example:
    
    Key processing steps:
    
    1. Given the parameter selection (lemmatizer type, lowercase, language), tokenization
    is performed against the input text using SpaCy's English language tokenizer. DataRobot does not
    currently support non-English lemmatization. When a non-English language is selected,
    the original text is returned without lemmatization.
    
    2. Each token, with its respective part-of-speech ``(verb, noun, adjective, adverb, etc.)``,
    is processed through the selected lemmatizer. SpaCy's part-of-speech tagger is used
    for both lemmatizers.
    
    3. Lemmatized tokens are then returned to the original sentence structure and added back to
    the input data.
    
    4. Input data column names that have been lemmatized are given the suffix ``lemmatized``.
    For example: ``text_col --> text_col_lemmatized``
    
    Example of lemmatization featurizer results:
    
    Input:
    
    ``The quick brown foxes jumping over the lazier dogs.``
    
    Output:
    
    ``The quick brown fox jump over the lazy dog.``
    
    Parameters
    ----------
    lemmatizer (lemma): select (default='wordnet')
        Specifies which lemmatization implementation is performed, either `wordnet` or `spacy`.
        ``values: [wordnet, spacy]``
    
    lowercase (lc): select (default=`True`)
        When `True`, converts input text to lowercase before the lemmatization begins. Default is True.
        ``values: [True, False]``
    
    language (lang): select (default=`english`)
        Informs the lemmatizer which language interpretation to use when processing input text.
        Default (and only supported language) is English.
        ``values: [danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, portuguese, romanian, russian, spanish, swedish, turkish]``
    
    References
    ----------
    .. [1] [1] Steven Bird, Edward Loper
       ""Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit""
       O'Reilly Media, Beijing, 2009, ISBN 978-0-596-51649-9
    
    See Also
    --------
    Source:
        `SpaCy Library
        <https://spacy.io/api/lemmatizer>`_
    
    Source:
        `NLTK Library
        <https://www.nltk.org/>`_",no url,Lemmatization Featurizer,LEMMATIZER,Text Preprocessing,no documentation retrieved,TXT,Lemmatization Featurizer,"Lemmatizer. The lemmatizer uses either Wordnet or Spacy lemmatization. In linguistics, lemmatization is the process of grouping together the inflected form of a word."
Convert raw text fields into a vector. Based on MiniLM embeddings.,https://arxiv.org/abs/2002.10957,Convert raw text fields into a vector,SENTENCE_MULTILINGUAL_MINILM,Text Preprocessing,"


 [2002.10957] MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers





























  








Skip to main content





We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate





 > cs > arXiv:2002.10957
  





Help | Advanced Search




All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text




Search















open search






GO



open navigation menu


quick links

Login
Help Pages
About












Computer Science > Computation and Language


arXiv:2002.10957 (cs)
    




  [Submitted on 25 Feb 2020 (v1), last revised 6 Apr 2020 (this version, v2)]
Title:MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Authors:Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou View a PDF of the paper titled MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers, by Wenhui Wang and 5 other authors
View PDF

Abstract:Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.
    


 
Comments:
Code and models: this https URL


Subjects:

Computation and Language (cs.CL)

Cite as:
arXiv:2002.10957 [cs.CL]


 
(or 
arXiv:2002.10957v2 [cs.CL] for this version)
          


 
 https://doi.org/10.48550/arXiv.2002.10957


Focus to learn more



                  arXiv-issued DOI via DataCite







Submission history From: Wenhui Wang [view email]       [v1]
        Tue, 25 Feb 2020 15:21:10 UTC (590 KB)
[v2]
        Mon, 6 Apr 2020 02:53:18 UTC (356 KB)



 

Full-text links:
Access Paper:


View a PDF of the paper titled MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers, by Wenhui Wang and 5 other authorsView PDFTeX SourceOther Formats
view license

 
    Current browse context: cs.CL


< prev

  |   
next >


new
 | 
recent
 | 2020-02

    Change to browse by:
    
cs




References & Citations

NASA ADSGoogle Scholar
Semantic Scholar




DBLP - CS Bibliography

listing | bibtex 

Wenhui WangFuru WeiLi DongHangbo BaoNan Yang …


a
export BibTeX citation
Loading...




BibTeX formatted citation
×


loading...


Data provided by: 




Bookmark





 




Bibliographic Tools

Bibliographic and Citation Tools






Bibliographic Explorer Toggle



Bibliographic Explorer (What is the Explorer?)







Connected Papers Toggle



Connected Papers (What is Connected Papers?)






Litmaps Toggle



Litmaps (What is Litmaps?)







scite.ai Toggle



scite Smart Citations (What are Smart Citations?)









Code, Data, Media

Code, Data and Media Associated with this Article






alphaXiv Toggle



alphaXiv (What is alphaXiv?)







Links to Code Toggle



CatalyzeX Code Finder for Papers (What is CatalyzeX?)







DagsHub Toggle



DagsHub (What is DagsHub?)







GotitPub Toggle



Gotit.pub (What is GotitPub?)







Huggingface Toggle



Hugging Face (What is Huggingface?)







Links to Code Toggle



Papers with Code (What is Papers with Code?)







ScienceCast Toggle



ScienceCast (What is ScienceCast?)













Demos

Demos






Replicate Toggle



Replicate (What is Replicate?)







Spaces Toggle



Hugging Face Spaces (What is Spaces?)







Spaces Toggle



TXYZ.AI (What is TXYZ.AI?)








Related Papers

Recommenders and Search Tools






Link to Influence Flower



Influence Flower (What are Influence Flowers?)







Core recommender toggle



CORE Recommender (What is CORE?)





Author
Venue
Institution
Topic













        About arXivLabs
      



arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.










Which authors of this paper are endorsers? |
    Disable MathJax (What is MathJax?)
    












About
Help





contact arXivClick here to contact arXiv
 Contact


subscribe to arXiv mailingsClick here to subscribe
 Subscribe











Copyright
Privacy Policy




Web Accessibility Assistance


arXiv Operational Status 
                    Get status notifications via
                    email
                    or slack





 






",TXT,Pretrained Sentence MultiLingual MiniLM Featurizer,Convert raw text fields into a vector. Based on MiniLM embeddings.
Computes TextBlob Sentiment for text features,no url,Computes TextBlob Sentiment for text features,TEXTBLOB_SENTIMENT,Text Preprocessing,no documentation retrieved,TXT,TextBlob Sentiment Featurizer,Computes TextBlob Sentiment for text features
Text encoding based on Keras BPE Tokenizer class,https://pypi.org/project/bpemb/,Text encoding based on Keras BPE Tokenizer class,KERAS_BPE_TOKENIZER,Text Preprocessing,"








bpemb · PyPI










 

















Skip to main content

Switch to mobile version    







Warning

Some features may not work without JavaScript. Please try enabling it if you encounter problems.


 












Search PyPI



Search



 


Help
Sponsors
Log in
Register




Menu      




Help
Sponsors
Log in
Register



 




Search PyPI



Search








        bpemb 0.3.6
      


pip install bpemb


Copy PIP instructions






Latest version


Released: 
  Oct 1, 2024
 





 
Byte-pair embeddings in 275 languages
 







Navigation





Project description                




Release history                




Download files                






Verified details    

These details have been verified by PyPI
Maintainers






            noutenki
          




Unverified details
These details have not been verified by PyPI
Project links



Homepage
      



Meta



License: MIT License (MIT)
      



Author: Benjamin Heinzerling





Classifiers


License



            OSI Approved :: MIT License
          




Operating System



            OS Independent
          




Programming Language



            Python :: 3
          















Project description              




Project details              




Release history            




Download files              




Project description

BPEmb
BPEmb is a collection of pre-trained subword embeddings in 275 languages, based on Byte-Pair Encoding (BPE) and trained on Wikipedia. Its intended use is as input for neural models in natural language processing.
Website ・
Usage ・
Download ・
MultiBPEmb ・
Paper (pdf) ・
Citing BPEmb
Usage
Install BPEmb with pip:
pip install bpemb

Embeddings and SentencePiece models will be downloaded automatically the first time you use them.
>>> from bpemb import BPEmb
# load English BPEmb model with default vocabulary size (10k) and 50-dimensional embeddings
>>> bpemb_en = BPEmb(lang=""en"", dim=50)
downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model
downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d50.w2v.bin.tar.gz

You can do two main things with BPEmb. The first is subword segmentation:
# apply English BPE subword segmentation model
>>> bpemb_en.encode(""Stratford"")
['▁strat', 'ford']
# load Chinese BPEmb model with vocabulary size 100k and default (100-dim) embeddings
>>> bpemb_zh = BPEmb(lang=""zh"", vs=100000)
# apply Chinese BPE subword segmentation model
>>> bpemb_zh.encode(""这是一个中文句子"")  # ""This is a Chinese sentence.""
['▁这是一个', '中文', '句子']  # [""This is a"", ""Chinese"", ""sentence""]

If / how a word gets split depends on the vocabulary size. Generally, a smaller vocabulary size will yield a segmentation into many subwords, while a large vocabulary size will result in frequent words not being split:



vocabulary size
segmentation




1000
['▁str', 'at', 'f', 'ord']


3000
['▁str', 'at', 'ford']


5000
['▁str', 'at', 'ford']


10000
['▁strat', 'ford']


25000
['▁stratford']


50000
['▁stratford']


100000
['▁stratford']


200000
['▁stratford']



The second purpose of BPEmb is to provide pretrained subword embeddings:
# Embeddings are wrapped in a gensim KeyedVectors object
>>> type(bpemb_zh.emb)
gensim.models.keyedvectors.Word2VecKeyedVectors
# You can use BPEmb objects like gensim KeyedVectors
>>> bpemb_en.most_similar(""ford"")
[('bury', 0.8745079040527344),
 ('ton', 0.8725000619888306),
 ('well', 0.871537446975708),
 ('ston', 0.8701574206352234),
 ('worth', 0.8672043085098267),
 ('field', 0.859795331954956),
 ('ley', 0.8591548204421997),
 ('ington', 0.8126075267791748),
 ('bridge', 0.8099068999290466),
 ('brook', 0.7979353070259094)]
>>> type(bpemb_en.vectors)
numpy.ndarray
>>> bpemb_en.vectors.shape
(10000, 50)
>>> bpemb_zh.vectors.shape
(100000, 100)

To use subword embeddings in your neural network, either encode your input into subword IDs:
>>> ids = bpemb_zh.encode_ids(""这是一个中文句子"")
[25950, 695, 20199]
>>> bpemb_zh.vectors[ids].shape
(3, 100)

Or use the embed method:
# apply Chinese subword segmentation and perform embedding lookup
>>> bpemb_zh.embed(""这是一个中文句子"").shape
(3, 100)

Downloads for each language
ab (Abkhazian) ・
ace (Achinese) ・
ady (Adyghe) ・
af (Afrikaans) ・
ak (Akan) ・
als (Alemannic) ・
am (Amharic) ・
an (Aragonese) ・
ang (Old English) ・
ar (Arabic) ・
arc (Official Aramaic) ・
arz (Egyptian Arabic) ・
as (Assamese) ・
ast (Asturian) ・
atj (Atikamekw) ・
av (Avaric) ・
ay (Aymara) ・
az (Azerbaijani) ・
azb (South Azerbaijani)
ba (Bashkir) ・
bar (Bavarian) ・
bcl (Central Bikol) ・
be (Belarusian) ・
bg (Bulgarian) ・
bi (Bislama) ・
bjn (Banjar) ・
bm (Bambara) ・
bn (Bengali) ・
bo (Tibetan) ・
bpy (Bishnupriya) ・
br (Breton) ・
bs (Bosnian) ・
bug (Buginese) ・
bxr (Russia Buriat)
ca (Catalan) ・
cdo (Min Dong Chinese) ・
ce (Chechen) ・
ceb (Cebuano) ・
ch (Chamorro) ・
chr (Cherokee) ・
chy (Cheyenne) ・
ckb (Central Kurdish) ・
co (Corsican) ・
cr (Cree) ・
crh (Crimean Tatar) ・
cs (Czech) ・
csb (Kashubian) ・
cu (Church Slavic) ・
cv (Chuvash) ・
cy (Welsh)
da (Danish) ・
de (German) ・
din (Dinka) ・
diq (Dimli) ・
dsb (Lower Sorbian) ・
dty (Dotyali) ・
dv (Dhivehi) ・
dz (Dzongkha)
ee (Ewe) ・
el (Modern Greek) ・
en (English) ・
eo (Esperanto) ・
es (Spanish) ・
et (Estonian) ・
eu (Basque) ・
ext (Extremaduran)
fa (Persian) ・
ff (Fulah) ・
fi (Finnish) ・
fj (Fijian) ・
fo (Faroese) ・
fr (French) ・
frp (Arpitan) ・
frr (Northern Frisian) ・
fur (Friulian) ・
fy (Western Frisian)
ga (Irish) ・
gag (Gagauz) ・
gan (Gan Chinese) ・
gd (Scottish Gaelic) ・
gl (Galician) ・
glk (Gilaki) ・
gn (Guarani) ・
gom (Goan Konkani) ・
got (Gothic) ・
gu (Gujarati) ・
gv (Manx)
ha (Hausa) ・
hak (Hakka Chinese) ・
haw (Hawaiian) ・
he (Hebrew) ・
hi (Hindi) ・
hif (Fiji Hindi) ・
hr (Croatian) ・
hsb (Upper Sorbian) ・
ht (Haitian) ・
hu (Hungarian) ・
hy (Armenian)
ia (Interlingua) ・
id (Indonesian) ・
ie (Interlingue) ・
ig (Igbo) ・
ik (Inupiaq) ・
ilo (Iloko) ・
io (Ido) ・
is (Icelandic) ・
it (Italian) ・
iu (Inuktitut)
ja (Japanese) ・
jam (Jamaican Creole English) ・
jbo (Lojban) ・
jv (Javanese)
ka (Georgian) ・
kaa (Kara-Kalpak) ・
kab (Kabyle) ・
kbd (Kabardian) ・
kbp (Kabiyè) ・
kg (Kongo) ・
ki (Kikuyu) ・
kk (Kazakh) ・
kl (Kalaallisut) ・
km (Central Khmer) ・
kn (Kannada) ・
ko (Korean) ・
koi (Komi-Permyak) ・
krc (Karachay-Balkar) ・
ks (Kashmiri) ・
ksh (Kölsch) ・
ku (Kurdish) ・
kv (Komi) ・
kw (Cornish) ・
ky (Kirghiz)
la (Latin) ・
lad (Ladino) ・
lb (Luxembourgish) ・
lbe (Lak) ・
lez (Lezghian) ・
lg (Ganda) ・
li (Limburgan) ・
lij (Ligurian) ・
lmo (Lombard) ・
ln (Lingala) ・
lo (Lao) ・
lrc (Northern Luri) ・
lt (Lithuanian) ・
ltg (Latgalian) ・
lv (Latvian)
mai (Maithili) ・
mdf (Moksha) ・
mg (Malagasy) ・
mh (Marshallese) ・
mhr (Eastern Mari) ・
mi (Maori) ・
min (Minangkabau) ・
mk (Macedonian) ・
ml (Malayalam) ・
mn (Mongolian) ・
mr (Marathi) ・
mrj (Western Mari) ・
ms (Malay) ・
mt (Maltese) ・
mwl (Mirandese) ・
my (Burmese) ・
myv (Erzya) ・
mzn (Mazanderani)
na (Nauru) ・
nap (Neapolitan) ・
nds (Low German) ・
ne (Nepali) ・
new (Newari) ・
ng (Ndonga) ・
nl (Dutch) ・
nn (Norwegian Nynorsk) ・
no (Norwegian) ・
nov (Novial) ・
nrm (Narom) ・
nso (Pedi) ・
nv (Navajo) ・
ny (Nyanja)
oc (Occitan) ・
olo (Livvi) ・
om (Oromo) ・
or (Oriya) ・
os (Ossetian)
pa (Panjabi) ・
pag (Pangasinan) ・
pam (Pampanga) ・
pap (Papiamento) ・
pcd (Picard) ・
pdc (Pennsylvania German) ・
pfl (Pfaelzisch) ・
pi (Pali) ・
pih (Pitcairn-Norfolk) ・
pl (Polish) ・
pms (Piemontese) ・
pnb (Western Panjabi) ・
pnt (Pontic) ・
ps (Pushto) ・
pt (Portuguese)
qu (Quechua)
rm (Romansh) ・
rmy (Vlax Romani) ・
rn (Rundi) ・
ro (Romanian) ・
ru (Russian) ・
rue (Rusyn) ・
rw (Kinyarwanda)
sa (Sanskrit) ・
sah (Yakut) ・
sc (Sardinian) ・
scn (Sicilian) ・
sco (Scots) ・
sd (Sindhi) ・
se (Northern Sami) ・
sg (Sango) ・
sh (Serbo-Croatian) ・
si (Sinhala) ・
sk (Slovak) ・
sl (Slovenian) ・
sm (Samoan) ・
sn (Shona) ・
so (Somali) ・
sq (Albanian) ・
sr (Serbian) ・
srn (Sranan Tongo) ・
ss (Swati) ・
st (Southern Sotho) ・
stq (Saterfriesisch) ・
su (Sundanese) ・
sv (Swedish) ・
sw (Swahili) ・
szl (Silesian)
ta (Tamil) ・
tcy (Tulu) ・
te (Telugu) ・
tet (Tetum) ・
tg (Tajik) ・
th (Thai) ・
ti (Tigrinya) ・
tk (Turkmen) ・
tl (Tagalog) ・
tn (Tswana) ・
to (Tonga) ・
tpi (Tok Pisin) ・
tr (Turkish) ・
ts (Tsonga) ・
tt (Tatar) ・
tum (Tumbuka) ・
tw (Twi) ・
ty (Tahitian) ・
tyv (Tuvinian)
udm (Udmurt) ・
ug (Uighur) ・
uk (Ukrainian) ・
ur (Urdu) ・
uz (Uzbek)
ve (Venda) ・
vec (Venetian) ・
vep (Veps) ・
vi (Vietnamese) ・
vls (Vlaams) ・
vo (Volapük)
wa (Walloon) ・
war (Waray) ・
wo (Wolof) ・
wuu (Wu Chinese)
xal (Kalmyk) ・
xh (Xhosa) ・
xmf (Mingrelian)
yi (Yiddish) ・
yo (Yoruba)
za (Zhuang) ・
zea (Zeeuws) ・
zh (Chinese) ・
zu (Zulu)
MultiBPEmb
multi (multilingual)
Citing BPEmb
If you use BPEmb in academic work, please cite:
@InProceedings{heinzerling2018bpemb,
  author = {Benjamin Heinzerling and Michael Strube},
  title = ""{BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages}"",
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year = {2018},
  month = {May 7-12, 2018},
  address = {Miyazaki, Japan},
  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {979-10-95546-00-9},
  language = {english}
  }




Project details


Verified details    

These details have been verified by PyPI
Maintainers






            noutenki
          




Unverified details
These details have not been verified by PyPI
Project links



Homepage
      



Meta



License: MIT License (MIT)
      



Author: Benjamin Heinzerling





Classifiers


License



            OSI Approved :: MIT License
          




Operating System



            OS Independent
          




Programming Language



            Python :: 3
          





 



Release history

Release notifications |
              RSS feed 





This version







                  0.3.6
                


  Oct 1, 2024













                  0.3.5
                


  Mar 16, 2024













                  0.3.4
                


  Sep 23, 2022













                  0.3.3
                


  Apr 12, 2021













                  0.3.2
                


  Jul 17, 2020













                  0.3.0
                


  Jun 13, 2019













                  0.2.12
                


  Apr 30, 2019













                  0.2.11
                


  Feb 25, 2019













                  0.2.10
                


  Feb 9, 2019













                  0.2.9
                


  Dec 1, 2018













                  0.2.8
                


  Dec 1, 2018













                  0.2.7
                


  Nov 19, 2018







Download files
Download the file for your platform. If you're not sure which to choose, learn more about installing packages.

Source Distribution            






          bpemb-0.3.6.tar.gz
        
        (24.3 kB
        view hashes)
        
          Uploaded 
  Oct 1, 2024

Source




Built Distribution            






          bpemb-0.3.6-py3-none-any.whl
        
        (20.0 kB
        view hashes)
        
          Uploaded 
  Oct 1, 2024

Python 3








Close



Hashes for bpemb-0.3.6.tar.gz      

Hashes for bpemb-0.3.6.tar.gz


Algorithm
Hash digest





SHA256
a33fa1dcdfaf3d4cb3eaebac430b6f23a684a888e1761f5a026ce3868153ee2d


Copy              



MD5
81868482da2b6e1a7de66c0c3d65c26a


Copy              



BLAKE2b-256
761304c4da4daf77a5cfa5dc911a3de91a394ca6236331799d8c9957bdc85185


Copy              






Close






Close



Hashes for bpemb-0.3.6-py3-none-any.whl      

Hashes for bpemb-0.3.6-py3-none-any.whl


Algorithm
Hash digest





SHA256
6eabc133bbd0a7dbeb52b2cfed55ca5cacbb38b236ebb1f504b279a2d835e8b7


Copy              



MD5
499c2f704bafc7a87aa832dba3338e65


Copy              



BLAKE2b-256
c5f3e878025903d935de64a92acceb0c2af0c225d0fd17d3fe9502c61c86e504


Copy              






Close














Help


Installing packages
Uploading packages
User guide
Project name retention
FAQs




About PyPI


PyPI Blog
Infrastructure dashboard
Statistics
Logos & trademarks
Our sponsors




Contributing to PyPI


Bugs and feedback
Contribute on GitHub
Translate PyPI
Sponsor PyPI
Development credits




Using PyPI


Code of conduct
Report security issue
Privacy Notice
Terms of Use
Acceptable Use Policy






Status:
all systems operational


Developed and maintained by the Python community, for the Python community.          
Donate today!


          ""PyPI"", ""Python Package Index"", and the blocks logos are registered trademarks of the Python Software Foundation.


          © 2024 Python Software Foundation
Site map




Switch to desktop version        







              English
            



              español
            



              français
            



              日本語
            



              português (Brasil)
            



              українська
            



              Ελληνικά
            



              Deutsch
            



              中文 (简体)
            



              中文 (繁體)
            



              русский
            



              עברית
            



              Esperanto
            





Supported by



AWS

            Cloud computing and Security Sponsor
          



Datadog

            Monitoring
          



Fastly

            CDN
          



Google

            Download Analytics
          



Microsoft

            PSF Sponsor
          



Pingdom

            Monitoring
          



Sentry

            Error logging
          



StatusPage

            Status page
          



",TXT,Keras Byte-Pair Encoding (BPE) of text variables,Text encoding based on Keras BPE Tokenizer class
"FastText Word Vectorization
    
    Vector Space Modeling methods for word vectorization Task.
    
    fastText is a library for efficient learning of word representations.
    It comes in two model flavors--Continuous Bag-of-Words model (CBOW) and Skip-Gram.
    Algorithmically, these models are similar, except that CBOW predicts target words
    from source-context words, while skip-gram does the inverse, predicting source-context words from target words. This inversion might seem arbitrary,
    but statistically, it has the effect that CBOW smoothes over a lot of
    distributional information (by treating an entire context as one observation).
    Frequently, CBOW proves beneficial for smaller datasets.
    Because skip-gram treats each context-target pair as a new observation, it tends to
    do better with larger datasets.
    
    For example, consider the text: ``The cat jumped over the puddle``
    
    The CBOW approach is to treat ``[The, cat, over, the, puddle]`` as a context, and from these
    words, predicts or generates the center word ``jumped``.
    The skip-gram approach is to create a model such that from the given the center word, ``jumped``,
    the model predicts or generates the surrounding words ``[The, cat, over, the,
    puddle]``. In this case the word 'jumped' is the context.
    
    In addition, fastText models learn word representations while taking into account
    morphology--considering sub-word units and representing words by a
    sum of its character n-grams.
    
    Key processing steps:
    1. Prepare the input text column based on the parameter setting such as language,
    stop-word, and tokenization.
    
    2. Create a fastText model by learning a reduced context, maintaining a mapping
    of words to vectors. These vectors are a numerical representation of individual words.
    
    3. Transform a sentence (text) column to a vector column. Transform on a sentence
    is performed by simple or weighted averaging of all the word vectors it contains.
    
    Input:
    One or more text (sentences/phrases) column
    Output:
    Real-numbered vector of specified dimension
    
    FastText Word Embeddings are trained from scratch for user-specified datasets in DataRobot.
    This method affords language independence; there is no reliance on pre-defined
    words from other sources (e.g., Wikipedia or other text corpuses).
    
    Parameters
    ----------
    learning_method (learning_method): select (default=skip)
        Predictive modeling method for learning representation, either 'skip' (the default) or 'cbow'.
        ``values: [skip, cbow]``
    
    loss_type (loss): select (default=ns)
        Method used to discriminate the target word. CBOW and skip-gram models are trained using a binary classification objective (logistic regression), either:
        Negative Sampling (ns) the default, which maximizes when the model assigns
        high probabilities to real words,and low probabilities to noise words.
        Softmax (softmax)  maximizes in term of the softmax function.
        Hierarchical softmax (hs) is an approximation of softmax.
        ``values: [ns, hs, softmax]``
    
    dimension (dim): int (default=50)
        Size of the word vector.
        The parameter setting can affect training time, prediction time, and model size.
        A higher dimension requires more training data and can lead to
        better models, but may result in out of memory errors.
        Recommended values are in the range of 10 to 100 for large datasets (>100MB).
        ``values: [1, 300]``
    
    mc (mincount) : int (default=1)
        Minimal number of word occurrences (default 1) used for word modeling.
        ``values: [1, 10000],``
    
    epoch (epoch): int (default=5)
        Number of epochs (default 5) used for training.
        ``values: [1, 5000]``
    
    bucket (bucket): int (default=100000)
        Number of buckets (default 100000) used by the model. Word and character ngram features are hashed into a buckets to limit the memory usage of the model.
        ``values: [1000, 2000000]``
    
    min_charNgram (minc): int (default=3)
        Minimum length of character n-gram.
        ``values': {int: [1, 12]}``
    
    max_charNgram (maxc): int (default=6)
        Maximum length of character n-gram.
        ``values: [1, 12]``
    
    wordcontext (ws): int (default=5)
        Size of the context window (default 5). The context of a word is the set of surrounding words. If ws = 2 , for example, the context of the word ""fox"" in the sentence ""The quick brown fox
        jumped over the lazy dog"" is {""quick"", ""brown"", ""jumped"", ""over""}.
        ``values: [3, 10000]``
    
    n_neg_samples (n_neg_samples): int (default=64)
        Number of negative samples for loss function (default 64).
        ``values: [1, 99999]``
    
    learning_rate (lr): float (default=0.1)
        Rate of updates for the learning rate (default 0.1).
        ``values: [0.005, 10]``
    
    tokenize (tok): select (default=0)
        Word tokenizer used on the text document.
        'none' uses the default sklearn word tokenizer.
        'space' tokenizes text based on white spaces and new-lines.
        'wordpunct' tokenizes text based on punctuation.
        'treebank' tokenizes text using regular expressions, as in Penn Treebank.
        'tiny-segmenter-jp/mecab' uses a Japanese word tokenizer
        ``values: [None, space, wordpunct, tweet, treebank, tiny-segmenter-jp, mecab]``
    
    stemmer (stem): select (default=None)
        Algorithm to use to normalize words through word stemming. By default, no normalizer is applied.
        ``values: [None, snowball, lancaster, porter, wordnet]``
    
    language (wa_l): select (default=en)
        Language of text for processing (default is English).
        ``values: [af, am, an, ar, as, az, be, bg, bn, br, bs, ca, cs, cy, da, de, dz, el, en, eo, es
        et, eu, fa, fi, fo, fr, ga, gl, gu, he, hi, hr, ht, hu, hy, id, is, it, ja, jv, ka
        kk, km, kn, ko, ku, ky, la, lb, lo, lt, lv, mg, mk, ml, mn, mr, ms, mt, nb, ne, nl
        nn, no, oc, or, pa, pl, ps, pt, qu, ro, ru, rw, se, si, sk, sl, sq, sr, sv, sw, ta
        te, th, tl, tr, ug, uk, ur, vi, vo, wa, xh, zh, zu
        ]``
    
    lowercase (lc): bool (default='True')
        Whether to enforce lower-case before tokenization (default is enabled).
        ``values: [False, True]``
    
    stopword (sw): bool (default='False')
         Whether to use a stop word list, excluding some extremely common words which would appear to be of little value in word representation. If enabled, the option sets a stop word-list based on the language.
         ``values: [False, True]``
    
    thread (thread): int (default=1)
        Number of threads (default is 1 thread). Any value >1 will lose prediction/vector consistency.
        FastText is using hogwild method for parallelizing stochastic gradient descent, which does
        not need a thread safe matrix implementation.
        ``values: [1, 16]``
    
    References
    ----------
    .. [1] P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov.
       ""Enriching Word Vectors with Subword Information"", arXiv preprint arXiv:1607.04606}, 2016
       `[link]
       <https://arxiv.org/pdf/1607.04606.pdf>`__
    
    See Also
    --------
    Source:
        `A Python interface for Facebook fastText
            <https://pypi.python.org/pypi/fasttext>`_
        `Facebook Library for fast text representation and classification.
            <https://github.com/facebookresearch/fastText>`_",no url,FastText Word Vectorization,TXTEM1,Text Preprocessing,no documentation retrieved,TXT,Fasttext Word Vectorization and Mean text embedding,Convert raw text fields into a vector. Based on fasttext and word2vec.
Text encoding based on Keras Tokenizer class,https://keras.io/preprocessing/text/,Text encoding based on Keras Tokenizer class,KERAS_TOKENIZER,Text Preprocessing,"

",TXT,Keras encoding of text variables,Text encoding based on Keras Tokenizer class
Post Processing of Pretrained Convolutional Neural Network Image features,no url,Post Processing of Pretrained Convolutional Neural Network Image features,IMAGE_POST_PROCESSOR,Image Preprocessing,no documentation retrieved,IMG,No Post Processing,Post Processing of Pretrained Convolutional Neural Network Image features
Detects largest rectangle from images,no url,Detects largest rectangle from images,OPENCV_DETECT_LARGEST_RECTANGLE,Image Preprocessing,no documentation retrieved,IMG,OpenCV Detect Largest Rectangle,Detects largest rectangle from images
"Image featurization by converting to grayscale, downscaling the image, and flattening the image into a one-dimensional array of pixels",no url,"Image featurization by converting to grayscale, downscaling the image, and flattening the image into a one-dimensional array of pixels",IMG_GRAYSCALE_DOWNSCALED_IMAGE_FEATURIZER,Image Preprocessing,no documentation retrieved,IMG,Grayscale Downscaled Image Featurizer,"Image featurization by converting to grayscale, downscaling the image, and flattening the image into a one-dimensional array of pixels"
"Image Pretrained CNN Featurizer
    
    IMGFEA converts images to numeric feature vectors using a pretrained convolutional neural network.
    
    This method used here is called *transfer learning*, where a model developed for a task is reused as
    the starting point for a model on a second task. The method is particularly useful when used on
    datasets where the sample size is small and when there isn't enough variation in the target dataset
    to fully represent the target environment. DataRobot's technique extracts features from various
    levels of the convolutional neural network initialized with pretrained weights that include low
    level details, like edges and corners all the way to very high level details, like human faces and
    vehicles, that generalize well to a variety of different problems and user cases.
    
    The feature vectors produced (one to four, based on the parameters
    `use_low_level_features`, `use_medium_level_features`, `use_high_level_features`, and
    `use_highest_level_features` of this task) represent extracted characteristics of the image at
    different levels of complexity. These feature vectors are then used downstream as input to a
    modeler.
    
    Parameters
    ----------
    Pretrained model (network): select (default=squeezenet)
        Pretrained model architecture.
    
        DarkNet:
          This simple neural network consists of eight 3x3 convolutional blocks
          with batch normalization,
          Leaky ReLu activation, and pooling. The channel depth increases by a factor of two each
          block. The network has nine layers in total, including a final dense layer.
          https://pjreddie.com/darknet/
    
        DarkNet-Pruned:
          This simple neural network consists of eight 3x3 convolutional blocks
          with batch normalization,
          Leaky ReLu activation, and pooling. The channel depth increases by a factor of two each
          block. The network has nine layers in total, including a final dense layer..
          https://pjreddie.com/darknet/
          This is the same as DarkNet but with BatchNormalization layers removed after the Conv2D
          operation.
    
        EfficientNet-b0:
          EfficientNet-b0 is the fastest network in the EfficientNet network family, which were
          developed in 2019 using a ""multi-objective neural architecture search that optimizes both
          accuracy and FLOPS."" The b0 model notably outperforms ResNet-50 top-1 and top-5 accuracy on
          ImageNet while having ~5x fewer parameters (ResNet has 26M while b0 has 5.3M). The main
          building of the EfficientNet models is the mobile inverted residual bottleneck (MBConv)
          convolutional block &mdash; which serves as a bottleneck, similar to the fire module in
          SqueezeNet, to constrain the number of parameters, but is ""inverted"" because the shape is
          opposite (in the mobile inverted residual bottleneck, the layer order is expand then
          squeeze).
          https://arxiv.org/abs/1905.11946
    
        EfficientNetV2-S-Pruned:
          EfficientNetV2-S-Pruned is the latest Neural Network part of EfficientNets family.
          Combining all previous insights from EfficientNetV1 models (2019), and applying
          newly found Fused-MBConv approach by Google Neural Architecture Search:
          1. Replaces ""DepthwiseConv2D 3x3 followed by Conv2D 1x1"" with ""Conv2D 3x3"" (operation is called Fused-MBConv).
          2. Improves training procedures. Models are now pre-trained with over 13M+ images from 21k+ classes.
          In addition, DataRobot applies a layer-reducing ""pruning operation"",
          where Conv2D+BatchNorm can be merged into a single linear transformation, keeping only the Conv2D layer. This reduces the total number of layers by 110, while preserving the same accuracy. See ""EfficientNetV2: Smaller Models and Faster Training"" (https://arxiv.org/pdf/2104.00298) for more information.
          https://arxiv.org/pdf/2104.00298
    
        EfficientNet-b0-Pruned:
          The pruned architecture of EfficientNet-b0 version, meaning layer reduction.
          Conv2D+BatchNorm and DepthWise2D+BatchNorm can be merged into a single linear
          transformation, keeping only Conv2D/DepthWise and adding the value of BatchNorm as
          bias to the convolution. This results in an architecture with ~50 less layers and
          a significantly faster inference stage.
    
        EfficientNet-b4:
          The neural network in DataRobot likely to be the most accurate for a given dataset.
          The main goal in developing the EfficientNet family of models is to create a
          simple way to ""scale up"" the size of a model to obtain more accuracy at the cost of more
          computation. The implementation of the b4 model scales up (from Efficient-b0)
          the width of the network
          (number of channels in each convolution) by 1.4, and the depth of the network (the number
          of convolutional blocks) by 1.8, providing a more accurate and slower model than b0 but with
          results comparable to ResNext-101 or PolyNet.
          https://arxiv.org/abs/1905.11946
    
        EfficientNet-b4-Pruned:
          The pruned architecture of EfficientNet-b4 version.
          Conv2D+BatchNorm and DepthWise2D+BatchNorm can be merged into a single linear
          transformation, keeping only Conv2D/DepthWise and adding the value of BatchNorm as
          bias to the convolution. This results in an architecture with ~100 less layers and
          a significantly faster inference stage.
    
        MobileNetV3-Small-Pruned:
          Mobilenet V3 is the latest incarnation of the MobileNet family of neural networks,
          which are specially designed for mobile phone CPUs and other low-resource devices.
          It comes in two variants: MobileNet3-Large and MobileNet3-Small.
          MobileNetV3-Large targets high resource usage, whereas MobileNet3-Small is for low
          resource usage. MobileNetV3-Small is 6.6% more accurate the previous MobileNetV2 network
          with almost identical or better latency.
          In addition to these lightweight blocks and operations, pruning is applied for even
          faster feature extraction. This pruned version keeps the same architecture but with
          signigicantly lower number of layers ( ~50 ). Conv2D + BatchNormalization and
          Depthwise2D + BatchNormalization are merged into single Conv2D layer.
          ""Searching for MobileNetV3"" https://arxiv.org/abs/1905.02244
    
    
        PreResNet10:
          PreResNet10 is based on ResNet, except within each residual block the batch norm and ReLu
          activation happen _before_ rather than _after_ the convolutional layer. This
          implementation of the PreResNet architecture has four PreRes blocks with two
          convolutional blocks each, which yield 10 total layers, including an input
          convolutional layer and output dense layer. According to the paper (p.14), the model's
          computational complexity scales linearly with the depth of the network, so this
          model is about 5x faster than ResNet50. However, because the richness of
          the features generated by the featurizer can affect the fitting time of downstream
          modelers like XGB with Early Stopping, the time taken to train a model using a deeper
          featurizer like ResNet50 could be even more than 5x.
          https://arxiv.org/abs/1603.05027
    
        ResNet50:
          This classic neural network is based on residual blocks containing skip-ahead layers,
          which allow for very deep networks that still train effectively. In each
          residual block, the inputs to the block are run through a 3x3 convolution, batch norm,
          and ReLu activation - twice. Then that result is added to the inputs to the block, which
          effectively turns that result into a residual of the layer. This implementation of
          ResNet has a total of 50 layers, including an input convolutional
          layer, 48 residual blocks, and a final dense layer.
          https://arxiv.org/abs/1512.03385
    
        ResNet50-Pruned:
          This classic neural network is based on residual blocks containing skip-ahead layers,
          which allow for very deep networks that still train effectively. In each
          residual block, the inputs to the block are run through a 3x3 convolution, batch norm,
          and ReLu activation - twice. Then that result is added to the inputs to the block, which
          effectively turns that result into a residual of the layer. This implementation of
          ResNet has a total of 50 layers, including an input convolutional
          layer, 48 residual blocks, and a final dense layer.
          The only difference between ResNet50-Pruned is that all
          BatchNormalization layers after Conv2D operation are removed, resulting in a network
          with ~50 less layers compared to original architecture. This makes it way faster
          for both CPU and GPU inference.
          https://arxiv.org/abs/1512.03385
    
        SqueezeNet:
          The fastest neural network in DataRobot&mdash;this network is designed to achieve
          the speed of AlexNet with 50x fewer parameters, allowing for faster training, prediction
          , and storage size. It is based around the concept of fire modules,
          consisting of a combination of ""squeeze"" layers followed by ""expand"" layers,
          which aim to dramatically reduce the number of parameters used while preserving
          relatively high accuracy. This implementation of SqueezNet v1.1 has an input convolutional
          layer followed by eight fire modules of three convolutions each, leading to a total of
          25 total layers.
          Original paper: https://arxiv.org/abs/1602.07360
          Explanation of the differences introduced in v1.1:
          https://github.com/forresti/SqueezeNet/tree/master/SqueezeNet_v1.1
    
        Xception:
          This neural network is an improvement in accuracy over the popular Inception V3 network
          , which has comparable speed to ResNet-50 but with better accuracy on some datasets. The
          core idea of this network is that it saves on parameters by learning spatial correlations
          separately from cross-channel correlations. The
          core building block of this network is the depthwise separable convolution
          (a depthwise convolution + pointwise convolution) with residual layers added (similar
          to PreResNet-10). This building block aims to ""decouple"" the learning happening across the
          spatial dimensions (height and width) with the learning happening across the channel
          dimensions (depth), so that they are handled in separate parameters where interaction
          can be learned from other parameters downstream in the network. This network has a total of 36
          convolutional layers, including has 11 convolutional layers in the ""entry flow"" where the
          width and height are reduced and the depth increases, and 24 convolutional layers where the
          size remains constant.
          https://arxiv.org/abs/1610.02357
    
        Below is an estimate of the architectures grouped into ""speed classes"".
    
        Non-Pruned Versions:
        1st (Fastest): SqueezeNet, PreResNet-10, DarkNet
        2nd: EfficientNet-b0
        3rd: ResNet50
        4th: ResNet50
        5th: Xception
        6th: (Slowest): EfficientNet-b4
    
        Pruned Versions:
        1st  (Fastest): MobileneNetV3-Small-Pruned
        2nd  DarkNet-Pruned
        3rd: EfficientNet-b0-Pruned
        4th: EfficientNet-B4-Pruned & EfficientNetV2-S-Pruned
        5th: (Slowest) ResNet50-Pruned
    
        We recommend using pruned version when available.
    
        Although ranking these neural network architectures on accuracy is difficult to do across all
        possible datasets (and input feature datatypes), note that speed is loosely
        inversely correlated to the accuracy. However, there are plenty of examples of datasets
        in which a faster model may be more accurate (i.e. squeezenet sometimes being more accurate
        than PreResNet, or EfficientNet-b0 often being more accurate than ResNet-50). It's also
        worth noting that advanced tuning of combinations of hyperparameters
        (including neural network architectures) can yield unexpected interactions that can boost
        accuracy, as deep learning hyperparameter tuning is still an active area of research that
        develops as new research and new neural network architectures are published.
        For extra accuracy, use the EfficientNetV2-S-Pruned model, as it is the latest incarnation of DL research for Computer Vision from Google AI. (2021)
    
        ``values: [
        'darknet',
        'darknet-pruned',
        'efficientnet-b0',
        'efficientnet-b0-pruned,
        'efficientnet-b4',
        'efficientnet-b4-pruned',
        'efficientnetv2-s-pruned',
        'mobilenetv3-small-pruned',
        'preresnet10',
        'resnet50',
        'resnet50-pruned,
        'squeezenet',
        'xception',
        ]``
    
    use_highest_level_features (use_highest_level_features): select  (default=True)
        Extracts features from the final hidden layer. This feature works well if the
        target problem contains ""ordinary"" images, such as animals, or vehicles, or foods, or
        plants.
    
        ``values: [True, False]``
    
    use_high_level_features (use_high_level_features): select (default=True)
        Extracts features from one of the last convolution layers. This layer provides high level
        features that are more complex than low or medium level features, and typically combines
        patterns from various low and medium level features to form complex objects that
        entirely depend on the architecture chosen. Note that this feature level might have
        different meanings for different architectures&mdash;for shallow networksm this layer might still
        produce features that are considered by humans as low level features.
        This generally works for classification problems.
    
        ``values: [True, False]``
    
    use_medium_level_features (use_medium_level_features): select (default=True)
        Extracts features from one of the intermediate layer convolution layers. This layer
        provides medium level visual features that are more complex than low level features,
        and typically combines patterns from different levels of low level features.
    
        ``values: [True, False]``
    
    use_low_level_features (use_low_level_features): select (default=False)
        Extracts features from one of the initial layer convolution layers. This layer provides low
        level visual features and will benefit problems that is vastly different from ""ordinary""
        image datasets.
    
        ``values: [True, False]``
    
    Featurizer Pooling Type (featurizer_pool) : select (default=avg)
        Type of summarizer to use to squash the multi dimensional CNN features
        applied on initial, intermediate, and top convolutional layers of the network.
        ``values: ['avg', 'gem', 'max', 'gem-max', 'avg-max']``
    
    metric_ace: (ma): select(default=None)
        The type of metric to be used to compute univariate feature importance ACE
    
    automated_feature_reduction: (automated_feature_reduction): select(default=False)
        Decides whether or not to use ACE scores for feature reduction by removing the features that have
        very low ACE scores.
        ``values: ['True', 'False']``
    
    batch_size: (batch_size): int(default=4)
        Number of images that will be featurized in a single batch. Larger values will lead to faster
        training/prediction times but will also increase resource consumption. Also, due to innate
        specifics of TensorFlow, any value larger than 1 may lead to instability of featurization
        results around the 1e-6 - 1e-7, which for some models can cause a prediction validation error.
        In those cases, it is recommended to retrain with batch_size=1.
        ``values: [1, 4]``
    
    References
    ----------
    .. [1] Pan, S. J., and Yang, Q.
       ""A Survey on Transfer Learning""
       Knowledge and Data Engineering, IEEE Transactionson 22 (10): 1345-1359.
       `[link]
       <https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf>`__
    
    See Also
    --------
    Source:
        `Convolutional Neural Network wikipedia
        <https://en.wikipedia.org/wiki/Convolutional_neural_network>`_",no url,Image Pretrained CNN Featurizer,IMGFEA,Image Preprocessing,no documentation retrieved,IMG,Pretrained Multi-Level Global Average Pooling Image Featurizer,Image featurization using pre-trained deep neural network models.
Computes OpenCV features for images,no url,Computes OpenCV features for images,OPENCV_FEATURIZER,Image Preprocessing,no documentation retrieved,IMG,OpenCV Image Featurizer,Computes OpenCV features for images
Selects a single Summarized Categorical column by name,no url,Selects a single Summarized Categorical column by name,SCBAGOFCAT2,Summarized Categorical Preprocessing,no documentation retrieved,COUNT_DICT,Single Column Converter for Summarized Categorical,Selects a single Summarized Categorical column by name
Convert the count dict data into the sparse matrix,no url,Convert the count dict data into the sparse matrix,CDICT2SP,Summarized Categorical Preprocessing,no documentation retrieved,COUNT_DICT,Summarized Categorical to Sparse Matrix,Convert the count dict data into the sparse matrix
Convert Geospatial Location features.,no url,Convert Geospatial Location features,GEO_IN,Geospatial Preprocessing,no documentation retrieved,GEO,Geospatial Location Converter,Convert Geospatial Location features.
Spatial Neighborhood Featurizer,no url,Spatial Neighborhood Featurizer,GEO_NEIGHBOR_V1,Geospatial Preprocessing,no documentation retrieved,GEO,Spatial Neighborhood Featurizer,Spatial Neighborhood Featurizer
Elasticnet model with Unsupervised Learning features. Trained using block coordinate descent-- a common form of derivated-free optimization. Based on lightning CDRegressor.,http://contrib.scikit-learn.org/lightning/,Elasticnet model with Unsupervised Learning features,UENETCD,Regression,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Elastic-Net Regressor (L1 / Least-Squares Loss) with Unsupervised Learning Features,Elasticnet model with Unsupervised Learning features. Trained using block coordinate descent-- a common form of derivated-free optimization. Based on lightning CDRegressor.
"Approximate Kernel Support Vector Regressor using ElasticNet. Support vector machines are a class of “maximum margin” classifiers. They seek to maximize the separation they find between classes, and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations. ",http://scikit-learn.org/stable/modules/kernel_approximation.html,Approximate Kernel Support Vector Regressor using ElasticNet,ASVMER,Regression,"













6.7. Kernel Approximation — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

1. Supervised learning
1.1. Linear Models
1.2. Linear and Quadratic Discriminant Analysis
1.3. Kernel ridge regression
1.4. Support Vector Machines
1.5. Stochastic Gradient Descent
1.6. Nearest Neighbors
1.7. Gaussian Processes
1.8. Cross decomposition
1.9. Naive Bayes
1.10. Decision Trees
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking
1.12. Multiclass and multioutput algorithms
1.13. Feature selection
1.14. Semi-supervised learning
1.15. Isotonic regression
1.16. Probability calibration
1.17. Neural network models (supervised)


2. Unsupervised learning
2.1. Gaussian mixture models
2.2. Manifold learning
2.3. Clustering
2.4. Biclustering
2.5. Decomposing signals in components (matrix factorization problems)
2.6. Covariance estimation
2.7. Novelty and Outlier Detection
2.8. Density Estimation
2.9. Neural network models (unsupervised)


3. Model selection and evaluation
3.1. Cross-validation: evaluating estimator performance
3.2. Tuning the hyper-parameters of an estimator
3.3. Tuning the decision threshold for class prediction
3.4. Metrics and scoring: quantifying the quality of predictions
3.5. Validation curves: plotting scores to evaluate models


4. Inspection
4.1. Partial Dependence and Individual Conditional Expectation plots
4.2. Permutation feature importance


5. Visualizations
6. Dataset transformations
6.1. Pipelines and composite estimators
6.2. Feature extraction
6.3. Preprocessing data
6.4. Imputation of missing values
6.5. Unsupervised dimensionality reduction
6.6. Random Projection
6.7. Kernel Approximation
6.8. Pairwise metrics, Affinities and Kernels
6.9. Transforming the prediction target (y)


7. Dataset loading utilities
7.1. Toy datasets
7.2. Real world datasets
7.3. Generated datasets
7.4. Loading other datasets


8. Computing with scikit-learn
8.1. Strategies to scale computationally: bigger data
8.2. Computational Performance
8.3. Parallelism, resource management, and configuration


9. Model persistence
10. Common pitfalls and recommended practices
11. Dispatching
11.1. Array API support (experimental)


12. Choosing the right estimator
13. External Resources, Videos and Talks






















User Guide
6. Dataset transformations










6.7. Kernel Approximation#
This submodule contains functions that approximate the feature mappings that
correspond to certain kernels, as they are used for example in support vector
machines (see Support Vector Machines).
The following feature functions perform non-linear transformations of the
input, which can serve as a basis for linear classification or other
algorithms.
The advantage of using approximate explicit feature maps compared to the
kernel trick,
which makes use of feature maps implicitly, is that explicit mappings
can be better suited for online learning and can significantly reduce the cost
of learning with very large datasets.
Standard kernelized SVMs do not scale well to large datasets, but using an
approximate kernel map it is possible to use much more efficient linear SVMs.
In particular, the combination of kernel map approximations with
SGDClassifier can make non-linear learning on large datasets possible.
Since there has not been much empirical work using approximate embeddings, it
is advisable to compare results against exact kernel methods when possible.

See also
Polynomial regression: extending linear models with basis functions for an exact polynomial transformation.


6.7.1. Nystroem Method for Kernel Approximation#
The Nystroem method, as implemented in Nystroem is a general method for
reduced rank approximations of kernels. It achieves this by subsampling without
replacement rows/columns of the data on which the kernel is evaluated. While the
computational complexity of the exact method is
\(\mathcal{O}(n^3_{\text{samples}})\), the complexity of the approximation
is \(\mathcal{O}(n^2_{\text{components}} \cdot n_{\text{samples}})\), where
one can set \(n_{\text{components}} \ll n_{\text{samples}}\) without a
significative decrease in performance [WS2001].
We can construct the eigendecomposition of the kernel matrix \(K\), based
on the features of the data, and then split it into sampled and unsampled data
points.

\[\begin{split}K = U \Lambda U^T
= \begin{bmatrix} U_1 \\ U_2\end{bmatrix} \Lambda \begin{bmatrix} U_1 \\ U_2 \end{bmatrix}^T
= \begin{bmatrix} U_1 \Lambda U_1^T & U_1 \Lambda U_2^T \\ U_2 \Lambda U_1^T & U_2 \Lambda U_2^T \end{bmatrix}
\equiv \begin{bmatrix} K_{11} & K_{12} \\ K_{21} & K_{22} \end{bmatrix}\end{split}\]
where:

\(U\) is orthonormal
\(\Lambda\) is diagonal matrix of eigenvalues
\(U_1\) is orthonormal matrix of samples that were chosen
\(U_2\) is orthonormal matrix of samples that were not chosen

Given that \(U_1 \Lambda U_1^T\) can be obtained by orthonormalization of
the matrix \(K_{11}\), and \(U_2 \Lambda U_1^T\) can be evaluated (as
well as its transpose), the only remaining term to elucidate is
\(U_2 \Lambda U_2^T\). To do this we can express it in terms of the already
evaluated matrices:

\[\begin{split}\begin{align} U_2 \Lambda U_2^T &= \left(K_{21} U_1 \Lambda^{-1}\right) \Lambda \left(K_{21} U_1 \Lambda^{-1}\right)^T
\\&= K_{21} U_1 (\Lambda^{-1} \Lambda) \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} U_1 \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} K_{11}^{-1} K_{21}^T
\\&= \left( K_{21} K_{11}^{-\frac12} \right) \left( K_{21} K_{11}^{-\frac12} \right)^T
.\end{align}\end{split}\]
During fit, the class Nystroem evaluates the basis \(U_1\), and
computes the normalization constant, \(K_{11}^{-\frac12}\). Later, during
transform, the kernel matrix is determined between the basis (given by the
components_ attribute) and the new data points, X. This matrix is then
multiplied by the normalization_ matrix for the final result.
By default Nystroem uses the rbf kernel, but it can use any kernel
function or a precomputed kernel matrix. The number of samples used - which is
also the dimensionality of the features computed - is given by the parameter
n_components.
Examples

See the example entitled
Time-related feature engineering,
that shows an efficient machine learning pipeline that uses a
Nystroem kernel.



6.7.2. Radial Basis Function Kernel#
The RBFSampler constructs an approximate mapping for the radial basis
function kernel, also known as Random Kitchen Sinks [RR2007]. This
transformation can be used to explicitly model a kernel map, prior to applying
a linear algorithm, for example a linear SVM:
>>> from sklearn.kernel_approximation import RBFSampler
>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
>>> y = [0, 0, 1, 1]
>>> rbf_feature = RBFSampler(gamma=1, random_state=1)
>>> X_features = rbf_feature.fit_transform(X)
>>> clf = SGDClassifier(max_iter=5)
>>> clf.fit(X_features, y)
SGDClassifier(max_iter=5)
>>> clf.score(X_features, y)
1.0


The mapping relies on a Monte Carlo approximation to the
kernel values. The fit function performs the Monte Carlo sampling, whereas
the transform method performs the mapping of the data.  Because of the
inherent randomness of the process, results may vary between different calls to
the fit function.
The fit function takes two arguments:
n_components, which is the target dimensionality of the feature transform,
and gamma, the parameter of the RBF-kernel.  A higher n_components will
result in a better approximation of the kernel and will yield results more
similar to those produced by a kernel SVM. Note that “fitting” the feature
function does not actually depend on the data given to the fit function.
Only the dimensionality of the data is used.
Details on the method can be found in [RR2007].
For a given value of n_components RBFSampler is often less accurate
as Nystroem. RBFSampler is cheaper to compute, though, making
use of larger feature spaces more efficient.




Comparing an exact RBF kernel (left) with the approximation (right)#


Examples

Explicit feature map approximation for RBF kernels



6.7.3. Additive Chi Squared Kernel#
The additive chi squared kernel is a kernel on histograms, often used in computer vision.
The additive chi squared kernel as used here is given by

\[k(x, y) = \sum_i \frac{2x_iy_i}{x_i+y_i}\]
This is not exactly the same as sklearn.metrics.pairwise.additive_chi2_kernel.
The authors of [VZ2010] prefer the version above as it is always positive
definite.
Since the kernel is additive, it is possible to treat all components
\(x_i\) separately for embedding. This makes it possible to sample
the Fourier transform in regular intervals, instead of approximating
using Monte Carlo sampling.
The class AdditiveChi2Sampler implements this component wise
deterministic sampling. Each component is sampled \(n\) times, yielding
\(2n+1\) dimensions per input dimension (the multiple of two stems
from the real and complex part of the Fourier transform).
In the literature, \(n\) is usually chosen to be 1 or 2, transforming
the dataset to size n_samples * 5 * n_features (in the case of \(n=2\)).
The approximate feature map provided by AdditiveChi2Sampler can be combined
with the approximate feature map provided by RBFSampler to yield an approximate
feature map for the exponentiated chi squared kernel.
See the [VZ2010] for details and [VVZ2010] for combination with the RBFSampler.


6.7.4. Skewed Chi Squared Kernel#
The skewed chi squared kernel is given by:

\[k(x,y) = \prod_i \frac{2\sqrt{x_i+c}\sqrt{y_i+c}}{x_i + y_i + 2c}\]
It has properties that are similar to the exponentiated chi squared kernel
often used in computer vision, but allows for a simple Monte Carlo
approximation of the feature map.
The usage of the SkewedChi2Sampler is the same as the usage described
above for the RBFSampler. The only difference is in the free
parameter, that is called \(c\).
For a motivation for this mapping and the mathematical details see [LS2010].


6.7.5. Polynomial Kernel Approximation via Tensor Sketch#
The polynomial kernel is a popular type of kernel
function given by:

\[k(x, y) = (\gamma x^\top y +c_0)^d\]
where:

x, y are the input vectors
d is the kernel degree

Intuitively, the feature space of the polynomial kernel of degree d
consists of all possible degree-d products among input features, which enables
learning algorithms using this kernel to account for interactions between features.
The TensorSketch [PP2013] method, as implemented in PolynomialCountSketch, is a
scalable, input data independent method for polynomial kernel approximation.
It is based on the concept of Count sketch [WIKICS] [CCF2002] , a dimensionality
reduction technique similar to feature hashing, which instead uses several
independent hash functions. TensorSketch obtains a Count Sketch of the outer product
of two vectors (or a vector with itself), which can be used as an approximation of the
polynomial kernel feature space. In particular, instead of explicitly computing
the outer product, TensorSketch computes the Count Sketch of the vectors and then
uses polynomial multiplication via the Fast Fourier Transform to compute the
Count Sketch of their outer product.
Conveniently, the training phase of TensorSketch simply consists of initializing
some random variables. It is thus independent of the input data, i.e. it only
depends on the number of input features, but not the data values.
In addition, this method can transform samples in
\(\mathcal{O}(n_{\text{samples}}(n_{\text{features}} + n_{\text{components}} \log(n_{\text{components}})))\)
time, where \(n_{\text{components}}\) is the desired output dimension,
determined by n_components.
Examples

Scalable learning with polynomial kernel approximation



6.7.6. Mathematical Details#
Kernel methods like support vector machines or kernelized
PCA rely on a property of reproducing kernel Hilbert spaces.
For any positive definite kernel function \(k\) (a so called Mercer kernel),
it is guaranteed that there exists a mapping \(\phi\)
into a Hilbert space \(\mathcal{H}\), such that

\[k(x,y) = \langle \phi(x), \phi(y) \rangle\]
Where \(\langle \cdot, \cdot \rangle\) denotes the inner product in the
Hilbert space.
If an algorithm, such as a linear support vector machine or PCA,
relies only on the scalar product of data points \(x_i\), one may use
the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm
to the mapped data points \(\phi(x_i)\).
The advantage of using \(k\) is that the mapping \(\phi\) never has
to be calculated explicitly, allowing for arbitrary large
features (even infinite).
One drawback of kernel methods is, that it might be necessary
to store many kernel values \(k(x_i, x_j)\) during optimization.
If a kernelized classifier is applied to new data \(y_j\),
\(k(x_i, y_j)\) needs to be computed to make predictions,
possibly for many different \(x_i\) in the training set.
The classes in this submodule allow to approximate the embedding
\(\phi\), thereby working explicitly with the representations
\(\phi(x_i)\), which obviates the need to apply the kernel
or store training examples.
References


[WS2001]
“Using the Nyström method to speed up kernel machines”
Williams, C.K.I.; Seeger, M. - 2001.


[RR2007]
(1,2)
“Random features for large-scale kernel machines”
Rahimi, A. and Recht, B. - Advances in neural information processing 2007,


[LS2010]
“Random Fourier approximations for skewed multiplicative histogram kernels”
Li, F., Ionescu, C., and Sminchisescu, C.
- Pattern Recognition,  DAGM 2010, Lecture Notes in Computer Science.


[VZ2010]
(1,2)
“Efficient additive kernels via explicit feature maps”
Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010


[VVZ2010]
“Generalized RBF feature maps for Efficient Detection”
Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010


[PP2013]
“Fast and scalable polynomial kernels via explicit feature maps”
Pham, N., & Pagh, R. - 2013


[CCF2002]
“Finding frequent items in data streams”
Charikar, M., Chen, K., & Farach-Colton - 2002


[WIKICS]
“Wikipedia: Count sketch”












previous
6.6. Random Projection




next
6.8. Pairwise metrics, Affinities and Kernels










 On this page
  


6.7.1. Nystroem Method for Kernel Approximation
6.7.2. Radial Basis Function Kernel
6.7.3. Additive Chi Squared Kernel
6.7.4. Skewed Chi Squared Kernel
6.7.5. Polynomial Kernel Approximation via Tensor Sketch
6.7.6. Mathematical Details





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Nystroem Kernel SVM Regressor using Elastic-Net,"Approximate Kernel Support Vector Regressor using ElasticNet. Support vector machines are a class of “maximum margin” classifiers. They seek to maximize the separation they find between classes, and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations. "
Frequency-Severity eXtreme Gradient Boosted Trees,no url,Frequency-Severity eXtreme Gradient Boosted Trees,FSXX2,Regression,no documentation retrieved,NUM,Frequency-Severity eXtreme Gradient Boosted Trees,Frequency-Severity eXtreme Gradient Boosted Trees
RuleFit Regressor. A rulefit model is an advanced ensemble method that combines the best qualities of tree-based and linear models.,no url,RuleFit Regressor,XRULEFITR,Regression,no documentation retrieved,NUM,XRuleFit Regressor,RuleFit Regressor. A rulefit model is an advanced ensemble method that combines the best qualities of tree-based and linear models.
Ridge Regression. This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Based on scikit-learn Ridge,http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge,Ridge Regression,RIDGEWC,Regression,"













Ridge — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.linear_model
Ridge









Ridge#


class sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=None, tol=0.0001, solver='auto', positive=False, random_state=None)[source]#
Linear least squares with l2 regularization.
Minimizes the objective function:
||y - Xw||^2_2 + alpha * ||w||^2_2


This model solves a regression model where the loss function is
the linear least squares function and regularization is given by
the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
This estimator has built-in support for multi-variate regression
(i.e., when y is a 2d-array of shape (n_samples, n_targets)).
Read more in the User Guide.

Parameters:

alpha{float, ndarray of shape (n_targets,)}, default=1.0Constant that multiplies the L2 term, controlling regularization
strength. alpha must be a non-negative float i.e. in [0, inf).
When alpha = 0, the objective is equivalent to ordinary least
squares, solved by the LinearRegression object. For numerical
reasons, using alpha = 0 with the Ridge object is not advised.
Instead, you should use the LinearRegression object.
If an array is passed, penalties are assumed to be specific to the
targets. Hence they must correspond in number.

fit_interceptbool, default=TrueWhether to fit the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. X and y are expected to be centered).

copy_Xbool, default=TrueIf True, X will be copied; else, it may be overwritten.

max_iterint, default=NoneMaximum number of iterations for conjugate gradient solver.
For ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined
by scipy.sparse.linalg. For ‘sag’ solver, the default value is 1000.
For ‘lbfgs’ solver, the default value is 15000.

tolfloat, default=1e-4The precision of the solution (coef_) is determined by tol which
specifies a different convergence criterion for each solver:

‘svd’: tol has no impact.
‘cholesky’: tol has no impact.
‘sparse_cg’: norm of residuals smaller than tol.
‘lsqr’: tol is set as atol and btol of scipy.sparse.linalg.lsqr,
which control the norm of the residual vector in terms of the norms of
matrix and coefficients.
‘sag’ and ‘saga’: relative change of coef smaller than tol.
‘lbfgs’: maximum of the absolute (projected) gradient=max|residuals|
smaller than tol.


Changed in version 1.2: Default value changed from 1e-3 to 1e-4 for consistency with other linear
models.


solver{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’,             ‘sag’, ‘saga’, ‘lbfgs’}, default=’auto’Solver to use in the computational routines:

‘auto’ chooses the solver automatically based on the type of data.
‘svd’ uses a Singular Value Decomposition of X to compute the Ridge
coefficients. It is the most stable solver, in particular more stable
for singular matrices than ‘cholesky’ at the cost of being slower.
‘cholesky’ uses the standard scipy.linalg.solve function to
obtain a closed-form solution.
‘sparse_cg’ uses the conjugate gradient solver as found in
scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
more appropriate than ‘cholesky’ for large-scale data
(possibility to set tol and max_iter).
‘lsqr’ uses the dedicated regularized least-squares routine
scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
procedure.
‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses
its improved, unbiased version named SAGA. Both methods also use an
iterative procedure, and are often faster than other solvers when
both n_samples and n_features are large. Note that ‘sag’ and
‘saga’ fast convergence is only guaranteed on features with
approximately the same scale. You can preprocess the data with a
scaler from sklearn.preprocessing.
‘lbfgs’ uses L-BFGS-B algorithm implemented in
scipy.optimize.minimize. It can be used only when positive
is True.

All solvers except ‘svd’ support both dense and sparse data. However, only
‘lsqr’, ‘sag’, ‘sparse_cg’, and ‘lbfgs’ support sparse input when
fit_intercept is True.

Added in version 0.17: Stochastic Average Gradient descent solver.


Added in version 0.19: SAGA solver.


positivebool, default=FalseWhen set to True, forces the coefficients to be positive.
Only ‘lbfgs’ solver is supported in this case.

random_stateint, RandomState instance, default=NoneUsed when solver == ‘sag’ or ‘saga’ to shuffle the data.
See Glossary for details.

Added in version 0.17: random_state to support Stochastic Average Gradient.




Attributes:

coef_ndarray of shape (n_features,) or (n_targets, n_features)Weight vector(s).

intercept_float or ndarray of shape (n_targets,)Independent term in decision function. Set to 0.0 if
fit_intercept = False.

n_iter_None or ndarray of shape (n_targets,)Actual number of iterations for each target. Available only for
sag and lsqr solvers. Other solvers will return None.

Added in version 0.17.


n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


solver_strThe solver that was used at fit time by the computational
routines.

Added in version 1.5.






See also

RidgeClassifierRidge classifier.

RidgeCVRidge regression with built-in cross validation.

KernelRidgeKernel ridge regression combines ridge regression with the kernel trick.



Notes
Regularization improves the conditioning of the problem and
reduces the variance of the estimates. Larger values specify stronger
regularization. Alpha corresponds to 1 / (2C) in other linear
models such as LogisticRegression or
LinearSVC.
Examples
>>> from sklearn.linear_model import Ridge
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> rng = np.random.RandomState(0)
>>> y = rng.randn(n_samples)
>>> X = rng.randn(n_samples, n_features)
>>> clf = Ridge(alpha=1.0)
>>> clf.fit(X, y)
Ridge()




fit(X, y, sample_weight=None)[source]#
Fit Ridge regression model.

Parameters:

X{ndarray, sparse matrix} of shape (n_samples, n_features)Training data.

yndarray of shape (n_samples,) or (n_samples, n_targets)Target values.

sample_weightfloat or ndarray of shape (n_samples,), default=NoneIndividual weights for each sample. If given a float, every sample
will have the same weight.



Returns:

selfobjectFitted estimator.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict using the linear model.

Parameters:

Xarray-like or sparse matrix, shape (n_samples, n_features)Samples.



Returns:

Carray, shape (n_samples,)Returns predicted values.







score(X, y, sample_weight=None)[source]#
Return the coefficient of determination of the prediction.
The coefficient of determination \(R^2\) is defined as
\((1 - \frac{u}{v})\), where \(u\) is the residual
sum of squares ((y_true - y_pred)** 2).sum() and \(v\)
is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of y, disregarding the input features, would get
a \(R^2\) score of 0.0.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples. For some estimators this may be a precomputed
kernel matrix or a list of generic objects instead with shape
(n_samples, n_samples_fitted), where n_samples_fitted
is the number of samples used in the fitting for the estimator.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True values for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloat\(R^2\) of self.predict(X) w.r.t. y.




Notes
The \(R^2\) score used when calling score on a regressor uses
multioutput='uniform_average' from version 0.23 to keep consistent
with default value of r2_score.
This influences the score method of all the multioutput
regressors (except for
MultiOutputRegressor).



set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → Ridge[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → Ridge[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







Gallery examples#

Compressive sensing: tomography reconstruction with L1 prior (Lasso)
Compressive sensing: tomography reconstruction with L1 prior (Lasso)

Prediction Latency
Prediction Latency

Comparison of kernel ridge and Gaussian process regression
Comparison of kernel ridge and Gaussian process regression

HuberRegressor vs Ridge on dataset with strong outliers
HuberRegressor vs Ridge on dataset with strong outliers

L1-based models for Sparse Signals
L1-based models for Sparse Signals

Ordinary Least Squares and Ridge Regression Variance
Ordinary Least Squares and Ridge Regression Variance

Plot Ridge coefficients as a function of the regularization
Plot Ridge coefficients as a function of the regularization

Poisson regression and non-normal loss
Poisson regression and non-normal loss

Polynomial and Spline interpolation
Polynomial and Spline interpolation

Ridge coefficients as a function of the L2 Regularization
Ridge coefficients as a function of the L2 Regularization

Common pitfalls in the interpretation of coefficients of linear models
Common pitfalls in the interpretation of coefficients of linear models

Imputing missing values with variants of IterativeImputer
Imputing missing values with variants of IterativeImputer

Target Encoder’s Internal Cross fitting
Target Encoder's Internal Cross fitting










previous
LinearRegression




next
RidgeCV










 On this page
  


Ridge
fit
get_metadata_routing
get_params
predict
score
set_fit_request
set_params
set_score_request


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Ridge Regression,Ridge Regression. This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Based on scikit-learn Ridge
Auto-tuned Stochastic Gradient Descent regressor. Based on scikit-learn. Stochastic Gradient Descent is a extremely scalable method for fitting linear regression models to big data.,http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDRegressor,Auto-tuned Stochastic Gradient Descent regressor,SGDRA,Regression,"













SGDClassifier — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.linear_model
SGDClassifier









SGDClassifier#


class sklearn.linear_model.SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)[source]#
Linear classifiers (SVM, logistic regression, etc.) with SGD training.
This estimator implements regularized linear models with stochastic
gradient descent (SGD) learning: the gradient of the loss is estimated
each sample at a time and the model is updated along the way with a
decreasing strength schedule (aka learning rate). SGD allows minibatch
(online/out-of-core) learning via the partial_fit method.
For best results using the default learning rate schedule, the data should
have zero mean and unit variance.
This implementation works with data represented as dense or sparse arrays
of floating point values for the features. The model it fits can be
controlled with the loss parameter; by default, it fits a linear support
vector machine (SVM).
The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.
Read more in the User Guide.

Parameters:

loss{‘hinge’, ‘log_loss’, ‘modified_huber’, ‘squared_hinge’,        ‘perceptron’, ‘squared_error’, ‘huber’, ‘epsilon_insensitive’,        ‘squared_epsilon_insensitive’}, default=’hinge’The loss function to be used.

‘hinge’ gives a linear SVM.
‘log_loss’ gives logistic regression, a probabilistic classifier.
‘modified_huber’ is another smooth loss that brings tolerance to
outliers as well as probability estimates.
‘squared_hinge’ is like hinge but is quadratically penalized.
‘perceptron’ is the linear loss used by the perceptron algorithm.
The other losses, ‘squared_error’, ‘huber’, ‘epsilon_insensitive’ and
‘squared_epsilon_insensitive’ are designed for regression but can be useful
in classification as well; see
SGDRegressor for a description.

More details about the losses formulas can be found in the
User Guide.

penalty{‘l2’, ‘l1’, ‘elasticnet’, None}, default=’l2’The penalty (aka regularization term) to be used. Defaults to ‘l2’
which is the standard regularizer for linear SVM models. ‘l1’ and
‘elasticnet’ might bring sparsity to the model (feature selection)
not achievable with ‘l2’. No penalty is added when set to None.

alphafloat, default=0.0001Constant that multiplies the regularization term. The higher the
value, the stronger the regularization. Also used to compute the
learning rate when learning_rate is set to ‘optimal’.
Values must be in the range [0.0, inf).

l1_ratiofloat, default=0.15The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Only used if penalty is ‘elasticnet’.
Values must be in the range [0.0, 1.0].

fit_interceptbool, default=TrueWhether the intercept should be estimated or not. If False, the
data is assumed to be already centered.

max_iterint, default=1000The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the fit method, and not the
partial_fit method.
Values must be in the range [1, inf).

Added in version 0.19.


tolfloat or None, default=1e-3The stopping criterion. If it is not None, training will stop
when (loss > best_loss - tol) for n_iter_no_change consecutive
epochs.
Convergence is checked against the training loss or the
validation loss depending on the early_stopping parameter.
Values must be in the range [0.0, inf).

Added in version 0.19.


shufflebool, default=TrueWhether or not the training data should be shuffled after each epoch.

verboseint, default=0The verbosity level.
Values must be in the range [0, inf).

epsilonfloat, default=0.1Epsilon in the epsilon-insensitive loss functions; only if loss is
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.
For ‘huber’, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.
Values must be in the range [0.0, inf).

n_jobsint, default=NoneThe number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation.
None means 1 unless in a joblib.parallel_backend context.
-1 means using all processors. See Glossary
for more details.

random_stateint, RandomState instance, default=NoneUsed for shuffling the data, when shuffle is set to True.
Pass an int for reproducible output across multiple function calls.
See Glossary.
Integer values must be in the range [0, 2**32 - 1].

learning_ratestr, default=’optimal’The learning rate schedule:

‘constant’: eta = eta0
‘optimal’: eta = 1.0 / (alpha * (t + t0))
where t0 is chosen by a heuristic proposed by Leon Bottou.
‘invscaling’: eta = eta0 / pow(t, power_t)
‘adaptive’: eta = eta0, as long as the training keeps decreasing.
Each time n_iter_no_change consecutive epochs fail to decrease the
training loss by tol or fail to increase validation score by tol if
early_stopping is True, the current learning rate is divided by 5.


Added in version 0.20: Added ‘adaptive’ option





eta0float, default=0.0The initial learning rate for the ‘constant’, ‘invscaling’ or
‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by
the default schedule ‘optimal’.
Values must be in the range [0.0, inf).

power_tfloat, default=0.5The exponent for inverse scaling learning rate.
Values must be in the range (-inf, inf).

early_stoppingbool, default=FalseWhether to use early stopping to terminate training when validation
score is not improving. If set to True, it will automatically set aside
a stratified fraction of training data as validation and terminate
training when validation score returned by the score method is not
improving by at least tol for n_iter_no_change consecutive epochs.

Added in version 0.20: Added ‘early_stopping’ option


validation_fractionfloat, default=0.1The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.
Values must be in the range (0.0, 1.0).

Added in version 0.20: Added ‘validation_fraction’ option


n_iter_no_changeint, default=5Number of iterations with no improvement to wait before stopping
fitting.
Convergence is checked against the training loss or the
validation loss depending on the early_stopping parameter.
Integer values must be in the range [1, max_iter).

Added in version 0.20: Added ‘n_iter_no_change’ option


class_weightdict, {class_label: weight} or “balanced”, default=NonePreset for the class_weight fit parameter.
Weights associated with classes. If not given, all classes
are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as n_samples / (n_classes * np.bincount(y)).

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See the Glossary.
Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.
If a dynamic learning rate is used, the learning rate is adapted
depending on the number of samples already seen. Calling fit resets
this counter, while partial_fit will result in increasing the
existing counter.

averagebool or int, default=FalseWhen set to True, computes the averaged SGD weights across all
updates and stores the result in the coef_ attribute. If set to
an int greater than 1, averaging will begin once the total number of
samples seen reaches average. So average=10 will begin
averaging after seeing 10 samples.
Integer values must be in the range [1, n_samples].



Attributes:

coef_ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)Weights assigned to the features.

intercept_ndarray of shape (1,) if n_classes == 2 else (n_classes,)Constants in decision function.

n_iter_intThe actual number of iterations before reaching the stopping criterion.
For multiclass fits, it is the maximum over every binary fit.

loss_function_concrete LossFunction
Deprecated since version 1.4: Attribute loss_function_ was deprecated in version 1.4 and will be
removed in 1.6.


classes_array of shape (n_classes,)
t_intNumber of weight updates performed during training.
Same as (n_iter_ * n_samples + 1).

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

sklearn.svm.LinearSVCLinear support vector classification.

LogisticRegressionLogistic regression.

PerceptronInherits from SGDClassifier. Perceptron() is equivalent to SGDClassifier(loss=""perceptron"", eta0=1, learning_rate=""constant"", penalty=None).



Examples
>>> import numpy as np
>>> from sklearn.linear_model import SGDClassifier
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.pipeline import make_pipeline
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> Y = np.array([1, 1, 2, 2])
>>> # Always scale the input. The most convenient way is to use a pipeline.
>>> clf = make_pipeline(StandardScaler(),
...                     SGDClassifier(max_iter=1000, tol=1e-3))
>>> clf.fit(X, Y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('sgdclassifier', SGDClassifier())])
>>> print(clf.predict([[-0.8, -1]]))
[1]




decision_function(X)[source]#
Predict confidence scores for samples.
The confidence score for a sample is proportional to the signed
distance of that sample to the hyperplane.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the confidence scores.



Returns:

scoresndarray of shape (n_samples,) or (n_samples, n_classes)Confidence scores per (n_samples, n_classes) combination. In the
binary case, confidence score for self.classes_[1] where >0 means
this class would be predicted.







densify()[source]#
Convert coefficient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the
default format of coef_ and is required for fitting, so calling
this method is only required on models that have previously been
sparsified; otherwise, it is a no-op.

Returns:

selfFitted estimator.







fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)[source]#
Fit linear model with Stochastic Gradient Descent.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Training data.

yndarray of shape (n_samples,)Target values.

coef_initndarray of shape (n_classes, n_features), default=NoneThe initial coefficients to warm-start the optimization.

intercept_initndarray of shape (n_classes,), default=NoneThe initial intercept to warm-start the optimization.

sample_weightarray-like, shape (n_samples,), default=NoneWeights applied to individual samples.
If not provided, uniform weights are assumed. These weights will
be multiplied with class_weight (passed through the
constructor) if class_weight is specified.



Returns:

selfobjectReturns an instance of self.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







partial_fit(X, y, classes=None, sample_weight=None)[source]#
Perform one epoch of stochastic gradient descent on given samples.
Internally, this method uses max_iter = 1. Therefore, it is not
guaranteed that a minimum of the cost function is reached after calling
it once. Matters such as objective convergence, early stopping, and
learning rate adjustments should be handled by the user.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Subset of the training data.

yndarray of shape (n_samples,)Subset of the target values.

classesndarray of shape (n_classes,), default=NoneClasses across all calls to partial_fit.
Can be obtained by via np.unique(y_all), where y_all is the
target vector of the entire dataset.
This argument is required for the first call to partial_fit
and can be omitted in the subsequent calls.
Note that y doesn’t need to contain all labels in classes.

sample_weightarray-like, shape (n_samples,), default=NoneWeights applied to individual samples.
If not provided, uniform weights are assumed.



Returns:

selfobjectReturns an instance of self.







predict(X)[source]#
Predict class labels for samples in X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the predictions.



Returns:

y_predndarray of shape (n_samples,)Vector containing the class labels for each sample.







predict_log_proba(X)[source]#
Log of probability estimates.
This method is only available for log loss and modified Huber loss.
When loss=”modified_huber”, probability estimates may be hard zeros
and ones, so taking the logarithm is not possible.
See predict_proba for details.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Input data for prediction.



Returns:

Tarray-like, shape (n_samples, n_classes)Returns the log-probability of the sample for each class in the
model, where classes are ordered as they are in
self.classes_.







predict_proba(X)[source]#
Probability estimates.
This method is only available for log loss and modified Huber loss.
Multiclass probability estimates are derived from binary (one-vs.-rest)
estimates by simple normalization, as recommended by Zadrozny and
Elkan.
Binary probability estimates for loss=”modified_huber” are given by
(clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions
it is necessary to perform proper probability calibration by wrapping
the classifier with
CalibratedClassifierCV instead.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Input data for prediction.



Returns:

ndarray of shape (n_samples, n_classes)Returns the probability of the sample for each class in the model,
where classes are ordered as they are in self.classes_.




References
Zadrozny and Elkan, “Transforming classifier scores into multiclass
probability estimates”, SIGKDD’02,
https://dl.acm.org/doi/pdf/10.1145/775047.775151
The justification for the formula in the loss=”modified_huber”
case is in the appendix B in:
http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf



score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, coef_init: bool | None | str = '$UNCHANGED$', intercept_init: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → SGDClassifier[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

coef_initstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for coef_init parameter in fit.

intercept_initstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for intercept_init parameter in fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_partial_fit_request(*, classes: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → SGDClassifier[source]#
Request metadata passed to the partial_fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to partial_fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to partial_fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

classesstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for classes parameter in partial_fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in partial_fit.



Returns:

selfobjectThe updated object.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → SGDClassifier[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







sparsify()[source]#
Convert coefficient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for
L1-regularized models can be much more memory- and storage-efficient
than the usual numpy.ndarray representation.
The intercept_ member is not converted.

Returns:

selfFitted estimator.




Notes
For non-sparse models, i.e. when there are not many zeros in coef_,
this may actually increase memory usage, so use this method with
care. A rule of thumb is that the number of zero elements, which can
be computed with (coef_ == 0).sum(), must be more than 50% for this
to provide significant benefits.
After calling this method, further fitting with the partial_fit
method (if any) will not work until you call densify.



Gallery examples#

Model Complexity Influence
Model Complexity Influence

Out-of-core classification of text documents
Out-of-core classification of text documents

Comparing various online solvers
Comparing various online solvers

Early stopping of Stochastic Gradient Descent
Early stopping of Stochastic Gradient Descent

Plot multi-class SGD on the iris dataset
Plot multi-class SGD on the iris dataset

SGD: Maximum margin separating hyperplane
SGD: Maximum margin separating hyperplane

SGD: Penalties
SGD: Penalties

SGD: Weighted samples
SGD: Weighted samples

SGD: convex loss functions
SGD: convex loss functions

Explicit feature map approximation for RBF kernels
Explicit feature map approximation for RBF kernels

Comparing randomized search and grid search for hyperparameter estimation
Comparing randomized search and grid search for hyperparameter estimation

Semi-supervised Classification on a Text Dataset
Semi-supervised Classification on a Text Dataset

Classification of text documents using sparse features
Classification of text documents using sparse features










previous
RidgeClassifierCV




next
SGDOneClassSVM










 On this page
  


SGDClassifier
decision_function
densify
fit
get_metadata_routing
get_params
partial_fit
predict
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_partial_fit_request
set_score_request
sparsify


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Auto-tuned Stochastic Gradient Descent Regression,Auto-tuned Stochastic Gradient Descent regressor. Based on scikit-learn. Stochastic Gradient Descent is a extremely scalable method for fitting linear regression models to big data.
Random Forests based on scikit-learn. Random forests are an ensemble method where hundreds (or thousands) of individual decision trees are fit to boostrap re-samples of the original dataset.  ExtraTrees are a variant of RandomForests with even more randomness.,,Random Forests based on scikit-learn,RFR,Regression,,NUM,ExtraTrees Regressor,Random Forests based on scikit-learn. Random forests are an ensemble method where hundreds (or thousands) of individual decision trees are fit to boostrap re-samples of the original dataset.  ExtraTrees are a variant of RandomForests with even more randomness.
Stochastic Gradient Descent Regressor. Based on scikit-learn. Stochastic Gradient Descent is a extremely scalable method for fitting linear regression models to big data.,http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor,Stochastic Gradient Descent Regressor,SGDR,Regression,"













SGDRegressor — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.linear_model
SGDRegressor









SGDRegressor#


class sklearn.linear_model.SGDRegressor(loss='squared_error', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)[source]#
Linear model fitted by minimizing a regularized empirical loss with SGD.
SGD stands for Stochastic Gradient Descent: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a decreasing strength schedule (aka learning rate).
The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.
This implementation works with data represented as dense numpy arrays of
floating point values for the features.
Read more in the User Guide.

Parameters:

lossstr, default=’squared_error’The loss function to be used. The possible values are ‘squared_error’,
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’
The ‘squared_error’ refers to the ordinary least squares fit.
‘huber’ modifies ‘squared_error’ to focus less on getting outliers
correct by switching from squared to linear loss past a distance of
epsilon. ‘epsilon_insensitive’ ignores errors less than epsilon and is
linear past that; this is the loss function used in SVR.
‘squared_epsilon_insensitive’ is the same but becomes squared loss past
a tolerance of epsilon.
More details about the losses formulas can be found in the
User Guide.

penalty{‘l2’, ‘l1’, ‘elasticnet’, None}, default=’l2’The penalty (aka regularization term) to be used. Defaults to ‘l2’
which is the standard regularizer for linear SVM models. ‘l1’ and
‘elasticnet’ might bring sparsity to the model (feature selection)
not achievable with ‘l2’. No penalty is added when set to None.

alphafloat, default=0.0001Constant that multiplies the regularization term. The higher the
value, the stronger the regularization. Also used to compute the
learning rate when learning_rate is set to ‘optimal’.
Values must be in the range [0.0, inf).

l1_ratiofloat, default=0.15The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Only used if penalty is ‘elasticnet’.
Values must be in the range [0.0, 1.0].

fit_interceptbool, default=TrueWhether the intercept should be estimated or not. If False, the
data is assumed to be already centered.

max_iterint, default=1000The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the fit method, and not the
partial_fit method.
Values must be in the range [1, inf).

Added in version 0.19.


tolfloat or None, default=1e-3The stopping criterion. If it is not None, training will stop
when (loss > best_loss - tol) for n_iter_no_change consecutive
epochs.
Convergence is checked against the training loss or the
validation loss depending on the early_stopping parameter.
Values must be in the range [0.0, inf).

Added in version 0.19.


shufflebool, default=TrueWhether or not the training data should be shuffled after each epoch.

verboseint, default=0The verbosity level.
Values must be in the range [0, inf).

epsilonfloat, default=0.1Epsilon in the epsilon-insensitive loss functions; only if loss is
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.
For ‘huber’, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.
Values must be in the range [0.0, inf).

random_stateint, RandomState instance, default=NoneUsed for shuffling the data, when shuffle is set to True.
Pass an int for reproducible output across multiple function calls.
See Glossary.

learning_ratestr, default=’invscaling’The learning rate schedule:

‘constant’: eta = eta0
‘optimal’: eta = 1.0 / (alpha * (t + t0))
where t0 is chosen by a heuristic proposed by Leon Bottou.
‘invscaling’: eta = eta0 / pow(t, power_t)
‘adaptive’: eta = eta0, as long as the training keeps decreasing.
Each time n_iter_no_change consecutive epochs fail to decrease the
training loss by tol or fail to increase validation score by tol if
early_stopping is True, the current learning rate is divided by 5.


Added in version 0.20: Added ‘adaptive’ option





eta0float, default=0.01The initial learning rate for the ‘constant’, ‘invscaling’ or
‘adaptive’ schedules. The default value is 0.01.
Values must be in the range [0.0, inf).

power_tfloat, default=0.25The exponent for inverse scaling learning rate.
Values must be in the range (-inf, inf).

early_stoppingbool, default=FalseWhether to use early stopping to terminate training when validation
score is not improving. If set to True, it will automatically set aside
a fraction of training data as validation and terminate
training when validation score returned by the score method is not
improving by at least tol for n_iter_no_change consecutive
epochs.

Added in version 0.20: Added ‘early_stopping’ option


validation_fractionfloat, default=0.1The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.
Values must be in the range (0.0, 1.0).

Added in version 0.20: Added ‘validation_fraction’ option


n_iter_no_changeint, default=5Number of iterations with no improvement to wait before stopping
fitting.
Convergence is checked against the training loss or the
validation loss depending on the early_stopping parameter.
Integer values must be in the range [1, max_iter).

Added in version 0.20: Added ‘n_iter_no_change’ option


warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See the Glossary.
Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.
If a dynamic learning rate is used, the learning rate is adapted
depending on the number of samples already seen. Calling fit resets
this counter, while partial_fit  will result in increasing the
existing counter.

averagebool or int, default=FalseWhen set to True, computes the averaged SGD weights across all
updates and stores the result in the coef_ attribute. If set to
an int greater than 1, averaging will begin once the total number of
samples seen reaches average. So average=10 will begin
averaging after seeing 10 samples.



Attributes:

coef_ndarray of shape (n_features,)Weights assigned to the features.

intercept_ndarray of shape (1,)The intercept term.

n_iter_intThe actual number of iterations before reaching the stopping criterion.

t_intNumber of weight updates performed during training.
Same as (n_iter_ * n_samples + 1).

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

HuberRegressorLinear regression model that is robust to outliers.

LarsLeast Angle Regression model.

LassoLinear Model trained with L1 prior as regularizer.

RANSACRegressorRANSAC (RANdom SAmple Consensus) algorithm.

RidgeLinear least squares with l2 regularization.

sklearn.svm.SVREpsilon-Support Vector Regression.

TheilSenRegressorTheil-Sen Estimator robust multivariate regression model.



Examples
>>> import numpy as np
>>> from sklearn.linear_model import SGDRegressor
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> n_samples, n_features = 10, 5
>>> rng = np.random.RandomState(0)
>>> y = rng.randn(n_samples)
>>> X = rng.randn(n_samples, n_features)
>>> # Always scale the input. The most convenient way is to use a pipeline.
>>> reg = make_pipeline(StandardScaler(),
...                     SGDRegressor(max_iter=1000, tol=1e-3))
>>> reg.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('sgdregressor', SGDRegressor())])




densify()[source]#
Convert coefficient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the
default format of coef_ and is required for fitting, so calling
this method is only required on models that have previously been
sparsified; otherwise, it is a no-op.

Returns:

selfFitted estimator.







fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)[source]#
Fit linear model with Stochastic Gradient Descent.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Training data.

yndarray of shape (n_samples,)Target values.

coef_initndarray of shape (n_features,), default=NoneThe initial coefficients to warm-start the optimization.

intercept_initndarray of shape (1,), default=NoneThe initial intercept to warm-start the optimization.

sample_weightarray-like, shape (n_samples,), default=NoneWeights applied to individual samples (1. for unweighted).



Returns:

selfobjectFitted SGDRegressor estimator.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







partial_fit(X, y, sample_weight=None)[source]#
Perform one epoch of stochastic gradient descent on given samples.
Internally, this method uses max_iter = 1. Therefore, it is not
guaranteed that a minimum of the cost function is reached after calling
it once. Matters such as objective convergence and early stopping
should be handled by the user.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Subset of training data.

ynumpy array of shape (n_samples,)Subset of target values.

sample_weightarray-like, shape (n_samples,), default=NoneWeights applied to individual samples.
If not provided, uniform weights are assumed.



Returns:

selfobjectReturns an instance of self.







predict(X)[source]#
Predict using the linear model.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Input data.



Returns:

ndarray of shape (n_samples,)Predicted target values per element in X.







score(X, y, sample_weight=None)[source]#
Return the coefficient of determination of the prediction.
The coefficient of determination \(R^2\) is defined as
\((1 - \frac{u}{v})\), where \(u\) is the residual
sum of squares ((y_true - y_pred)** 2).sum() and \(v\)
is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of y, disregarding the input features, would get
a \(R^2\) score of 0.0.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples. For some estimators this may be a precomputed
kernel matrix or a list of generic objects instead with shape
(n_samples, n_samples_fitted), where n_samples_fitted
is the number of samples used in the fitting for the estimator.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True values for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloat\(R^2\) of self.predict(X) w.r.t. y.




Notes
The \(R^2\) score used when calling score on a regressor uses
multioutput='uniform_average' from version 0.23 to keep consistent
with default value of r2_score.
This influences the score method of all the multioutput
regressors (except for
MultiOutputRegressor).



set_fit_request(*, coef_init: bool | None | str = '$UNCHANGED$', intercept_init: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → SGDRegressor[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

coef_initstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for coef_init parameter in fit.

intercept_initstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for intercept_init parameter in fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_partial_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → SGDRegressor[source]#
Request metadata passed to the partial_fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to partial_fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to partial_fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in partial_fit.



Returns:

selfobjectThe updated object.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → SGDRegressor[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







sparsify()[source]#
Convert coefficient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for
L1-regularized models can be much more memory- and storage-efficient
than the usual numpy.ndarray representation.
The intercept_ member is not converted.

Returns:

selfFitted estimator.




Notes
For non-sparse models, i.e. when there are not many zeros in coef_,
this may actually increase memory usage, so use this method with
care. A rule of thumb is that the number of zero elements, which can
be computed with (coef_ == 0).sum(), must be more than 50% for this
to provide significant benefits.
After calling this method, further fitting with the partial_fit
method (if any) will not work until you call densify.



Gallery examples#

Prediction Latency
Prediction Latency

SGD: Penalties
SGD: Penalties










previous
RidgeCV




next
ElasticNet










 On this page
  


SGDRegressor
densify
fit
get_metadata_routing
get_params
partial_fit
predict
score
set_fit_request
set_params
set_partial_fit_request
set_score_request
sparsify


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Stochastic Gradient Descent Regression,Stochastic Gradient Descent Regressor. Based on scikit-learn. Stochastic Gradient Descent is a extremely scalable method for fitting linear regression models to big data.
Ridge Regression. This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Based on scikit-learn Ridge,http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge,Ridge Regression,RIDGE,Regression,"













Ridge — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.linear_model
Ridge









Ridge#


class sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=None, tol=0.0001, solver='auto', positive=False, random_state=None)[source]#
Linear least squares with l2 regularization.
Minimizes the objective function:
||y - Xw||^2_2 + alpha * ||w||^2_2


This model solves a regression model where the loss function is
the linear least squares function and regularization is given by
the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
This estimator has built-in support for multi-variate regression
(i.e., when y is a 2d-array of shape (n_samples, n_targets)).
Read more in the User Guide.

Parameters:

alpha{float, ndarray of shape (n_targets,)}, default=1.0Constant that multiplies the L2 term, controlling regularization
strength. alpha must be a non-negative float i.e. in [0, inf).
When alpha = 0, the objective is equivalent to ordinary least
squares, solved by the LinearRegression object. For numerical
reasons, using alpha = 0 with the Ridge object is not advised.
Instead, you should use the LinearRegression object.
If an array is passed, penalties are assumed to be specific to the
targets. Hence they must correspond in number.

fit_interceptbool, default=TrueWhether to fit the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. X and y are expected to be centered).

copy_Xbool, default=TrueIf True, X will be copied; else, it may be overwritten.

max_iterint, default=NoneMaximum number of iterations for conjugate gradient solver.
For ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined
by scipy.sparse.linalg. For ‘sag’ solver, the default value is 1000.
For ‘lbfgs’ solver, the default value is 15000.

tolfloat, default=1e-4The precision of the solution (coef_) is determined by tol which
specifies a different convergence criterion for each solver:

‘svd’: tol has no impact.
‘cholesky’: tol has no impact.
‘sparse_cg’: norm of residuals smaller than tol.
‘lsqr’: tol is set as atol and btol of scipy.sparse.linalg.lsqr,
which control the norm of the residual vector in terms of the norms of
matrix and coefficients.
‘sag’ and ‘saga’: relative change of coef smaller than tol.
‘lbfgs’: maximum of the absolute (projected) gradient=max|residuals|
smaller than tol.


Changed in version 1.2: Default value changed from 1e-3 to 1e-4 for consistency with other linear
models.


solver{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’,             ‘sag’, ‘saga’, ‘lbfgs’}, default=’auto’Solver to use in the computational routines:

‘auto’ chooses the solver automatically based on the type of data.
‘svd’ uses a Singular Value Decomposition of X to compute the Ridge
coefficients. It is the most stable solver, in particular more stable
for singular matrices than ‘cholesky’ at the cost of being slower.
‘cholesky’ uses the standard scipy.linalg.solve function to
obtain a closed-form solution.
‘sparse_cg’ uses the conjugate gradient solver as found in
scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
more appropriate than ‘cholesky’ for large-scale data
(possibility to set tol and max_iter).
‘lsqr’ uses the dedicated regularized least-squares routine
scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
procedure.
‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses
its improved, unbiased version named SAGA. Both methods also use an
iterative procedure, and are often faster than other solvers when
both n_samples and n_features are large. Note that ‘sag’ and
‘saga’ fast convergence is only guaranteed on features with
approximately the same scale. You can preprocess the data with a
scaler from sklearn.preprocessing.
‘lbfgs’ uses L-BFGS-B algorithm implemented in
scipy.optimize.minimize. It can be used only when positive
is True.

All solvers except ‘svd’ support both dense and sparse data. However, only
‘lsqr’, ‘sag’, ‘sparse_cg’, and ‘lbfgs’ support sparse input when
fit_intercept is True.

Added in version 0.17: Stochastic Average Gradient descent solver.


Added in version 0.19: SAGA solver.


positivebool, default=FalseWhen set to True, forces the coefficients to be positive.
Only ‘lbfgs’ solver is supported in this case.

random_stateint, RandomState instance, default=NoneUsed when solver == ‘sag’ or ‘saga’ to shuffle the data.
See Glossary for details.

Added in version 0.17: random_state to support Stochastic Average Gradient.




Attributes:

coef_ndarray of shape (n_features,) or (n_targets, n_features)Weight vector(s).

intercept_float or ndarray of shape (n_targets,)Independent term in decision function. Set to 0.0 if
fit_intercept = False.

n_iter_None or ndarray of shape (n_targets,)Actual number of iterations for each target. Available only for
sag and lsqr solvers. Other solvers will return None.

Added in version 0.17.


n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


solver_strThe solver that was used at fit time by the computational
routines.

Added in version 1.5.






See also

RidgeClassifierRidge classifier.

RidgeCVRidge regression with built-in cross validation.

KernelRidgeKernel ridge regression combines ridge regression with the kernel trick.



Notes
Regularization improves the conditioning of the problem and
reduces the variance of the estimates. Larger values specify stronger
regularization. Alpha corresponds to 1 / (2C) in other linear
models such as LogisticRegression or
LinearSVC.
Examples
>>> from sklearn.linear_model import Ridge
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> rng = np.random.RandomState(0)
>>> y = rng.randn(n_samples)
>>> X = rng.randn(n_samples, n_features)
>>> clf = Ridge(alpha=1.0)
>>> clf.fit(X, y)
Ridge()




fit(X, y, sample_weight=None)[source]#
Fit Ridge regression model.

Parameters:

X{ndarray, sparse matrix} of shape (n_samples, n_features)Training data.

yndarray of shape (n_samples,) or (n_samples, n_targets)Target values.

sample_weightfloat or ndarray of shape (n_samples,), default=NoneIndividual weights for each sample. If given a float, every sample
will have the same weight.



Returns:

selfobjectFitted estimator.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict using the linear model.

Parameters:

Xarray-like or sparse matrix, shape (n_samples, n_features)Samples.



Returns:

Carray, shape (n_samples,)Returns predicted values.







score(X, y, sample_weight=None)[source]#
Return the coefficient of determination of the prediction.
The coefficient of determination \(R^2\) is defined as
\((1 - \frac{u}{v})\), where \(u\) is the residual
sum of squares ((y_true - y_pred)** 2).sum() and \(v\)
is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of y, disregarding the input features, would get
a \(R^2\) score of 0.0.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples. For some estimators this may be a precomputed
kernel matrix or a list of generic objects instead with shape
(n_samples, n_samples_fitted), where n_samples_fitted
is the number of samples used in the fitting for the estimator.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True values for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloat\(R^2\) of self.predict(X) w.r.t. y.




Notes
The \(R^2\) score used when calling score on a regressor uses
multioutput='uniform_average' from version 0.23 to keep consistent
with default value of r2_score.
This influences the score method of all the multioutput
regressors (except for
MultiOutputRegressor).



set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → Ridge[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → Ridge[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







Gallery examples#

Compressive sensing: tomography reconstruction with L1 prior (Lasso)
Compressive sensing: tomography reconstruction with L1 prior (Lasso)

Prediction Latency
Prediction Latency

Comparison of kernel ridge and Gaussian process regression
Comparison of kernel ridge and Gaussian process regression

HuberRegressor vs Ridge on dataset with strong outliers
HuberRegressor vs Ridge on dataset with strong outliers

L1-based models for Sparse Signals
L1-based models for Sparse Signals

Ordinary Least Squares and Ridge Regression Variance
Ordinary Least Squares and Ridge Regression Variance

Plot Ridge coefficients as a function of the regularization
Plot Ridge coefficients as a function of the regularization

Poisson regression and non-normal loss
Poisson regression and non-normal loss

Polynomial and Spline interpolation
Polynomial and Spline interpolation

Ridge coefficients as a function of the L2 Regularization
Ridge coefficients as a function of the L2 Regularization

Common pitfalls in the interpretation of coefficients of linear models
Common pitfalls in the interpretation of coefficients of linear models

Imputing missing values with variants of IterativeImputer
Imputing missing values with variants of IterativeImputer

Target Encoder’s Internal Cross fitting
Target Encoder's Internal Cross fitting










previous
LinearRegression




next
RidgeCV










 On this page
  


Ridge
fit
get_metadata_routing
get_params
predict
score
set_fit_request
set_params
set_score_request


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Ridge Regression,Ridge Regression. This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Based on scikit-learn Ridge
Gaussian Process Regression with Rational Quadratic Kernel. The Rational Quadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized  by a length-scale parameter length_scale > 0 and a scale mixture parameter alpha > 0. Only the isotropic variant where length_scale is a scalar is supported at the moment.,https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RationalQuadratic.html,Gaussian Process Regression with Rational Quadratic Kernel,GPRRQ,Regression,"













RationalQuadratic — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.gaussian_process
RationalQuadratic









RationalQuadratic#


class sklearn.gaussian_process.kernels.RationalQuadratic(length_scale=1.0, alpha=1.0, length_scale_bounds=(1e-05, 100000.0), alpha_bounds=(1e-05, 100000.0))[source]#
Rational Quadratic kernel.
The RationalQuadratic kernel can be seen as a scale mixture (an infinite
sum) of RBF kernels with different characteristic length scales. It is
parameterized by a length scale parameter \(l>0\) and a scale
mixture parameter \(\alpha>0\). Only the isotropic variant
where length_scale \(l\) is a scalar is supported at the moment.
The kernel is given by:

\[k(x_i, x_j) = \left(
1 + \frac{d(x_i, x_j)^2 }{ 2\alpha  l^2}\right)^{-\alpha}\]
where \(\alpha\) is the scale mixture parameter, \(l\) is
the length scale of the kernel and \(d(\cdot,\cdot)\) is the
Euclidean distance.
For advice on how to set the parameters, see e.g. [1].
Read more in the User Guide.

Added in version 0.18.


Parameters:

length_scalefloat > 0, default=1.0The length scale of the kernel.

alphafloat > 0, default=1.0Scale mixture parameter

length_scale_boundspair of floats >= 0 or “fixed”, default=(1e-5, 1e5)The lower and upper bound on ‘length_scale’.
If set to “fixed”, ‘length_scale’ cannot be changed during
hyperparameter tuning.

alpha_boundspair of floats >= 0 or “fixed”, default=(1e-5, 1e5)The lower and upper bound on ‘alpha’.
If set to “fixed”, ‘alpha’ cannot be changed during
hyperparameter tuning.




References


[1]
David Duvenaud (2014). “The Kernel Cookbook:
Advice on Covariance functions”.


Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn.gaussian_process import GaussianProcessClassifier
>>> from sklearn.gaussian_process.kernels import RationalQuadratic
>>> X, y = load_iris(return_X_y=True)
>>> kernel = RationalQuadratic(length_scale=1.0, alpha=1.5)
>>> gpc = GaussianProcessClassifier(kernel=kernel,
...         random_state=0).fit(X, y)
>>> gpc.score(X, y)
0.9733...
>>> gpc.predict_proba(X[:2,:])
array([[0.8881..., 0.0566..., 0.05518...],
        [0.8678..., 0.0707... , 0.0614...]])




__call__(X, Y=None, eval_gradient=False)[source]#
Return the kernel k(X, Y) and optionally its gradient.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)

Yndarray of shape (n_samples_Y, n_features), default=NoneRight argument of the returned kernel k(X, Y). If None, k(X, X)
if evaluated instead.

eval_gradientbool, default=FalseDetermines whether the gradient with respect to the log of
the kernel hyperparameter is computed.
Only supported when Y is None.



Returns:

Kndarray of shape (n_samples_X, n_samples_Y)Kernel k(X, Y)

K_gradientndarray of shape (n_samples_X, n_samples_X, n_dims)The gradient of the kernel k(X, X) with respect to the log of the
hyperparameter of the kernel. Only returned when eval_gradient
is True.







property bounds#
Returns the log-transformed bounds on the theta.

Returns:

boundsndarray of shape (n_dims, 2)The log-transformed bounds on the kernel’s hyperparameters theta







clone_with_theta(theta)[source]#
Returns a clone of self with given hyperparameters theta.

Parameters:

thetandarray of shape (n_dims,)The hyperparameters







diag(X)[source]#
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however,
it can be evaluated more efficiently since only the diagonal is
evaluated.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)



Returns:

K_diagndarray of shape (n_samples_X,)Diagonal of kernel k(X, X)







get_params(deep=True)[source]#
Get parameters of this kernel.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







property hyperparameters#
Returns a list of all hyperparameter specifications.



is_stationary()[source]#
Returns whether the kernel is stationary.



property n_dims#
Returns the number of non-fixed hyperparameters of the kernel.



property requires_vector_input#
Returns whether the kernel is defined on fixed-length feature
vectors or generic objects. Defaults to True for backward
compatibility.



set_params(**params)[source]#
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels.
The latter have parameters of the form <component>__<parameter>
so that it’s possible to update each component of a nested object.

Returns:

self






property theta#
Returns the (flattened, log-transformed) non-fixed hyperparameters.
Note that theta are typically the log-transformed values of the
kernel’s hyperparameters as this representation of the search space
is more amenable for hyperparameter search, as hyperparameters like
length-scales naturally live on a log-scale.

Returns:

thetandarray of shape (n_dims,)The non-fixed, log-transformed hyperparameters of the kernel







Gallery examples#

Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)
Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)

Illustration of prior and posterior Gaussian process for different kernels
Illustration of prior and posterior Gaussian process for different kernels










previous
RBF




next
Sum










 On this page
  


RationalQuadratic
__call__
bounds
clone_with_theta
diag
get_params
hyperparameters
is_stationary
n_dims
requires_vector_input
set_params
theta


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gaussian Process Regressor with Rational Quadratic Kernel,Gaussian Process Regression with Rational Quadratic Kernel. The Rational Quadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized  by a length-scale parameter length_scale > 0 and a scale mixture parameter alpha > 0. Only the isotropic variant where length_scale is a scalar is supported at the moment.
Gradient Boosting Regressor (xgboost) with Early-Stopping and Unsupervised Learning features. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Regressor (xgboost) with Early-Stopping and Unsupervised Learning features,UESXGBR2,Regression,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Regressor with Early Stopping and Unsupervised Learning Features,Gradient Boosting Regressor (xgboost) with Early-Stopping and Unsupervised Learning features. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
"Lasso Regression. The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent. Based on lightning CDRegressor",http://contrib.scikit-learn.org/lightning/,Lasso Regression,LASSO2,Regression,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Lasso Regression,"Lasso Regression. The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent. Based on lightning CDRegressor"
Generalized Linear Model. The generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. . Based on lightning CDRegressor,http://contrib.scikit-learn.org/lightning/,Generalized Linear Model,GLMCD,Regression,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Linear Regression,Generalized Linear Model. The generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. . Based on lightning CDRegressor
"Bin numerical values into non-uniform bins using decision trees, followed by Elasticnet model using block coordinate descent-- a common form of derivated-free optimization. Based on lightning CDRegressor.",http://contrib.scikit-learn.org/lightning/,"Bin numerical values into non-uniform bins using decision trees, followed by Elasticnet model using block coordinate descent-- a common form of derivated-free optimization",BENETCD2,Regression,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Elastic-Net Regressor (L1 / Least-Squares Loss) with Binned numeric features,"Bin numerical values into non-uniform bins using decision trees, followed by Elasticnet model using block coordinate descent-- a common form of derivated-free optimization. Based on lightning CDRegressor."
Tunes character n-grams and generates out-of-sample predictions.,no url,Tunes character n-grams and generates out-of-sample predictions,CNGER2,Regression,no documentation retrieved,NUM,Auto-Tuned Char N-Gram Text Modeler using token counts,Tunes character n-grams and generates out-of-sample predictions.
Elasticnet model using block coordinate descent-- a common form of derivative-free optimization. Based on lightning CDRegressor.,http://contrib.scikit-learn.org/lightning/,Elasticnet model using block coordinate descent-- a common form of derivative-free optimization,ENETCD,Regression,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Elastic-Net Regressor (L1 / Least-Squares Loss),Elasticnet model using block coordinate descent-- a common form of derivative-free optimization. Based on lightning CDRegressor.
Auto Tuned Elasticnet Regressor for Summarized categorical with Category cloud,no url,Auto Tuned Elasticnet Regressor for Summarized categorical with Category cloud,SCENETR,Regression,no documentation retrieved,NUM,Auto-Tuned Summarized Categorical Regressor,Auto Tuned Elasticnet Regressor for Summarized categorical with Category cloud
"LightGBM Dropout Additive Regression Trees Regressor
    
    LightGBM is a gradient boosting framework. It uses a tree-based algorithm and is designed to be
    distributed and efficient, providing the following advantages:
    
    #. Faster training speed and higher efficiency
    #. Lower memory usage
    #. Better accuracy
    #. Support for parallel learning
    #. Handling of large-scale data
    
    DART (Dropout Additive Regression Trees, proposed by Rasmi et al.) is a novel way of employing
    dropouts in Gradient Boosted Trees. It results in the Dropout Additive Regression Trees algorithm.
    By employing dropout techniques commonly used by deep neural nets, Rasmi et al. showed improve
    Gradient Boosted Trees results (in some situations).
    
    **Gradient Boosting Machines:**
    
    Gradient Boosting Machines (or Generalized Boosted Models, depending on who you
    ask to explain the acronym 'GBM') are an advanced algorithm for fitting
    extremely accurate predictive models. GBMs have won a number of recent predictive
    modeling competitions and are considered by many data scientists to be the
    most versatile and useful predictive modeling algorithm. GBMs require very
    little preprocessing, elegantly handle missing data, strike a good balance between
    bias and variance, and are typically able to find complicated interaction terms, making them a
    useful ""Swiss army knife"" of predictive models.
    
    GBMs are a generalization of Freund and Schapire's adaboost algorithm (1995) that handles
    arbitrary loss functions. They are very similar in concept to random forests, in that
    they fit individual decision trees to random re-samples of input data, where each
    tree sees a bootstrap sample of the rows of the dataset and N arbitrarily chosen
    columns, where N is a configurable parameter of the model. GBMs differ from random
    forests in a single major aspect: rather than fitting the trees independently, the
    GBM fits each successive tree to the residual errors from all the previous trees
    combined. This is advantageous, as the model focuses each iteration on the examples
    that are most difficult to predict (and therefore most useful to get correct).
    
    Due to their iterative nature, GBMs are almost guaranteed to overfit the training data,
    given enough iterations. Therefore, the 2 critical parameters of the algorithm are the
    learning rate (or how fast the model fits the data) and the number of trees the model
    is allowed to fit. It is critical to tune one of these 2 parameters, and
    when done correctly, GBMs are capable of finding the exact point in the training data
    where overfitting begins, and halt one iteration prior to that point. In this manner GBMs
    are usually capable of squeezing every last bit of information out of the training
    set and producing a model with the highest possible accuracy without overfitting.
    
    Parameters
    ----------
    learning_rate (lr): floatgrid (default='0.1')
        Shrink the contribution of each tree by `learning_rate`.
        There is a trade-off between learning_rate (lr) and n_estimators(n).
        In dart, it also affects normalization
        ``values: [1e-7, 1e2]``
    n_estimators (n): intgrid (default='10')
        Number of boosting stages to perform.
        Gradient boosting is fairly robust to overfitting so a large number usually results in better
        performance.
        ``values: [1, 1e6]``
    num_leaves (nl): intgrid (default='31')
        Number of leaves in one tree.
        ``values: [2, 1e4]``
    max_depth (md): intgrid (default='none')
        Maximum depth of the individual regression estimators.
        The maximum depth limits the number of nodes in the tree. Tune this parameter
        for best performance; the best value depends on the interaction
        of the input variables. Deeper the tree the more variable interactions
        the model can capture. Tree still grow by leaf-wise.
        <0 means no limit
        ``values: ['none', [1, 1e4]]``
    max_bin (mb): intgrid (default='255')
        Max number of bin that feature values will bucket in.
        Small bin may reduce training accuracy but may increase general power (deal with overfit).
        LightGBM will auto compress memory according max_bin. For example, LightGBM will use
        uint8_t for feature value if max_bin=255.
        ``values: [3, 1e4]``
    subsample_for_bin (ssfb): int (default=50000)
        Number of samples for constructing bins.
        ``values: [1, 1e6]``
    min_split_gain (msg): floatgrid (default='0')
        Minimum loss reduction required to make a further partition on a leaf node of the tree.
        ``values: [0, 100]``
    min_child_weight (mcw): intgrid (default='5')
        Minimum sum of instance weight(hessian) needed in a child(leaf).
        ``values: [0, 1e2]``
    min_child_samples (mcs): int (default='10')
        Minimum number of data need in a child(leaf).
        ``values: [0, 1e3]``
    subsample (ss): floatgrid (default='1.0')
        Subsample ratio of the training instance.
        ``values: [0.01, 1]``
    subsample_freq (ssf): intgrid (default='1')
        Frequency of subsample 'none' means it is not enabled.
        ``values: ['none', [1, 1e3]]``
    colsample_bytree (cbt): floatgrid (default='1.0')
        Subsample ratio of columns when constructing each tree.
        By default, the value of ``colsample_bytree`` for LightGBM classes is 1.0. However, based on the
        training data, DataRobot may choose a different initial value for this parameter.
        ``values: [0, 1]``
    reg_alpha (ra): floatgrid (default='0')
        L1 regularization term on weights.
        ``values: [0, 1e6]``
    reg_lambda (rl): floatgrid (default='0')
        L2 regularization term on weights.
        ``values: [0, 1e6]``
    objective (obj): select (default='regression')
        Objective function to be optimized.
        ``values: ['regression_l1', 'regression_l2', 'huber', 'fair', 'poisson', 'gamma', 'tweedie']``
    huber_delta (hd): floatgrid (default='1.0')
        Parameter for Huber loss.
        ``values: [0, 1e3]``
    fair_c (c): floatgrid (default='1.0')
        Parameter for Fair loss.
        ``values: [0, 1e3]``
    max_delta_step (pmds): floatgrid (default='0.7')
        Parameter used to safeguard optimization.
        The higher, the more conservative the increments are.
        ``values: [0, 1e3]``
    tweedie_p (p): floatgrid (default='1.5')
        Parameter for Tweedie loss.
        ``values: [1, 2]``
    boost_from_average (bfa): select (default=True)
        Adjust initial score to the mean of labels for faster convergence.
        ``values: [True, False]``
    drop_rate (dr): floatgrid (default='0.1')
        Dropout rate.
        ``values: [0, 1]``
    skip_drop (sd): floatgrid (default='0.5')
        Probability of skipping drop.
        ``values: [0, 1]``
    max_drop (md): intgrid (default='50')
        Max number of dropped trees on one iteration. <=0 means no limit.
        ``values: ['auto', [1, 1e3]]``
    uniform_drop (ud): select (default=False)
        True if want to use uniform drop.
        ``values: [True, False]``
    xgboost_dart_mode (xdm): select (default=False)
        True if want to use xgboost dart mode.
        ``values: [True, False]``
    drop_seed (ds): intgrid (default='4')
        Used to random seed to choose dropping models.
        ``values: [0, 1e2]``
    
    References
    ----------
    .. [1] Chen, T, and He, T.
       Higgs Boson Discovery with Boosted Trees."" Cowan et al.,
       editor, JMLR: Workshop and Conference Proceedings. No. 42. 2015.
       `[link]
       <http://proceedings.mlr.press/v42/chen14.pdf>`__
    .. [2] Freund, Yoav, and Robert E. Schapire.
       ""A decision-theoretic generalization of on-line learning and an application to boosting.""
       Journal of computer and system sciences
       55.1 (1997): 119-139.
       `[link]
       <https://doi.org/10.1006/jcss.1997.1504>`__
    .. [3] Friedman, Jerome H.
       ""Greedy function approximation: a gradient boosting machine.""
       Annals of statistics (2001): 1189-1232.
       `[link]
       <https://statweb.stanford.edu/~jhf/ftp/trebst.pdf>`__
    .. [4] Breiman, Leo. Arcing the edge.
       Technical Report 486, Statistics Department,
       University of California at Berkeley, 1997.
       `[link]
       <https://www.stat.berkeley.edu/~breiman/arcing-the-edge.pdf>`__
    .. [5] Rashmi Korlakai Vinayak, Ran Gilad-Bachrach.
       ""DART: Dropouts meet Multiple Additive Regression Trees.""
       `[link]
       <http://proceedings.mlr.press/v38/korlakaivinayak15.pdf>`__
    
    See Also
    --------
    Source:
        `LightGBM on GitHub
        <https://github.com/Microsoft/LightGBM>`_
    Source:
        `Gradient boosting wikipedia
        <https://en.wikipedia.org/wiki/Gradient_boosting>`_",no url,LightGBM Dropout Additive Regression Trees Regressor,PLGBMDR,Regression,no documentation retrieved,NUM,Dropout Additive Regression Trees Regressor,Dropout Additive Regression Trees Regressor
"Gaussian Process Regression with Dot-Product Kernel. The Dot-Product Kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0 ** 2. For sigma_0 ** 2 = 0, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous.",https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.DotProduct.html,Gaussian Process Regression with Dot-Product Kernel,GPRDP,Regression,"













DotProduct — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.gaussian_process
DotProduct









DotProduct#


class sklearn.gaussian_process.kernels.DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0))[source]#
Dot-Product kernel.
The DotProduct kernel is non-stationary and can be obtained from linear
regression by putting \(N(0, 1)\) priors on the coefficients
of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\)
on the bias. The DotProduct kernel is invariant to a rotation of
the coordinates about the origin, but not translations.
It is parameterized by a parameter sigma_0 \(\sigma\)
which controls the inhomogenity of the kernel. For \(\sigma_0^2 =0\),
the kernel is called the homogeneous linear kernel, otherwise
it is inhomogeneous. The kernel is given by

\[k(x_i, x_j) = \sigma_0 ^ 2 + x_i \cdot x_j\]
The DotProduct kernel is commonly combined with exponentiation.
See [1], Chapter 4, Section 4.2, for further details regarding the
DotProduct kernel.
Read more in the User Guide.

Added in version 0.18.


Parameters:

sigma_0float >= 0, default=1.0Parameter controlling the inhomogenity of the kernel. If sigma_0=0,
the kernel is homogeneous.

sigma_0_boundspair of floats >= 0 or “fixed”, default=(1e-5, 1e5)The lower and upper bound on ‘sigma_0’.
If set to “fixed”, ‘sigma_0’ cannot be changed during
hyperparameter tuning.




References


[1]
Carl Edward Rasmussen, Christopher K. I. Williams (2006).
“Gaussian Processes for Machine Learning”. The MIT Press.


Examples
>>> from sklearn.datasets import make_friedman2
>>> from sklearn.gaussian_process import GaussianProcessRegressor
>>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel
>>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)
>>> kernel = DotProduct() + WhiteKernel()
>>> gpr = GaussianProcessRegressor(kernel=kernel,
...         random_state=0).fit(X, y)
>>> gpr.score(X, y)
0.3680...
>>> gpr.predict(X[:2,:], return_std=True)
(array([653.0..., 592.1...]), array([316.6..., 316.6...]))




__call__(X, Y=None, eval_gradient=False)[source]#
Return the kernel k(X, Y) and optionally its gradient.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)

Yndarray of shape (n_samples_Y, n_features), default=NoneRight argument of the returned kernel k(X, Y). If None, k(X, X)
if evaluated instead.

eval_gradientbool, default=FalseDetermines whether the gradient with respect to the log of
the kernel hyperparameter is computed.
Only supported when Y is None.



Returns:

Kndarray of shape (n_samples_X, n_samples_Y)Kernel k(X, Y)

K_gradientndarray of shape (n_samples_X, n_samples_X, n_dims),                optionalThe gradient of the kernel k(X, X) with respect to the log of the
hyperparameter of the kernel. Only returned when eval_gradient
is True.







property bounds#
Returns the log-transformed bounds on the theta.

Returns:

boundsndarray of shape (n_dims, 2)The log-transformed bounds on the kernel’s hyperparameters theta







clone_with_theta(theta)[source]#
Returns a clone of self with given hyperparameters theta.

Parameters:

thetandarray of shape (n_dims,)The hyperparameters







diag(X)[source]#
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however,
it can be evaluated more efficiently since only the diagonal is
evaluated.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y).



Returns:

K_diagndarray of shape (n_samples_X,)Diagonal of kernel k(X, X).







get_params(deep=True)[source]#
Get parameters of this kernel.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







property hyperparameters#
Returns a list of all hyperparameter specifications.



is_stationary()[source]#
Returns whether the kernel is stationary.



property n_dims#
Returns the number of non-fixed hyperparameters of the kernel.



property requires_vector_input#
Returns whether the kernel is defined on fixed-length feature
vectors or generic objects. Defaults to True for backward
compatibility.



set_params(**params)[source]#
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels.
The latter have parameters of the form <component>__<parameter>
so that it’s possible to update each component of a nested object.

Returns:

self






property theta#
Returns the (flattened, log-transformed) non-fixed hyperparameters.
Note that theta are typically the log-transformed values of the
kernel’s hyperparameters as this representation of the search space
is more amenable for hyperparameter search, as hyperparameters like
length-scales naturally live on a log-scale.

Returns:

thetandarray of shape (n_dims,)The non-fixed, log-transformed hyperparameters of the kernel







Gallery examples#

Illustration of Gaussian process classification (GPC) on the XOR dataset
Illustration of Gaussian process classification (GPC) on the XOR dataset

Illustration of prior and posterior Gaussian process for different kernels
Illustration of prior and posterior Gaussian process for different kernels

Iso-probability lines for Gaussian Processes classification (GPC)
Iso-probability lines for Gaussian Processes classification (GPC)










previous
ConstantKernel




next
ExpSineSquared










 On this page
  


DotProduct
__call__
bounds
clone_with_theta
diag
get_params
hyperparameters
is_stationary
n_dims
requires_vector_input
set_params
theta


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gaussian Process Regressor with Dot Product Kernel,"Gaussian Process Regression with Dot-Product Kernel. The Dot-Product Kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0 ** 2. For sigma_0 ** 2 = 0, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous."
"Cluster distances based on K-means Clustering, followed by an Elasticnet Regressor model based on block coordinate descent.",http://contrib.scikit-learn.org/lightning/,"Cluster distances based on K-means Clustering, followed by an Elasticnet Regressor model based on block coordinate descent",KMDENETCD,Regression,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Elastic-Net Regressor (L1 / Least-Squares Loss) with K-Means Distance Features,"Cluster distances based on K-means Clustering, followed by an Elasticnet Regressor model based on block coordinate descent."
Gaussian Process Regression with Exponential Since Squared Kernel. The ESS Kernel allows modeling periodic functions. It is parameterized by a length-scale parameter length_scale > 0 and a periodicity parameter periodicity > 0. Only the isotropic variant where l is a scalar is supported at the moment.,https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ExpSineSquared.html,Gaussian Process Regression with Exponential Since Squared Kernel,GPRESS,Regression,"













ExpSineSquared — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.gaussian_process
ExpSineSquared









ExpSineSquared#


class sklearn.gaussian_process.kernels.ExpSineSquared(length_scale=1.0, periodicity=1.0, length_scale_bounds=(1e-05, 100000.0), periodicity_bounds=(1e-05, 100000.0))[source]#
Exp-Sine-Squared kernel (aka periodic kernel).
The ExpSineSquared kernel allows one to model functions which repeat
themselves exactly. It is parameterized by a length scale
parameter \(l>0\) and a periodicity parameter \(p>0\).
Only the isotropic variant where \(l\) is a scalar is
supported at the moment. The kernel is given by:

\[k(x_i, x_j) = \text{exp}\left(-
\frac{ 2\sin^2(\pi d(x_i, x_j)/p) }{ l^ 2} \right)\]
where \(l\) is the length scale of the kernel, \(p\) the
periodicity of the kernel and \(d(\cdot,\cdot)\) is the
Euclidean distance.
Read more in the User Guide.

Added in version 0.18.


Parameters:

length_scalefloat > 0, default=1.0The length scale of the kernel.

periodicityfloat > 0, default=1.0The periodicity of the kernel.

length_scale_boundspair of floats >= 0 or “fixed”, default=(1e-5, 1e5)The lower and upper bound on ‘length_scale’.
If set to “fixed”, ‘length_scale’ cannot be changed during
hyperparameter tuning.

periodicity_boundspair of floats >= 0 or “fixed”, default=(1e-5, 1e5)The lower and upper bound on ‘periodicity’.
If set to “fixed”, ‘periodicity’ cannot be changed during
hyperparameter tuning.




Examples
>>> from sklearn.datasets import make_friedman2
>>> from sklearn.gaussian_process import GaussianProcessRegressor
>>> from sklearn.gaussian_process.kernels import ExpSineSquared
>>> X, y = make_friedman2(n_samples=50, noise=0, random_state=0)
>>> kernel = ExpSineSquared(length_scale=1, periodicity=1)
>>> gpr = GaussianProcessRegressor(kernel=kernel, alpha=5,
...         random_state=0).fit(X, y)
>>> gpr.score(X, y)
0.0144...
>>> gpr.predict(X[:2,:], return_std=True)
(array([425.6..., 457.5...]), array([0.3894..., 0.3467...]))




__call__(X, Y=None, eval_gradient=False)[source]#
Return the kernel k(X, Y) and optionally its gradient.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)

Yndarray of shape (n_samples_Y, n_features), default=NoneRight argument of the returned kernel k(X, Y). If None, k(X, X)
if evaluated instead.

eval_gradientbool, default=FalseDetermines whether the gradient with respect to the log of
the kernel hyperparameter is computed.
Only supported when Y is None.



Returns:

Kndarray of shape (n_samples_X, n_samples_Y)Kernel k(X, Y)

K_gradientndarray of shape (n_samples_X, n_samples_X, n_dims),                 optionalThe gradient of the kernel k(X, X) with respect to the log of the
hyperparameter of the kernel. Only returned when eval_gradient
is True.







property bounds#
Returns the log-transformed bounds on the theta.

Returns:

boundsndarray of shape (n_dims, 2)The log-transformed bounds on the kernel’s hyperparameters theta







clone_with_theta(theta)[source]#
Returns a clone of self with given hyperparameters theta.

Parameters:

thetandarray of shape (n_dims,)The hyperparameters







diag(X)[source]#
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however,
it can be evaluated more efficiently since only the diagonal is
evaluated.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)



Returns:

K_diagndarray of shape (n_samples_X,)Diagonal of kernel k(X, X)







get_params(deep=True)[source]#
Get parameters of this kernel.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







property hyperparameter_length_scale#
Returns the length scale



property hyperparameters#
Returns a list of all hyperparameter specifications.



is_stationary()[source]#
Returns whether the kernel is stationary.



property n_dims#
Returns the number of non-fixed hyperparameters of the kernel.



property requires_vector_input#
Returns whether the kernel is defined on fixed-length feature
vectors or generic objects. Defaults to True for backward
compatibility.



set_params(**params)[source]#
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels.
The latter have parameters of the form <component>__<parameter>
so that it’s possible to update each component of a nested object.

Returns:

self






property theta#
Returns the (flattened, log-transformed) non-fixed hyperparameters.
Note that theta are typically the log-transformed values of the
kernel’s hyperparameters as this representation of the search space
is more amenable for hyperparameter search, as hyperparameters like
length-scales naturally live on a log-scale.

Returns:

thetandarray of shape (n_dims,)The non-fixed, log-transformed hyperparameters of the kernel







Gallery examples#

Comparison of kernel ridge and Gaussian process regression
Comparison of kernel ridge and Gaussian process regression

Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)
Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)

Illustration of prior and posterior Gaussian process for different kernels
Illustration of prior and posterior Gaussian process for different kernels










previous
DotProduct




next
Exponentiation










 On this page
  


ExpSineSquared
__call__
bounds
clone_with_theta
diag
get_params
hyperparameter_length_scale
hyperparameters
is_stationary
n_dims
requires_vector_input
set_params
theta


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gaussian Process Regressor with Exponential Sine Squared Kernel,Gaussian Process Regression with Exponential Since Squared Kernel. The ESS Kernel allows modeling periodic functions. It is parameterized by a length-scale parameter length_scale > 0 and a periodicity parameter periodicity > 0. Only the isotropic variant where l is a scalar is supported at the moment.
Frequency-Severity Generalized Additive Model,no url,Frequency-Severity Generalized Additive Model,FSGG2,Regression,no documentation retrieved,NUM,Frequency-Severity Generalized Additive Model,Frequency-Severity Generalized Additive Model
Statsmodel implementation of quantile linear regression using iterative reweighted least squares,https://www.statsmodels.org/devel/generated/statsmodels.regression.quantile_regression.QuantReg.html,Statsmodel implementation of quantile linear regression using iterative reweighted least squares,QUANTILER,Regression,"






statsmodels.regression.quantile_regression.QuantReg - statsmodels 0.15.0 (+518)





















          Skip to content
        

















            statsmodels 0.15.0 (+518)
          



            
              statsmodels.regression.quantile_regression.QuantReg
            
          

























            Initializing search
          












    statsmodels
  















    statsmodels 0.15.0 (+518)
  






    statsmodels
  





Installing statsmodels




Getting started





User Guide







User Guide




Background





Regression and Linear Models







Regression and Linear Models





Linear Regression







Linear Regression





Module Reference







Module Reference





Model Classes







Model Classes




statsmodels.regression.linear_model.OLS




statsmodels.regression.linear_model.GLS




statsmodels.regression.linear_model.WLS




statsmodels.regression.linear_model.GLSAR




statsmodels.regression.linear_model.yule_walker




statsmodels.regression.linear_model.burg





statsmodels.regression.quantile_regression.QuantReg







statsmodels.regression.quantile_regression.QuantReg





Cstatsmodels.regression.quantile_regression.QuantReg







Cstatsmodels.regression.quantile_regression.QuantReg




statsmodels.regression.quantile_regression.QuantReg.fit




statsmodels.regression.quantile_regression.QuantReg.from_formula




statsmodels.regression.quantile_regression.QuantReg.get_distribution




statsmodels.regression.quantile_regression.QuantReg.hessian




statsmodels.regression.quantile_regression.QuantReg.information




statsmodels.regression.quantile_regression.QuantReg.initialize




statsmodels.regression.quantile_regression.QuantReg.loglike




statsmodels.regression.quantile_regression.QuantReg.predict




statsmodels.regression.quantile_regression.QuantReg.score




statsmodels.regression.quantile_regression.QuantReg.whiten




statsmodels.regression.quantile_regression.QuantReg.df_model




statsmodels.regression.quantile_regression.QuantReg.df_resid




statsmodels.regression.quantile_regression.QuantReg.endog_names




statsmodels.regression.quantile_regression.QuantReg.exog_names











statsmodels.regression.quantile_regression.QuantReg



statsmodels.regression.quantile_regression.QuantReg




      Contents
    



Cstatsmodels.regression.quantile_regression.QuantReg





Parameters




Attributes




statsmodels.regression.quantile_regression.QuantReg.fit




statsmodels.regression.quantile_regression.QuantReg.from_formula




statsmodels.regression.quantile_regression.QuantReg.get_distribution




statsmodels.regression.quantile_regression.QuantReg.hessian




statsmodels.regression.quantile_regression.QuantReg.information




statsmodels.regression.quantile_regression.QuantReg.initialize




statsmodels.regression.quantile_regression.QuantReg.loglike




statsmodels.regression.quantile_regression.QuantReg.predict




statsmodels.regression.quantile_regression.QuantReg.score




statsmodels.regression.quantile_regression.QuantReg.whiten




statsmodels.regression.quantile_regression.QuantReg.df_model




statsmodels.regression.quantile_regression.QuantReg.df_resid




statsmodels.regression.quantile_regression.QuantReg.endog_names




statsmodels.regression.quantile_regression.QuantReg.exog_names










statsmodels.regression.recursive_ls.RecursiveLS




statsmodels.regression.rolling.RollingWLS




statsmodels.regression.rolling.RollingOLS




statsmodels.regression.process_regression.GaussianCovariance




statsmodels.regression.process_regression.ProcessMLE




statsmodels.regression.dimred.SlicedInverseReg




statsmodels.regression.dimred.PrincipalHessianDirections




statsmodels.regression.dimred.SlicedAverageVarianceEstimation







Results Classes










Generalized Linear Models




Generalized Estimating Equations




Generalized Additive Models (GAM)




Robust Linear Models




Linear Mixed Effects Models




Regression with Discrete Dependent Variable




Generalized Linear Mixed Effects Models




ANOVA




Other Models othermod







Time Series Analysis




Other Models




Statistics and Tools




Data Sets




Sandbox







Examples




API Reference




About statsmodels




Developer Page




Release Notes













      Contents
    



Cstatsmodels.regression.quantile_regression.QuantReg





Parameters




Attributes












statsmodels.regression.quantile_regression.QuantReg¶


class statsmodels.regression.quantile_regression.QuantReg(endog, exog, **kwargs)[source]¶
Quantile Regression
Estimate a quantile regression model using iterative reweighted least
squares.

Parameters:¶

endogarray or dataframeendogenous/response variable

exogarray or dataframeexogenous/explanatory variable(s)



Attributes:¶

df_modelThe model degree of freedom.

df_residThe residual degree of freedom.

endog_namesNames of endogenous variables.

exog_namesNames of exogenous variables.




Notes
The Least Absolute Deviation (LAD) estimator is a special case where
quantile is set to 0.5 (q argument of the fit method).
The asymptotic covariance matrix is estimated following the procedure in
Greene (2008, p.407-408), using either the logistic or gaussian kernels
(kernel argument of the fit method).
References
General:

Birkes, D. and Y. Dodge(1993). Alternative Methods of Regression, John Wiley and Sons.
Green,W. H. (2008). Econometric Analysis. Sixth Edition. International Student Edition.
Koenker, R. (2005). Quantile Regression. New York: Cambridge University Press.
LeSage, J. P.(1999). Applied Econometrics Using MATLAB,

Kernels (used by the fit method):

Green (2008) Table 14.2

Bandwidth selection (used by the fit method):

Bofinger, E. (1975). Estimation of a density function using order statistics. Australian Journal of Statistics 17: 1-17.
Chamberlain, G. (1994). Quantile regression, censoring, and the structure of wages. In Advances in Econometrics, Vol. 1: Sixth World Congress, ed. C. A. Sims, 171-209. Cambridge: Cambridge University Press.
Hall, P., and S. Sheather. (1988). On the distribution of the Studentized quantile. Journal of the Royal Statistical Society, Series B 50: 381-391.

Keywords: Least Absolute Deviation(LAD) Regression, Quantile Regression,
Regression, Robust Estimation.
Methods


fit([q, vcov, kernel, bandwidth, max_iter, ...])
Solve by Iterative Weighted Least Squares

from_formula(formula, data[, subset, drop_cols])
Create a Model from a formula and dataframe.

get_distribution(params, scale[, exog, ...])
Construct a random number generator for the predictive distribution.

hessian(params)
The Hessian matrix of the model.

information(params)
Fisher information matrix of model.

initialize()
Initialize model components.

loglike(params)
Log-likelihood of model.

predict(params[, exog])
Return linear predicted values from a design matrix.

score(params)
Score vector of model.

whiten(data)
QuantReg model whitener does nothing: returns data.



Properties


df_model
The model degree of freedom.

df_resid
The residual degree of freedom.

endog_names
Names of endogenous variables.

exog_names
Names of exogenous variables.







    
      Last update:
      Oct 29, 2024
    
  














                Previous
              
              statsmodels.regression.linear_model.burg
            






                Next
              
              statsmodels.regression.quantile_regression.QuantReg.fit
            










        © Copyright 2009-2023, Josef Perktold, Skipper Seabold, Jonathan Taylor, statsmodels-developers.
        
    
  
    Created using
    Sphinx
    7.3.7.
     and
    Sphinx-Immaterial























",NUM,Statsmodels Quantile Regressor,Statsmodel implementation of quantile linear regression using iterative reweighted least squares
"LightGBM Trees Regressor with Grid Search and Early Stopping support
    
    LightGBM is a gradient boosting framework. It uses a tree-based algorithm and is designed to be
    distributed and efficient, providing the following advantages:
    
    #. Faster training speed and higher efficiency
    #. Lower memory usage
    #. Better accuracy
    #. Support for parallel learning
    #. Handling of large-scale data
    
    **Gradient Boosting Machines:**
    
    Gradient Boosting Machines (or Generalized Boosted Models, depending on who you
    ask to explain the acronym 'GBM') are an advanced algorithm for fitting
    extremely accurate predictive models. GBMs have won a number of recent predictive
    modeling competitions and are considered by many data scientists to be the
    most versatile and useful predictive modeling algorithm. GBMs require very
    little preprocessing, elegantly handle missing data, strike a good balance between
    bias and variance, and are typically able to find complicated interaction terms, making them a
    useful ""Swiss army knife"" of predictive models.
    
    GBMs are a generalization of Freund and Schapire's adaboost algorithm (1995) that handles
    arbitrary loss functions. They are very similar in concept to random forests, in that
    they fit individual decision trees to random re-samples of input data, where each
    tree sees a bootstrap sample of the rows of the dataset and N arbitrarily chosen
    columns, where N is a configurable parameter of the model. GBMs differ from random
    forests in a single major aspect: rather than fitting the trees independently, the
    GBM fits each successive tree to the residual errors from all the previous trees
    combined. This is advantageous, as the model focuses each iteration on the examples
    that are most difficult to predict (and therefore most useful to get correct).
    
    Due to their iterative nature, GBMs are almost guaranteed to overfit the training data,
    given enough iterations. Therefore, the 2 critical parameters of the algorithm are the
    learning rate (or how fast the model fits the data) and the number of trees the model
    is allowed to fit. It is critical to tune one of these 2 parameters, and
    when done correctly, GBMs are capable of finding the exact point in the training data
    where overfitting begins, and halt one iteration prior to that point. In this manner GBMs
    are usually capable of squeezing every last bit of information out of the training
    set and producing a model with the highest possible accuracy without overfitting.
    
    **Early Stopping Support:**
    
    Early stopping is a method for determining the number of trees to use
    for a boosted trees model. The training data is split into a training set and a test set, and at
    each iteration the model is scored on the test set. If test set performance decreases for 200
    iterations (tunable in Advanced Tuning), the training procedure stops and the model returns
    the fit from the best tree seen so far. The approach saves time by not continuing past the point
    where it is clear that the model is overfitting and further trees will not result in more accuracy.
    
    
    Note that the early stopping test set uses a 90/10 train/test split *within* the training data
    for a given model. For example, a 64% model on the Leaderboard will internally use 57.6% of the
    data for training, and 6.4% of the data for early stopping. A 100% model on the Leaderboard will
    internally use 90% of the data for training and 10% of the data for early stopping.
    
    Since the early stopping test set was used for early stopping, it cannot be used for training.
    
    This limitation also applies to grid search: within the grid search train/test split, the model will
    use a 90/10 train/test split for early stopping.
    
    **Grid Search Support:**
    
    Grid search is supported in this task. During training, grid search is run to estimate the optimal
    model parameter values that yield the best performance (evaluated by the configured loss function
    ). The grid search runs on a 70/30 train/test split within the training data; the estimated score
    uses 30% of the training data split. After the grid search completes and the best
    tuning parameters are found, the final model is retrained on 100% of training data. Validation
    scores of the final model are different from the validation scores of the grid search.
    
    Grid search is run on the task parameter with one of the following types:
    'intgrid', 'floatgrid', 'listgrid(int)', 'listgrid(float)', 'selectgrid', or 'multi'. Refer to the
    **Parameters** section for details of task parameter definitions.
    
    For each grid search parameter, the search space is defined by the parameter values. Refer to the
    **Parameters** section for details of task parameter definitions.
    
    Parameters
    ----------
    learning_rate (lr): floatgrid (default='0.1')
        Shrink the contribution of each tree by `learning_rate`.
        There is a trade-off between learning_rate (lr) and n_estimators(n).
        In dart, it also affects normalization
        ``values: [1e-7, 1e2]``
    n_estimators (n): intgrid (default='10')
        Number of boosting stages to perform.
        Gradient boosting is fairly robust to overfitting so a large number usually results in better
        performance.
        ``values: [1, 1e6]``
    num_leaves (nl): intgrid (default='31')
        Number of leaves in one tree.
        ``values: [2, 1e4]``
    max_depth (md): intgrid (default='none')
        Maximum depth of the individual regression estimators.
        The maximum depth limits the number of nodes in the tree. Tune this parameter
        for best performance; the best value depends on the interaction
        of the input variables. Deeper the tree the more variable interactions
        the model can capture. Tree still grow by leaf-wise.
        <0 means no limit
        ``values: ['none', [1, 1e4]]``
    max_bin (mb): intgrid (default='255')
        Max number of bin that feature values will bucket in.
        Small bin may reduce training accuracy but may increase general power (deal with overfit).
        LightGBM will auto compress memory according max_bin. For example, LightGBM will use
        uint8_t for feature value if max_bin=255.
        ``values: [3, 1e4]``
    subsample_for_bin (ssfb): int (default=50000)
        Number of samples for constructing bins.
        ``values: [1, 1e6]``
    min_split_gain (msg): floatgrid (default='0')
        Minimum loss reduction required to make a further partition on a leaf node of the tree.
        ``values: [0, 100]``
    min_child_weight (mcw): intgrid (default='5')
        Minimum sum of instance weight(hessian) needed in a child(leaf).
        ``values: [0, 1e2]``
    min_child_samples (mcs): int (default='10')
        Minimum number of data need in a child(leaf).
        ``values: [0, 1e3]``
    subsample (ss): floatgrid (default='1.0')
        Subsample ratio of the training instance.
        ``values: [0.01, 1]``
    subsample_freq (ssf): intgrid (default='1')
        Frequency of subsample 'none' means it is not enabled.
        ``values: ['none', [1, 1e3]]``
    colsample_bytree (cbt): floatgrid (default='1.0')
        Subsample ratio of columns when constructing each tree.
        By default, the value of ``colsample_bytree`` for LightGBM classes is 1.0. However, based on the
        training data, DataRobot may choose a different initial value for this parameter.
        ``values: [0, 1]``
    reg_alpha (ra): floatgrid (default='0')
        L1 regularization term on weights.
        ``values: [0, 1e6]``
    reg_lambda (rl): floatgrid (default='0')
        L2 regularization term on weights.
        ``values: [0, 1e6]``
    objective (obj): select (default='regression')
        Objective function to be optimized.
        ``values: ['regression_l1', 'regression_l2', 'huber', 'fair', 'poisson', 'gamma', 'tweedie']``
    huber_delta (hd): floatgrid (default='1.0')
        Parameter for Huber loss.
        ``values: [0, 1e3]``
    fair_c (c): floatgrid (default='1.0')
        Parameter for Fair loss.
        ``values: [0, 1e3]``
    max_delta_step (pmds): floatgrid (default='0.7')
        Parameter used to safeguard optimization.
        The higher, the more conservative the increments are.
        ``values: [0, 1e3]``
    tweedie_p (p): floatgrid (default='1.5')
        Parameter for Tweedie loss.
        ``values: [1, 2]``
    boost_from_average (bfa): select (default=True)
        Adjust initial score to the mean of labels for faster convergence.
        ``values: [True, False]``
    early_stopping_rounds (esr): int (default='10')
        Will stop training if one metric of one validation data doesn't improve in last
        early_stopping_round rounds.
        ``values: [0, 1e3]``
    
    References
    ----------
    .. [1] Chen, T, and He, T.
       Higgs Boson Discovery with Boosted Trees."" Cowan et al.,
       editor, JMLR: Workshop and Conference Proceedings. No. 42. 2015.
       `[link]
       <http://proceedings.mlr.press/v42/chen14.pdf>`__
    .. [2] Freund, Yoav, and Robert E. Schapire.
       ""A decision-theoretic generalization of on-line learning and an application to boosting.""
       Journal of computer and system sciences
       55.1 (1997): 119-139.
       `[link]
       <https://doi.org/10.1006/jcss.1997.1504>`__
    .. [3] Friedman, Jerome H.
       ""Greedy function approximation: a gradient boosting machine.""
       Annals of statistics (2001): 1189-1232.
       `[link]
       <https://statweb.stanford.edu/~jhf/ftp/trebst.pdf>`__
    .. [4] Breiman, Leo. Arcing the edge.
       Technical Report 486, Statistics Department,
       University of California at Berkeley, 1997.
       `[link]
       <https://www.stat.berkeley.edu/~breiman/arcing-the-edge.pdf>`__
    
    See Also
    --------
    Source:
        `LightGBM on GitHub
        <https://github.com/Microsoft/LightGBM>`_
    Source:
        `Gradient boosting wikipedia
        <https://en.wikipedia.org/wiki/Gradient_boosting>`_",no url,LightGBM Trees Regressor with Grid Search and Early Stopping support,ESLGBMTR,Regression,no documentation retrieved,NUM,Light Gradient Boosted Trees Regressor with Early Stopping,Light GBM Regressor with Early Stopping with GBDT
Frequency-Severity ElasticNet,no url,Frequency-Severity ElasticNet,FSEE,Regression,no documentation retrieved,NUM,Frequency-Severity ElasticNet,Frequency-Severity ElasticNet
Gradient Boosting Regressor (scikit-learn) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor,Gradient Boosting Regressor (scikit-learn) with Early-Stopping,ESGBR2,Regression,"













GradientBoostingRegressor — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.ensemble
GradientBoos...









GradientBoostingRegressor#


class sklearn.ensemble.GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)[source]#
Gradient Boosting for regression.
This estimator builds an additive model in a forward stage-wise fashion; it
allows for the optimization of arbitrary differentiable loss functions. In
each stage a regression tree is fit on the negative gradient of the given
loss function.
HistGradientBoostingRegressor is a much faster variant
of this algorithm for intermediate and large datasets (n_samples >= 10_000) and
supports monotonic constraints.
Read more in the User Guide.

Parameters:

loss{‘squared_error’, ‘absolute_error’, ‘huber’, ‘quantile’},             default=’squared_error’Loss function to be optimized. ‘squared_error’ refers to the squared
error for regression. ‘absolute_error’ refers to the absolute error of
regression and is a robust loss function. ‘huber’ is a
combination of the two. ‘quantile’ allows quantile regression (use
alpha to specify the quantile).

learning_ratefloat, default=0.1Learning rate shrinks the contribution of each tree by learning_rate.
There is a trade-off between learning_rate and n_estimators.
Values must be in the range [0.0, inf).

n_estimatorsint, default=100The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.
Values must be in the range [1, inf).

subsamplefloat, default=1.0The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. subsample interacts with the parameter n_estimators.
Choosing subsample < 1.0 leads to a reduction of variance
and an increase in bias.
Values must be in the range (0.0, 1.0].

criterion{‘friedman_mse’, ‘squared_error’}, default=’friedman_mse’The function to measure the quality of a split. Supported criteria are
“friedman_mse” for the mean squared error with improvement score by
Friedman, “squared_error” for mean squared error. The default value of
“friedman_mse” is generally the best as it can provide a better
approximation in some cases.

Added in version 0.18.


min_samples_splitint or float, default=2The minimum number of samples required to split an internal node:

If int, values must be in the range [2, inf).
If float, values must be in the range (0.0, 1.0] and min_samples_split
will be ceil(min_samples_split * n_samples).


Changed in version 0.18: Added float values for fractions.


min_samples_leafint or float, default=1The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least min_samples_leaf training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.

If int, values must be in the range [1, inf).
If float, values must be in the range (0.0, 1.0) and min_samples_leaf
will be ceil(min_samples_leaf * n_samples).


Changed in version 0.18: Added float values for fractions.


min_weight_fraction_leaffloat, default=0.0The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.
Values must be in the range [0.0, 0.5].

max_depthint or None, default=3Maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
If int, values must be in the range [1, inf).

min_impurity_decreasefloat, default=0.0A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.
Values must be in the range [0.0, inf).
The weighted impurity decrease equation is the following:
N_t / N * (impurity - N_t_R / N_t * right_impurity
                    - N_t_L / N_t * left_impurity)


where N is the total number of samples, N_t is the number of
samples at the current node, N_t_L is the number of samples in the
left child, and N_t_R is the number of samples in the right child.
N, N_t, N_t_R and N_t_L all refer to the weighted sum,
if sample_weight is passed.

Added in version 0.19.


initestimator or ‘zero’, default=NoneAn estimator object that is used to compute the initial predictions.
init has to provide fit and predict. If ‘zero’, the
initial raw predictions are set to zero. By default a
DummyEstimator is used, predicting either the average target value
(for loss=’squared_error’), or a quantile for the other losses.

random_stateint, RandomState instance or None, default=NoneControls the random seed given to each Tree estimator at each
boosting iteration.
In addition, it controls the random permutation of the features at
each split (see Notes for more details).
It also controls the random splitting of the training data to obtain a
validation set if n_iter_no_change is not None.
Pass an int for reproducible output across multiple function calls.
See Glossary.

max_features{‘sqrt’, ‘log2’}, int or float, default=NoneThe number of features to consider when looking for the best split:

If int, values must be in the range [1, inf).
If float, values must be in the range (0.0, 1.0] and the features
considered at each split will be max(1, int(max_features * n_features_in_)).
If “sqrt”, then max_features=sqrt(n_features).
If “log2”, then max_features=log2(n_features).
If None, then max_features=n_features.

Choosing max_features < n_features leads to a reduction of variance
and an increase in bias.
Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than max_features features.

alphafloat, default=0.9The alpha-quantile of the huber loss function and the quantile
loss function. Only if loss='huber' or loss='quantile'.
Values must be in the range (0.0, 1.0).

verboseint, default=0Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.
Values must be in the range [0, inf).

max_leaf_nodesint, default=NoneGrow trees with max_leaf_nodes in best-first fashion.
Best nodes are defined as relative reduction in impurity.
Values must be in the range [2, inf).
If None, then unlimited number of leaf nodes.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See the Glossary.

validation_fractionfloat, default=0.1The proportion of training data to set aside as validation set for
early stopping. Values must be in the range (0.0, 1.0).
Only used if n_iter_no_change is set to an integer.

Added in version 0.20.


n_iter_no_changeint, default=Nonen_iter_no_change is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside validation_fraction size of the training
data as validation and terminate training when validation score is not
improving in all of the previous n_iter_no_change numbers of
iterations.
Values must be in the range [1, inf).
See
Early stopping in Gradient Boosting.

Added in version 0.20.


tolfloat, default=1e-4Tolerance for the early stopping. When the loss is not improving
by at least tol for n_iter_no_change iterations (if set to a
number), the training stops.
Values must be in the range [0.0, inf).

Added in version 0.20.


ccp_alphanon-negative float, default=0.0Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
ccp_alpha will be chosen. By default, no pruning is performed.
Values must be in the range [0.0, inf).
See Minimal Cost-Complexity Pruning for details.

Added in version 0.22.




Attributes:

n_estimators_intThe number of estimators as selected by early stopping (if
n_iter_no_change is specified). Otherwise it is set to
n_estimators.

n_trees_per_iteration_intThe number of trees that are built at each iteration. For regressors, this is
always 1.

Added in version 1.4.0.


feature_importances_ndarray of shape (n_features,)The impurity-based feature importances.

oob_improvement_ndarray of shape (n_estimators,)The improvement in loss on the out-of-bag samples
relative to the previous iteration.
oob_improvement_[0] is the improvement in
loss of the first stage over the init estimator.
Only available if subsample < 1.0.

oob_scores_ndarray of shape (n_estimators,)The full history of the loss values on the out-of-bag
samples. Only available if subsample < 1.0.

Added in version 1.3.


oob_score_floatThe last value of the loss on the out-of-bag samples. It is
the same as oob_scores_[-1]. Only available if subsample < 1.0.

Added in version 1.3.


train_score_ndarray of shape (n_estimators,)The i-th score train_score_[i] is the loss of the
model at iteration i on the in-bag sample.
If subsample == 1 this is the loss on the training data.

init_estimatorThe estimator that provides the initial predictions. Set via the init
argument.

estimators_ndarray of DecisionTreeRegressor of shape (n_estimators, 1)The collection of fitted sub-estimators.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


max_features_intThe inferred value of max_features.





See also

HistGradientBoostingRegressorHistogram-based Gradient Boosting Classification Tree.

sklearn.tree.DecisionTreeRegressorA decision tree regressor.

sklearn.ensemble.RandomForestRegressorA random forest regressor.



Notes
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
max_features=n_features, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
random_state has to be fixed.
References
J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.

Friedman, Stochastic Gradient Boosting, 1999

T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.
Examples
>>> from sklearn.datasets import make_regression
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> from sklearn.model_selection import train_test_split
>>> X, y = make_regression(random_state=0)
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=0)
>>> reg = GradientBoostingRegressor(random_state=0)
>>> reg.fit(X_train, y_train)
GradientBoostingRegressor(random_state=0)
>>> reg.predict(X_test[1:2])
array([-61...])
>>> reg.score(X_test, y_test)
0.4...


For a detailed example of utilizing
GradientBoostingRegressor
to fit an ensemble of weak predictive models, please refer to
Gradient Boosting regression.


apply(X)[source]#
Apply trees in the ensemble to X, return leaf indices.

Added in version 0.17.


Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, its dtype will be converted to
dtype=np.float32. If a sparse matrix is provided, it will
be converted to a sparse csr_matrix.



Returns:

X_leavesarray-like of shape (n_samples, n_estimators)For each datapoint x in X and for each tree in the ensemble,
return the index of the leaf x ends up in each estimator.







property feature_importances_#
The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.
Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
sklearn.inspection.permutation_importance as an alternative.

Returns:

feature_importances_ndarray of shape (n_features,)The values of this array sum to 1, unless all trees are single node
trees consisting of only the root node, in which case it will be an
array of zeros.







fit(X, y, sample_weight=None, monitor=None)[source]#
Fit the gradient boosting model.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.

yarray-like of shape (n_samples,)Target values (strings or integers in classification, real numbers
in regression)
For classification, labels must correspond to classes.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. In the case of
classification, splits are also ignored if they would result in any
single class carrying a negative weight in either child node.

monitorcallable, default=NoneThe monitor is called after each iteration with the current
iteration, a reference to the estimator and the local variables of
_fit_stages as keyword arguments callable(i, self,
locals()). If the callable returns True the fitting procedure
is stopped. The monitor can be used for various things such as
computing held-out estimates, early stopping, model introspect, and
snapshotting.



Returns:

selfobjectFitted estimator.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict regression target for X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

yndarray of shape (n_samples,)The predicted values.







score(X, y, sample_weight=None)[source]#
Return the coefficient of determination of the prediction.
The coefficient of determination \(R^2\) is defined as
\((1 - \frac{u}{v})\), where \(u\) is the residual
sum of squares ((y_true - y_pred)** 2).sum() and \(v\)
is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of y, disregarding the input features, would get
a \(R^2\) score of 0.0.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples. For some estimators this may be a precomputed
kernel matrix or a list of generic objects instead with shape
(n_samples, n_samples_fitted), where n_samples_fitted
is the number of samples used in the fitting for the estimator.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True values for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloat\(R^2\) of self.predict(X) w.r.t. y.




Notes
The \(R^2\) score used when calling score on a regressor uses
multioutput='uniform_average' from version 0.23 to keep consistent
with default value of r2_score.
This influences the score method of all the multioutput
regressors (except for
MultiOutputRegressor).



set_fit_request(*, monitor: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → GradientBoostingRegressor[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

monitorstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for monitor parameter in fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → GradientBoostingRegressor[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







staged_predict(X)[source]#
Predict regression target at each stage for X.
This method allows monitoring (i.e. determine error on testing set)
after each stage.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Yields:

ygenerator of ndarray of shape (n_samples,)The predicted value of the input samples.







Gallery examples#

Early stopping in Gradient Boosting
Early stopping in Gradient Boosting

Gradient Boosting regression
Gradient Boosting regression

Plot individual and voting regression predictions
Plot individual and voting regression predictions

Prediction Intervals for Gradient Boosting Regression
Prediction Intervals for Gradient Boosting Regression

Model Complexity Influence
Model Complexity Influence










previous
GradientBoostingClassifier




next
HistGradientBoostingClassifier










 On this page
  


GradientBoostingRegressor
apply
feature_importances_
fit
get_metadata_routing
get_params
predict
score
set_fit_request
set_params
set_score_request
staged_predict


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gradient Boosted Trees Regressor with Early Stopping (Least-Squares Loss),Gradient Boosting Regressor (scikit-learn) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Eureqa Generalized Additive Model. The Regressor is a surrogate model that approximates Gradient Boosting Machine predictions using Eureqa modeling engine. Eureqa is a proprietary A.I.-powered modeling engine that automates much of the heavy lifting inherent in analytics and data science.,no url,Eureqa Generalized Additive Model,EQ_ESXGBR,Regression,no documentation retrieved,NUM,Eureqa Generalized Additive Model,Eureqa Generalized Additive Model. The Regressor is a surrogate model that approximates Gradient Boosting Machine predictions using Eureqa modeling engine. Eureqa is a proprietary A.I.-powered modeling engine that automates much of the heavy lifting inherent in analytics and data science.
Keras Neural Network Regressor,no url,Keras Neural Network Regressor,KERASR,Regression,no documentation retrieved,NUM,Keras Neural Network Regressor,Keras Neural Network Regressor
Partial Least Squares Regression. Similar to PCA regression. This method projects the variables into a new space. Based on scikit-learn Lasso,http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html,Partial Least Squares Regression,PLS,Regression,"













PLSRegression — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.cross_decomposition
PLSRegression









PLSRegression#


class sklearn.cross_decomposition.PLSRegression(n_components=2, *, scale=True, max_iter=500, tol=1e-06, copy=True)[source]#
PLS regression.
PLSRegression is also known as PLS2 or PLS1, depending on the number of
targets.
For a comparison between other cross decomposition algorithms, see
Compare cross decomposition methods.
Read more in the User Guide.

Added in version 0.8.


Parameters:

n_componentsint, default=2Number of components to keep. Should be in [1, n_features].

scalebool, default=TrueWhether to scale X and Y.

max_iterint, default=500The maximum number of iterations of the power method when
algorithm='nipals'. Ignored otherwise.

tolfloat, default=1e-06The tolerance used as convergence criteria in the power method: the
algorithm stops whenever the squared norm of u_i - u_{i-1} is less
than tol, where u corresponds to the left singular vector.

copybool, default=TrueWhether to copy X and Y in fit before applying centering,
and potentially scaling. If False, these operations will be done
inplace, modifying both arrays.



Attributes:

x_weights_ndarray of shape (n_features, n_components)The left singular vectors of the cross-covariance matrices of each
iteration.

y_weights_ndarray of shape (n_targets, n_components)The right singular vectors of the cross-covariance matrices of each
iteration.

x_loadings_ndarray of shape (n_features, n_components)The loadings of X.

y_loadings_ndarray of shape (n_targets, n_components)The loadings of Y.

x_scores_ndarray of shape (n_samples, n_components)The transformed training samples.

y_scores_ndarray of shape (n_samples, n_components)The transformed training targets.

x_rotations_ndarray of shape (n_features, n_components)The projection matrix used to transform X.

y_rotations_ndarray of shape (n_targets, n_components)The projection matrix used to transform Y.

coef_ndarray of shape (n_target, n_features)The coefficients of the linear model such that Y is approximated as
Y = X @ coef_.T + intercept_.

intercept_ndarray of shape (n_targets,)The intercepts of the linear model such that Y is approximated as
Y = X @ coef_.T + intercept_.

Added in version 1.1.


n_iter_list of shape (n_components,)Number of iterations of the power method, for each
component.

n_features_in_intNumber of features seen during fit.

feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

PLSCanonicalPartial Least Squares transformer and regressor.



Examples
>>> from sklearn.cross_decomposition import PLSRegression
>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]
>>> y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
>>> pls2 = PLSRegression(n_components=2)
>>> pls2.fit(X, y)
PLSRegression()
>>> Y_pred = pls2.predict(X)


For a comparison between PLS Regression and PCA, see
Principal Component Regression vs Partial Least Squares Regression.


fit(X, y=None, Y=None)[source]#
Fit model to data.

Parameters:

Xarray-like of shape (n_samples, n_features)Training vectors, where n_samples is the number of samples and
n_features is the number of predictors.

yarray-like of shape (n_samples,) or (n_samples, n_targets)Target vectors, where n_samples is the number of samples and
n_targets is the number of response variables.

Yarray-like of shape (n_samples,) or (n_samples, n_targets)Target vectors, where n_samples is the number of samples and
n_targets is the number of response variables.

Deprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.




Returns:

selfobjectFitted model.







fit_transform(X, y=None)[source]#
Learn and apply the dimension reduction on the train data.

Parameters:

Xarray-like of shape (n_samples, n_features)Training vectors, where n_samples is the number of samples and
n_features is the number of predictors.

yarray-like of shape (n_samples, n_targets), default=NoneTarget vectors, where n_samples is the number of samples and
n_targets is the number of response variables.



Returns:

selfndarray of shape (n_samples, n_components)Return x_scores if Y is not given, (x_scores, y_scores) otherwise.







get_feature_names_out(input_features=None)[source]#
Get output feature names for transformation.
The feature names out will prefixed by the lowercased class name. For
example, if the transformer outputs 3 features, then the feature names
out are: [""class_name0"", ""class_name1"", ""class_name2""].

Parameters:

input_featuresarray-like of str or None, default=NoneOnly used to validate feature names with the names seen in fit.



Returns:

feature_names_outndarray of str objectsTransformed feature names.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







inverse_transform(X, y=None, Y=None)[source]#
Transform data back to its original space.

Parameters:

Xarray-like of shape (n_samples, n_components)New data, where n_samples is the number of samples
and n_components is the number of pls components.

yarray-like of shape (n_samples,) or (n_samples, n_components)New target, where n_samples is the number of samples
and n_components is the number of pls components.

Yarray-like of shape (n_samples, n_components)New target, where n_samples is the number of samples
and n_components is the number of pls components.

Deprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.




Returns:

X_reconstructedndarray of shape (n_samples, n_features)Return the reconstructed X data.

y_reconstructedndarray of shape (n_samples, n_targets)Return the reconstructed X target. Only returned when y is given.




Notes
This transformation will only be exact if n_components=n_features.



predict(X, copy=True)[source]#
Predict targets of given samples.

Parameters:

Xarray-like of shape (n_samples, n_features)Samples.

copybool, default=TrueWhether to copy X and Y, or perform in-place normalization.



Returns:

y_predndarray of shape (n_samples,) or (n_samples, n_targets)Returns predicted values.




Notes
This call requires the estimation of a matrix of shape
(n_features, n_targets), which may be an issue in high dimensional
space.



score(X, y, sample_weight=None)[source]#
Return the coefficient of determination of the prediction.
The coefficient of determination \(R^2\) is defined as
\((1 - \frac{u}{v})\), where \(u\) is the residual
sum of squares ((y_true - y_pred)** 2).sum() and \(v\)
is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of y, disregarding the input features, would get
a \(R^2\) score of 0.0.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples. For some estimators this may be a precomputed
kernel matrix or a list of generic objects instead with shape
(n_samples, n_samples_fitted), where n_samples_fitted
is the number of samples used in the fitting for the estimator.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True values for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloat\(R^2\) of self.predict(X) w.r.t. y.




Notes
The \(R^2\) score used when calling score on a regressor uses
multioutput='uniform_average' from version 0.23 to keep consistent
with default value of r2_score.
This influences the score method of all the multioutput
regressors (except for
MultiOutputRegressor).



set_output(*, transform=None)[source]#
Set output container.
See Introducing the set_output API
for an example on how to use the API.

Parameters:

transform{“default”, “pandas”, “polars”}, default=NoneConfigure output of transform and fit_transform.

""default"": Default output format of a transformer
""pandas"": DataFrame output
""polars"": Polars output
None: Transform configuration is unchanged


Added in version 1.4: ""polars"" option was added.




Returns:

selfestimator instanceEstimator instance.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_predict_request(*, copy: bool | None | str = '$UNCHANGED$') → PLSRegression[source]#
Request metadata passed to the predict method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to predict if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to predict.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

copystr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for copy parameter in predict.



Returns:

selfobjectThe updated object.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → PLSRegression[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







set_transform_request(*, copy: bool | None | str = '$UNCHANGED$') → PLSRegression[source]#
Request metadata passed to the transform method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to transform if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to transform.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

copystr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for copy parameter in transform.



Returns:

selfobjectThe updated object.







transform(X, y=None, Y=None, copy=True)[source]#
Apply the dimension reduction.

Parameters:

Xarray-like of shape (n_samples, n_features)Samples to transform.

yarray-like of shape (n_samples, n_targets), default=NoneTarget vectors.

Yarray-like of shape (n_samples, n_targets), default=NoneTarget vectors.

Deprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.


copybool, default=TrueWhether to copy X and Y, or perform in-place normalization.



Returns:

x_scores, y_scoresarray-like or tuple of array-likeReturn x_scores if Y is not given, (x_scores, y_scores) otherwise.







Gallery examples#

Compare cross decomposition methods
Compare cross decomposition methods

Principal Component Regression vs Partial Least Squares Regression
Principal Component Regression vs Partial Least Squares Regression










previous
PLSCanonical




next
PLSSVD










 On this page
  


PLSRegression
fit
fit_transform
get_feature_names_out
get_metadata_routing
get_params
inverse_transform
predict
score
set_output
set_params
set_predict_request
set_score_request
set_transform_request
transform


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Partial Least-Squares Regression,Partial Least Squares Regression. Similar to PCA regression. This method projects the variables into a new space. Based on scikit-learn Lasso
Keras Convolutional Neural Network Regressor for Text,no url,Keras Convolutional Neural Network Regressor for Text,KERAS_CNN_TEXT_R,Regression,no documentation retrieved,NUM,Keras Text Convolutional Neural Network Regressor,Keras Convolutional Neural Network Regressor for Text
Gradient Boosting Regressor (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Regressor (xgboost),PXGBR2,Regression,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Regressor,Gradient Boosting Regressor (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
"Regression based on k-nearest neighbors. Neighbors-based regression is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Based on scikit-learn KNeighborsRegressor",http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html,Regression based on k-nearest neighbors,KNNR,Regression,"













KNeighborsRegressor — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.neighbors
KNeighborsRegressor









KNeighborsRegressor#


class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)[source]#
Regression based on k-nearest neighbors.
The target is predicted by local interpolation of the targets
associated of the nearest neighbors in the training set.
Read more in the User Guide.

Added in version 0.9.


Parameters:

n_neighborsint, default=5Number of neighbors to use by default for kneighbors queries.

weights{‘uniform’, ‘distance’}, callable or None, default=’uniform’Weight function used in prediction.  Possible values:

‘uniform’ : uniform weights.  All points in each neighborhood
are weighted equally.
‘distance’ : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.
[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.

Uniform weights are used by default.
See the following example for a demonstration of the impact of
different weighting schemes on predictions:
Nearest Neighbors regression.

algorithm{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’Algorithm used to compute the nearest neighbors:

‘ball_tree’ will use BallTree
‘kd_tree’ will use KDTree
‘brute’ will use a brute-force search.
‘auto’ will attempt to decide the most appropriate algorithm
based on the values passed to fit method.

Note: fitting on sparse input will override the setting of
this parameter, using brute force.

leaf_sizeint, default=30Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.

pfloat, default=2Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.

metricstr, DistanceMetric object or callable, default=’minkowski’Metric to use for distance computation. Default is “minkowski”, which
results in the standard Euclidean distance when p = 2. See the
documentation of scipy.spatial.distance and
the metrics listed in
distance_metrics for valid metric
values.
If metric is “precomputed”, X is assumed to be a distance matrix and
must be square during fit. X may be a sparse graph, in which
case only “nonzero” elements may be considered neighbors.
If metric is a callable function, it takes two arrays representing 1D
vectors as inputs and must return one value indicating the distance
between those vectors. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.
If metric is a DistanceMetric object, it will be passed directly to
the underlying computation routines.

metric_paramsdict, default=NoneAdditional keyword arguments for the metric function.

n_jobsint, default=NoneThe number of parallel jobs to run for neighbors search.
None means 1 unless in a joblib.parallel_backend context.
-1 means using all processors. See Glossary
for more details.
Doesn’t affect fit method.



Attributes:

effective_metric_str or callableThe distance metric to use. It will be same as the metric parameter
or a synonym of it, e.g. ‘euclidean’ if the metric parameter set to
‘minkowski’ and p parameter set to 2.

effective_metric_params_dictAdditional keyword arguments for the metric function. For most metrics
will be same with metric_params parameter, but may also contain the
p parameter value if the effective_metric_ attribute is set to
‘minkowski’.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


n_samples_fit_intNumber of samples in the fitted data.





See also

NearestNeighborsUnsupervised learner for implementing neighbor searches.

RadiusNeighborsRegressorRegression based on neighbors within a fixed radius.

KNeighborsClassifierClassifier implementing the k-nearest neighbors vote.

RadiusNeighborsClassifierClassifier implementing a vote among neighbors within a given radius.



Notes
See Nearest Neighbors in the online documentation
for a discussion of the choice of algorithm and leaf_size.

Warning
Regarding the Nearest Neighbors algorithms, if it is found that two
neighbors, neighbor k+1 and k, have identical distances but
different labels, the results will depend on the ordering of the
training data.

https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm
Examples
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsRegressor
>>> neigh = KNeighborsRegressor(n_neighbors=2)
>>> neigh.fit(X, y)
KNeighborsRegressor(...)
>>> print(neigh.predict([[1.5]]))
[0.5]




fit(X, y)[source]#
Fit the k-nearest neighbors regressor from the training dataset.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric=’precomputed’Training data.

y{array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)Target values.



Returns:

selfKNeighborsRegressorThe fitted k-nearest neighbors regressor.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







kneighbors(X=None, n_neighbors=None, return_distance=True)[source]#
Find the K-neighbors of a point.
Returns indices of and distances to the neighbors of each point.

Parameters:

X{array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == ‘precomputed’, default=NoneThe query point or points.
If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.

n_neighborsint, default=NoneNumber of neighbors required for each sample. The default is the
value passed to the constructor.

return_distancebool, default=TrueWhether or not to return the distances.



Returns:

neigh_distndarray of shape (n_queries, n_neighbors)Array representing the lengths to points, only present if
return_distance=True.

neigh_indndarray of shape (n_queries, n_neighbors)Indices of the nearest points in the population matrix.




Examples
In the following example, we construct a NearestNeighbors
class from an array representing our data set and ask who’s
the closest point to [1,1,1]
>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=1)
>>> neigh.fit(samples)
NearestNeighbors(n_neighbors=1)
>>> print(neigh.kneighbors([[1., 1., 1.]]))
(array([[0.5]]), array([[2]]))


As you can see, it returns [[0.5]], and [[2]], which means that the
element is at distance 0.5 and is the third element of samples
(indexes start at 0). You can also query for multiple points:
>>> X = [[0., 1., 0.], [1., 0., 1.]]
>>> neigh.kneighbors(X, return_distance=False)
array([[1],
       [2]]...)





kneighbors_graph(X=None, n_neighbors=None, mode='connectivity')[source]#
Compute the (weighted) graph of k-Neighbors for points in X.

Parameters:

X{array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == ‘precomputed’, default=NoneThe query point or points.
If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
For metric='precomputed' the shape should be
(n_queries, n_indexed). Otherwise the shape should be
(n_queries, n_features).

n_neighborsint, default=NoneNumber of neighbors for each sample. The default is the value
passed to the constructor.

mode{‘connectivity’, ‘distance’}, default=’connectivity’Type of returned matrix: ‘connectivity’ will return the
connectivity matrix with ones and zeros, in ‘distance’ the
edges are distances between points, type of distance
depends on the selected metric parameter in
NearestNeighbors class.



Returns:

Asparse-matrix of shape (n_queries, n_samples_fit)n_samples_fit is the number of samples in the fitted data.
A[i, j] gives the weight of the edge connecting i to j.
The matrix is of CSR format.





See also

NearestNeighbors.radius_neighbors_graphCompute the (weighted) graph of Neighbors for points in X.



Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=2)
>>> neigh.fit(X)
NearestNeighbors(n_neighbors=2)
>>> A = neigh.kneighbors_graph(X)
>>> A.toarray()
array([[1., 0., 1.],
       [0., 1., 1.],
       [1., 0., 1.]])





predict(X)[source]#
Predict the target for the provided data.

Parameters:

X{array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == ‘precomputed’Test samples.



Returns:

yndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=intTarget values.







score(X, y, sample_weight=None)[source]#
Return the coefficient of determination of the prediction.
The coefficient of determination \(R^2\) is defined as
\((1 - \frac{u}{v})\), where \(u\) is the residual
sum of squares ((y_true - y_pred)** 2).sum() and \(v\)
is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of y, disregarding the input features, would get
a \(R^2\) score of 0.0.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples. For some estimators this may be a precomputed
kernel matrix or a list of generic objects instead with shape
(n_samples, n_samples_fitted), where n_samples_fitted
is the number of samples used in the fitting for the estimator.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True values for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloat\(R^2\) of self.predict(X) w.r.t. y.




Notes
The \(R^2\) score used when calling score on a regressor uses
multioutput='uniform_average' from version 0.23 to keep consistent
with default value of r2_score.
This influences the score method of all the multioutput
regressors (except for
MultiOutputRegressor).



set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → KNeighborsRegressor[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







Gallery examples#

Face completion with a multi-output estimators
Face completion with a multi-output estimators

Imputing missing values with variants of IterativeImputer
Imputing missing values with variants of IterativeImputer

Nearest Neighbors regression
Nearest Neighbors regression










previous
KNeighborsClassifier




next
KNeighborsTransformer










 On this page
  


KNeighborsRegressor
fit
get_metadata_routing
get_params
kneighbors
kneighbors_graph
predict
score
set_params
set_score_request


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Auto-tuned K-Nearest Neighbors Regressor (Euclidean Distance),"Regression based on k-nearest neighbors. Neighbors-based regression is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Based on scikit-learn KNeighborsRegressor"
Random Forests based on scikit-learn. These are built shallowly for TreeShap performance,no url,Random Forests based on scikit-learn,SHAPRFR,Regression,no documentation retrieved,NUM,ExtraTrees Regressor (Shallow),Random Forests based on scikit-learn. These are built shallowly for TreeShap performance
"Gaussian Process Regression with Matern Kernel. The class of Matern kernels is a generalization of the RBF parameterized by a parameter nu that controls exponentiation of the kernel. The smaller the nu used, the less smooth the approximated function is. For nu=0.5, the Matern kernel is equivalent to the Absolute Exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).",https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html,Gaussian Process Regression with Matern Kernel,GPRM,Regression,"













Matern — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.gaussian_process
Matern









Matern#


class sklearn.gaussian_process.kernels.Matern(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0), nu=1.5)[source]#
Matern kernel.
The class of Matern kernels is a generalization of the RBF.
It has an additional parameter \(\nu\) which controls the
smoothness of the resulting function. The smaller \(\nu\),
the less smooth the approximated function is.
As \(\nu\rightarrow\infty\), the kernel becomes equivalent to
the RBF kernel. When \(\nu = 1/2\), the Matérn kernel
becomes identical to the absolute exponential kernel.
Important intermediate values are
\(\nu=1.5\) (once differentiable functions)
and \(\nu=2.5\) (twice differentiable functions).
The kernel is given by:

\[k(x_i, x_j) =  \frac{1}{\Gamma(\nu)2^{\nu-1}}\Bigg(
\frac{\sqrt{2\nu}}{l} d(x_i , x_j )
\Bigg)^\nu K_\nu\Bigg(
\frac{\sqrt{2\nu}}{l} d(x_i , x_j )\Bigg)\]
where \(d(\cdot,\cdot)\) is the Euclidean distance,
\(K_{\nu}(\cdot)\) is a modified Bessel function and
\(\Gamma(\cdot)\) is the gamma function.
See [1], Chapter 4, Section 4.2, for details regarding the different
variants of the Matern kernel.
Read more in the User Guide.

Added in version 0.18.


Parameters:

length_scalefloat or ndarray of shape (n_features,), default=1.0The length scale of the kernel. If a float, an isotropic kernel is
used. If an array, an anisotropic kernel is used where each dimension
of l defines the length-scale of the respective feature dimension.

length_scale_boundspair of floats >= 0 or “fixed”, default=(1e-5, 1e5)The lower and upper bound on ‘length_scale’.
If set to “fixed”, ‘length_scale’ cannot be changed during
hyperparameter tuning.

nufloat, default=1.5The parameter nu controlling the smoothness of the learned function.
The smaller nu, the less smooth the approximated function is.
For nu=inf, the kernel becomes equivalent to the RBF kernel and for
nu=0.5 to the absolute exponential kernel. Important intermediate
values are nu=1.5 (once differentiable functions) and nu=2.5
(twice differentiable functions). Note that values of nu not in
[0.5, 1.5, 2.5, inf] incur a considerably higher computational cost
(appr. 10 times higher) since they require to evaluate the modified
Bessel function. Furthermore, in contrast to l, nu is kept fixed to
its initial value and not optimized.




References


[1]
Carl Edward Rasmussen, Christopher K. I. Williams (2006).
“Gaussian Processes for Machine Learning”. The MIT Press.


Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn.gaussian_process import GaussianProcessClassifier
>>> from sklearn.gaussian_process.kernels import Matern
>>> X, y = load_iris(return_X_y=True)
>>> kernel = 1.0 * Matern(length_scale=1.0, nu=1.5)
>>> gpc = GaussianProcessClassifier(kernel=kernel,
...         random_state=0).fit(X, y)
>>> gpc.score(X, y)
0.9866...
>>> gpc.predict_proba(X[:2,:])
array([[0.8513..., 0.0368..., 0.1117...],
        [0.8086..., 0.0693..., 0.1220...]])




__call__(X, Y=None, eval_gradient=False)[source]#
Return the kernel k(X, Y) and optionally its gradient.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)

Yndarray of shape (n_samples_Y, n_features), default=NoneRight argument of the returned kernel k(X, Y). If None, k(X, X)
if evaluated instead.

eval_gradientbool, default=FalseDetermines whether the gradient with respect to the log of
the kernel hyperparameter is computed.
Only supported when Y is None.



Returns:

Kndarray of shape (n_samples_X, n_samples_Y)Kernel k(X, Y)

K_gradientndarray of shape (n_samples_X, n_samples_X, n_dims),                 optionalThe gradient of the kernel k(X, X) with respect to the log of the
hyperparameter of the kernel. Only returned when eval_gradient
is True.







property bounds#
Returns the log-transformed bounds on the theta.

Returns:

boundsndarray of shape (n_dims, 2)The log-transformed bounds on the kernel’s hyperparameters theta







clone_with_theta(theta)[source]#
Returns a clone of self with given hyperparameters theta.

Parameters:

thetandarray of shape (n_dims,)The hyperparameters







diag(X)[source]#
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however,
it can be evaluated more efficiently since only the diagonal is
evaluated.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)



Returns:

K_diagndarray of shape (n_samples_X,)Diagonal of kernel k(X, X)







get_params(deep=True)[source]#
Get parameters of this kernel.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







property hyperparameters#
Returns a list of all hyperparameter specifications.



is_stationary()[source]#
Returns whether the kernel is stationary.



property n_dims#
Returns the number of non-fixed hyperparameters of the kernel.



property requires_vector_input#
Returns whether the kernel is defined on fixed-length feature
vectors or generic objects. Defaults to True for backward
compatibility.



set_params(**params)[source]#
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels.
The latter have parameters of the form <component>__<parameter>
so that it’s possible to update each component of a nested object.

Returns:

self






property theta#
Returns the (flattened, log-transformed) non-fixed hyperparameters.
Note that theta are typically the log-transformed values of the
kernel’s hyperparameters as this representation of the search space
is more amenable for hyperparameter search, as hyperparameters like
length-scales naturally live on a log-scale.

Returns:

thetandarray of shape (n_dims,)The non-fixed, log-transformed hyperparameters of the kernel







Gallery examples#

Illustration of prior and posterior Gaussian process for different kernels
Illustration of prior and posterior Gaussian process for different kernels










previous
Kernel




next
PairwiseKernel










 On this page
  


Matern
__call__
bounds
clone_with_theta
diag
get_params
hyperparameters
is_stationary
n_dims
requires_vector_input
set_params
theta


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gaussian Process Regressor with Matern Kernel,"Gaussian Process Regression with Matern Kernel. The class of Matern kernels is a generalization of the RBF parameterized by a parameter nu that controls exponentiation of the kernel. The smaller the nu used, the less smooth the approximated function is. For nu=0.5, the Matern kernel is equivalent to the Absolute Exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions)."
"AdaBoost Regressor (scikit-learn). An AdaBoost regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.",http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor,AdaBoost Regressor (scikit-learn),ABR,Regression,"













AdaBoostRegressor — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.ensemble
AdaBoostRegressor









AdaBoostRegressor#


class sklearn.ensemble.AdaBoostRegressor(estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)[source]#
An AdaBoost regressor.
An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
regressor on the original dataset and then fits additional copies of the
regressor on the same dataset but where the weights of instances are
adjusted according to the error of the current prediction. As such,
subsequent regressors focus more on difficult cases.
This class implements the algorithm known as AdaBoost.R2 [2].
Read more in the User Guide.

Added in version 0.14.


Parameters:

estimatorobject, default=NoneThe base estimator from which the boosted ensemble is built.
If None, then the base estimator is
DecisionTreeRegressor initialized with
max_depth=3.

Added in version 1.2: base_estimator was renamed to estimator.


n_estimatorsint, default=50The maximum number of estimators at which boosting is terminated.
In case of perfect fit, the learning procedure is stopped early.
Values must be in the range [1, inf).

learning_ratefloat, default=1.0Weight applied to each regressor at each boosting iteration. A higher
learning rate increases the contribution of each regressor. There is
a trade-off between the learning_rate and n_estimators parameters.
Values must be in the range (0.0, inf).

loss{‘linear’, ‘square’, ‘exponential’}, default=’linear’The loss function to use when updating the weights after each
boosting iteration.

random_stateint, RandomState instance or None, default=NoneControls the random seed given at each estimator at each
boosting iteration.
Thus, it is only used when estimator exposes a random_state.
In addition, it controls the bootstrap of the weights used to train the
estimator at each boosting iteration.
Pass an int for reproducible output across multiple function calls.
See Glossary.



Attributes:

estimator_estimatorThe base estimator from which the ensemble is grown.

Added in version 1.2: base_estimator_ was renamed to estimator_.


estimators_list of regressorsThe collection of fitted sub-estimators.

estimator_weights_ndarray of floatsWeights for each estimator in the boosted ensemble.

estimator_errors_ndarray of floatsRegression error for each estimator in the boosted ensemble.

feature_importances_ndarray of shape (n_features,)The impurity-based feature importances.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

AdaBoostClassifierAn AdaBoost classifier.

GradientBoostingRegressorGradient Boosting Classification Tree.

sklearn.tree.DecisionTreeRegressorA decision tree regressor.



References


[1]
Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of
on-Line Learning and an Application to Boosting”, 1995.


[2]

Drucker, “Improving Regressors using Boosting Techniques”, 1997.



Examples
>>> from sklearn.ensemble import AdaBoostRegressor
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(n_features=4, n_informative=2,
...                        random_state=0, shuffle=False)
>>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)
>>> regr.fit(X, y)
AdaBoostRegressor(n_estimators=100, random_state=0)
>>> regr.predict([[0, 0, 0, 0]])
array([4.7972...])
>>> regr.score(X, y)
0.9771...


For a detailed example of utilizing AdaBoostRegressor
to fit a sequence of decision trees as weak learners, please refer to
Decision Tree Regression with AdaBoost.


property feature_importances_#
The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.
Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
sklearn.inspection.permutation_importance as an alternative.

Returns:

feature_importances_ndarray of shape (n_features,)The feature importances.







fit(X, y, sample_weight=None)[source]#
Build a boosted classifier/regressor from the training set (X, y).

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.

yarray-like of shape (n_samples,)The target values.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights. If None, the sample weights are initialized to
1 / n_samples.



Returns:

selfobjectFitted estimator.







get_metadata_routing()[source]#
Raise NotImplementedError.
This estimator does not support metadata routing yet.



get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict regression value for X.
The predicted regression value of an input sample is computed
as the weighted median prediction of the regressors in the ensemble.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.



Returns:

yndarray of shape (n_samples,)The predicted regression values.







score(X, y, sample_weight=None)[source]#
Return the coefficient of determination of the prediction.
The coefficient of determination \(R^2\) is defined as
\((1 - \frac{u}{v})\), where \(u\) is the residual
sum of squares ((y_true - y_pred)** 2).sum() and \(v\)
is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of y, disregarding the input features, would get
a \(R^2\) score of 0.0.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples. For some estimators this may be a precomputed
kernel matrix or a list of generic objects instead with shape
(n_samples, n_samples_fitted), where n_samples_fitted
is the number of samples used in the fitting for the estimator.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True values for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloat\(R^2\) of self.predict(X) w.r.t. y.




Notes
The \(R^2\) score used when calling score on a regressor uses
multioutput='uniform_average' from version 0.23 to keep consistent
with default value of r2_score.
This influences the score method of all the multioutput
regressors (except for
MultiOutputRegressor).



set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → AdaBoostRegressor[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → AdaBoostRegressor[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







staged_predict(X)[source]#
Return staged predictions for X.
The predicted regression value of an input sample is computed
as the weighted median prediction of the regressors in the ensemble.
This generator method yields the ensemble prediction after each
iteration of boosting and therefore allows monitoring, such as to
determine the prediction on a test set after each boost.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples.



Yields:

ygenerator of ndarray of shape (n_samples,)The predicted regression values.







staged_score(X, y, sample_weight=None)[source]#
Return staged scores for X, y.
This generator method yields the ensemble score after each iteration of
boosting and therefore allows monitoring, such as to determine the
score on a test set after each boost.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.

yarray-like of shape (n_samples,)Labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Yields:

zfloat






Gallery examples#

Decision Tree Regression with AdaBoost
Decision Tree Regression with AdaBoost










previous
AdaBoostClassifier




next
BaggingClassifier










 On this page
  


AdaBoostRegressor
feature_importances_
fit
get_metadata_routing
get_params
predict
score
set_fit_request
set_params
set_score_request
staged_predict
staged_score


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Adaboost Regressor,"AdaBoost Regressor (scikit-learn). An AdaBoost regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases."
Gradient Boosting Regressor (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Regressor (xgboost) with Early-Stopping,ESXGBR2,Regression,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Regressor with Early Stopping,Gradient Boosting Regressor (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
"LightGBM Random Forest Regressor
    
    LightGBM is a gradient boosting framework. It uses a tree-based algorithm and is designed to be
    distributed and efficient, providing the following advantages:
    
    #. Faster training speed and higher efficiency
    #. Lower memory usage
    #. Better accuracy
    #. Support for parallel learning
    #. Handling of large-scale data
    
    LightGBM implements both the gradient boosted trees and random forest algorithms. Random forests are
    an ensemble method where hundreds (or thousands) of individual decision trees are fit to bootstrap
    re-samples of the original dataset, with each tree being allowed to use a random selection of N
    variables, where N is the major configurable parameter of this algorithm.
    
    **Gradient Boosting Machines:**
    
    Gradient Boosting Machines (or Generalized Boosted Models, depending on who you
    ask to explain the acronym 'GBM') are an advanced algorithm for fitting
    extremely accurate predictive models. GBMs have won a number of recent predictive
    modeling competitions and are considered by many data scientists to be the
    most versatile and useful predictive modeling algorithm. GBMs require very
    little preprocessing, elegantly handle missing data, strike a good balance between
    bias and variance, and are typically able to find complicated interaction terms, making them a
    useful ""Swiss army knife"" of predictive models.
    
    GBMs are a generalization of Freund and Schapire's adaboost algorithm (1995) that handles
    arbitrary loss functions. They are very similar in concept to random forests, in that
    they fit individual decision trees to random re-samples of input data, where each
    tree sees a bootstrap sample of the rows of the dataset and N arbitrarily chosen
    columns, where N is a configurable parameter of the model. GBMs differ from random
    forests in a single major aspect: rather than fitting the trees independently, the
    GBM fits each successive tree to the residual errors from all the previous trees
    combined. This is advantageous, as the model focuses each iteration on the examples
    that are most difficult to predict (and therefore most useful to get correct).
    
    Due to their iterative nature, GBMs are almost guaranteed to overfit the training data,
    given enough iterations. Therefore, the 2 critical parameters of the algorithm are the
    learning rate (or how fast the model fits the data) and the number of trees the model
    is allowed to fit. It is critical to tune one of these 2 parameters, and
    when done correctly, GBMs are capable of finding the exact point in the training data
    where overfitting begins, and halt one iteration prior to that point. In this manner GBMs
    are usually capable of squeezing every last bit of information out of the training
    set and producing a model with the highest possible accuracy without overfitting.
    
    Parameters
    ----------
    learning_rate (lr): floatgrid (default='0.1')
        Not used in random forest mode.
        ``values: [1e-7, 1e2]``
    n_estimators (n): intgrid (default='10')
        Number of boosting stages to perform.
        Gradient boosting is fairly robust to overfitting so a large number usually results in better
        performance.
        ``values: [1, 1e6]``
    num_leaves (nl): intgrid (default='31')
        Number of leaves in one tree.
        ``values: [2, 1e4]``
    max_depth (md): intgrid (default='none')
        Maximum depth of the individual regression estimators.
        The maximum depth limits the number of nodes in the tree. Tune this parameter
        for best performance; the best value depends on the interaction
        of the input variables. Deeper the tree the more variable interactions
        the model can capture. Tree still grow by leaf-wise.
        <0 means no limit
        ``values: ['none', [1, 1e4]]``
    max_bin (mb): intgrid (default='255')
        Max number of bin that feature values will bucket in.
        Small bin may reduce training accuracy but may increase general power (deal with overfit).
        LightGBM will auto compress memory according max_bin. For example, LightGBM will use
        uint8_t for feature value if max_bin=255.
        ``values: [3, 1e4]``
    subsample_for_bin (ssfb): int (default=50000)
        Number of samples for constructing bins.
        ``values: [1, 1e6]``
    min_split_gain (msg): floatgrid (default='0')
        Minimum loss reduction required to make a further partition on a leaf node of the tree.
        ``values: [0, 100]``
    min_child_weight (mcw): intgrid (default='5')
        Minimum sum of instance weight(hessian) needed in a child(leaf).
        ``values: [0, 1e2]``
    min_child_samples (mcs): int (default='10')
        Minimum number of data need in a child(leaf).
        ``values: [0, 1e3]``
    subsample (ss): floatgrid (default='1.0')
        Subsample ratio of the training instance.
        ``values: [0.01, 1]``
    subsample_freq (ssf): intgrid (default='1')
        Frequency of subsample 'none' means it is not enabled.
        ``values: ['none', [1, 1e3]]``
    colsample_bytree (cbt): floatgrid (default='1.0')
        Subsample ratio of columns when constructing each tree.
        By default, the value of ``colsample_bytree`` for LightGBM classes is 1.0. However, based on the
        training data, DataRobot may choose a different initial value for this parameter.
        ``values: [0, 1]``
    reg_alpha (ra): floatgrid (default='0')
        L1 regularization term on weights.
        ``values: [0, 1e6]``
    reg_lambda (rl): floatgrid (default='0')
        L2 regularization term on weights.
        ``values: [0, 1e6]``
    objective (obj): select (default='regression')
        Objective function to be optimized.
        ``values: ['regression_l1', 'regression_l2', 'huber', 'fair', 'poisson', 'gamma', 'tweedie']``
    huber_delta (hd): floatgrid (default='1.0')
        Parameter for Huber loss.
        ``values: [0, 1e3]``
    fair_c (c): floatgrid (default='1.0')
        Parameter for Fair loss.
        ``values: [0, 1e3]``
    max_delta_step (pmds): floatgrid (default='0.7')
        Parameter used to safeguard optimization.
        The higher, the more conservative the increments are.
        ``values: [0, 1e3]``
    tweedie_p (p): floatgrid (default='1.5')
        Parameter for Tweedie loss.
        ``values: [1, 2]``
    boost_from_average (bfa): select (default=True)
        Adjust initial score to the mean of labels for faster convergence.
        ``values: [True, False]``
    
    References
    ----------
    .. [1] Breiman, Leo.
       ""Random forests.""
       Machine learning 45.1 (2001): 5-32.
       `[link]
       <http://machinelearning202.pbworks.com/w/file/fetch/60606349/breiman_randomforests.pdf>`__
    .. [2] Liaw, Andy, and Matthew Wiener.
       ""Classification and regression by randomForest.""
       R news 2.3 (2002): 18-22.
       `[link]
       <https://cogns.northwestern.edu/cbmg/LiawAndWiener2002.pdf>`__
    .. [3] Ho, Tin Kam.
       ""Random decision forests."" Document Analysis and Recognition, 1995.,
       Proceedings of the Third International Conference on. Vol. 1. IEEE, 1995.
       `[link]
       <https://doi.org/10.1109/ICDAR.1995.598994>`__
    .. [4] Geurts, Pierre, Damien Ernst, and Louis Wehenkel.
       ""Extremely randomized trees."" Machine learning 63.1 (2006): 3-42.
       `[link]
       <https://doi.org/10.1007/s10994-006-6226-1>`__
    
    See Also
    --------
    Source:
        `sklearn.ensemble.RandomForestRegressor
        <http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html>`_
    
    Source:
        `sklearn ensemble methods: Random Forests
        <http://scikit-learn.org/stable/modules/ensemble.html>`_
    
    Source:
        `Random Forests wikipedia
        <https://en.wikipedia.org/wiki/Random_forest>`_
    
    Source:
        `sklearn.ensemble.ExtraTreesRegressor
        <http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html>`_",no url,LightGBM Random Forest Regressor,PLGBMRFR,Regression,no documentation retrieved,NUM,LightGBM Random Forest Regressor,Light GBM Random Forest Regressor
Eureqa Regressor. Eureqa is a proprietary A.I.-powered modeling engine that automates much of the heavy lifting inherent in analytics and data science.,no url,Eureqa Regressor,EQR,Regression,no documentation retrieved,NUM,Eureqa Regressor,Eureqa Regressor. Eureqa is a proprietary A.I.-powered modeling engine that automates much of the heavy lifting inherent in analytics and data science.
Regularized Quantile Regression implemented in Keras,no url,Regularized Quantile Regression implemented in Keras,KERAS_REGULARIZED_QUANTILE_REG,Regression,no documentation retrieved,NUM,Regularized Quantile Regressor with Keras,Regularized Quantile Regression implemented in Keras
Tunes word n-grams and generates out-of-sample predictions,no url,Tunes word n-grams and generates out-of-sample predictions,WNGER2,Regression,no documentation retrieved,NUM,Auto-Tuned N-Gram Text Regressor using token counts,Tunes word n-grams and generates out-of-sample predictions
RuleFit Regressor,no url,RuleFit Regressor,RULEFITR,Regression,no documentation retrieved,NUM,RuleFit Regressor,RuleFit Regressor
Elasticnet model using block coordinate descent-- a common form of derivative-free optimization. Based on lightning CDRegressor.,http://contrib.scikit-learn.org/lightning/,Elasticnet model using block coordinate descent-- a common form of derivative-free optimization,ENETCDWC,Regression,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Elastic-Net Regressor (L1 / Least-Squares Loss),Elasticnet model using block coordinate descent-- a common form of derivative-free optimization. Based on lightning CDRegressor.
Hot / Cold Spots Regressor. The regressor implements the Patient Rule Induction method to generate hot / cold spots in the data,no url,Hot / Cold Spots Regressor,XPRIMR,Regression,no documentation retrieved,NUM,Hot Spots,Hot / Cold Spots Regressor. The regressor implements the Patient Rule Induction method to generate hot / cold spots in the data
Gradient Boosting Quantile Regressor (scikit-learn). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor,Gradient Boosting Quantile Regressor (scikit-learn),QGBR2,Regression,"













GradientBoostingRegressor — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.ensemble
GradientBoos...









GradientBoostingRegressor#


class sklearn.ensemble.GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)[source]#
Gradient Boosting for regression.
This estimator builds an additive model in a forward stage-wise fashion; it
allows for the optimization of arbitrary differentiable loss functions. In
each stage a regression tree is fit on the negative gradient of the given
loss function.
HistGradientBoostingRegressor is a much faster variant
of this algorithm for intermediate and large datasets (n_samples >= 10_000) and
supports monotonic constraints.
Read more in the User Guide.

Parameters:

loss{‘squared_error’, ‘absolute_error’, ‘huber’, ‘quantile’},             default=’squared_error’Loss function to be optimized. ‘squared_error’ refers to the squared
error for regression. ‘absolute_error’ refers to the absolute error of
regression and is a robust loss function. ‘huber’ is a
combination of the two. ‘quantile’ allows quantile regression (use
alpha to specify the quantile).

learning_ratefloat, default=0.1Learning rate shrinks the contribution of each tree by learning_rate.
There is a trade-off between learning_rate and n_estimators.
Values must be in the range [0.0, inf).

n_estimatorsint, default=100The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.
Values must be in the range [1, inf).

subsamplefloat, default=1.0The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. subsample interacts with the parameter n_estimators.
Choosing subsample < 1.0 leads to a reduction of variance
and an increase in bias.
Values must be in the range (0.0, 1.0].

criterion{‘friedman_mse’, ‘squared_error’}, default=’friedman_mse’The function to measure the quality of a split. Supported criteria are
“friedman_mse” for the mean squared error with improvement score by
Friedman, “squared_error” for mean squared error. The default value of
“friedman_mse” is generally the best as it can provide a better
approximation in some cases.

Added in version 0.18.


min_samples_splitint or float, default=2The minimum number of samples required to split an internal node:

If int, values must be in the range [2, inf).
If float, values must be in the range (0.0, 1.0] and min_samples_split
will be ceil(min_samples_split * n_samples).


Changed in version 0.18: Added float values for fractions.


min_samples_leafint or float, default=1The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least min_samples_leaf training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.

If int, values must be in the range [1, inf).
If float, values must be in the range (0.0, 1.0) and min_samples_leaf
will be ceil(min_samples_leaf * n_samples).


Changed in version 0.18: Added float values for fractions.


min_weight_fraction_leaffloat, default=0.0The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.
Values must be in the range [0.0, 0.5].

max_depthint or None, default=3Maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
If int, values must be in the range [1, inf).

min_impurity_decreasefloat, default=0.0A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.
Values must be in the range [0.0, inf).
The weighted impurity decrease equation is the following:
N_t / N * (impurity - N_t_R / N_t * right_impurity
                    - N_t_L / N_t * left_impurity)


where N is the total number of samples, N_t is the number of
samples at the current node, N_t_L is the number of samples in the
left child, and N_t_R is the number of samples in the right child.
N, N_t, N_t_R and N_t_L all refer to the weighted sum,
if sample_weight is passed.

Added in version 0.19.


initestimator or ‘zero’, default=NoneAn estimator object that is used to compute the initial predictions.
init has to provide fit and predict. If ‘zero’, the
initial raw predictions are set to zero. By default a
DummyEstimator is used, predicting either the average target value
(for loss=’squared_error’), or a quantile for the other losses.

random_stateint, RandomState instance or None, default=NoneControls the random seed given to each Tree estimator at each
boosting iteration.
In addition, it controls the random permutation of the features at
each split (see Notes for more details).
It also controls the random splitting of the training data to obtain a
validation set if n_iter_no_change is not None.
Pass an int for reproducible output across multiple function calls.
See Glossary.

max_features{‘sqrt’, ‘log2’}, int or float, default=NoneThe number of features to consider when looking for the best split:

If int, values must be in the range [1, inf).
If float, values must be in the range (0.0, 1.0] and the features
considered at each split will be max(1, int(max_features * n_features_in_)).
If “sqrt”, then max_features=sqrt(n_features).
If “log2”, then max_features=log2(n_features).
If None, then max_features=n_features.

Choosing max_features < n_features leads to a reduction of variance
and an increase in bias.
Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than max_features features.

alphafloat, default=0.9The alpha-quantile of the huber loss function and the quantile
loss function. Only if loss='huber' or loss='quantile'.
Values must be in the range (0.0, 1.0).

verboseint, default=0Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.
Values must be in the range [0, inf).

max_leaf_nodesint, default=NoneGrow trees with max_leaf_nodes in best-first fashion.
Best nodes are defined as relative reduction in impurity.
Values must be in the range [2, inf).
If None, then unlimited number of leaf nodes.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See the Glossary.

validation_fractionfloat, default=0.1The proportion of training data to set aside as validation set for
early stopping. Values must be in the range (0.0, 1.0).
Only used if n_iter_no_change is set to an integer.

Added in version 0.20.


n_iter_no_changeint, default=Nonen_iter_no_change is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside validation_fraction size of the training
data as validation and terminate training when validation score is not
improving in all of the previous n_iter_no_change numbers of
iterations.
Values must be in the range [1, inf).
See
Early stopping in Gradient Boosting.

Added in version 0.20.


tolfloat, default=1e-4Tolerance for the early stopping. When the loss is not improving
by at least tol for n_iter_no_change iterations (if set to a
number), the training stops.
Values must be in the range [0.0, inf).

Added in version 0.20.


ccp_alphanon-negative float, default=0.0Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
ccp_alpha will be chosen. By default, no pruning is performed.
Values must be in the range [0.0, inf).
See Minimal Cost-Complexity Pruning for details.

Added in version 0.22.




Attributes:

n_estimators_intThe number of estimators as selected by early stopping (if
n_iter_no_change is specified). Otherwise it is set to
n_estimators.

n_trees_per_iteration_intThe number of trees that are built at each iteration. For regressors, this is
always 1.

Added in version 1.4.0.


feature_importances_ndarray of shape (n_features,)The impurity-based feature importances.

oob_improvement_ndarray of shape (n_estimators,)The improvement in loss on the out-of-bag samples
relative to the previous iteration.
oob_improvement_[0] is the improvement in
loss of the first stage over the init estimator.
Only available if subsample < 1.0.

oob_scores_ndarray of shape (n_estimators,)The full history of the loss values on the out-of-bag
samples. Only available if subsample < 1.0.

Added in version 1.3.


oob_score_floatThe last value of the loss on the out-of-bag samples. It is
the same as oob_scores_[-1]. Only available if subsample < 1.0.

Added in version 1.3.


train_score_ndarray of shape (n_estimators,)The i-th score train_score_[i] is the loss of the
model at iteration i on the in-bag sample.
If subsample == 1 this is the loss on the training data.

init_estimatorThe estimator that provides the initial predictions. Set via the init
argument.

estimators_ndarray of DecisionTreeRegressor of shape (n_estimators, 1)The collection of fitted sub-estimators.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


max_features_intThe inferred value of max_features.





See also

HistGradientBoostingRegressorHistogram-based Gradient Boosting Classification Tree.

sklearn.tree.DecisionTreeRegressorA decision tree regressor.

sklearn.ensemble.RandomForestRegressorA random forest regressor.



Notes
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
max_features=n_features, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
random_state has to be fixed.
References
J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.

Friedman, Stochastic Gradient Boosting, 1999

T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.
Examples
>>> from sklearn.datasets import make_regression
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> from sklearn.model_selection import train_test_split
>>> X, y = make_regression(random_state=0)
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=0)
>>> reg = GradientBoostingRegressor(random_state=0)
>>> reg.fit(X_train, y_train)
GradientBoostingRegressor(random_state=0)
>>> reg.predict(X_test[1:2])
array([-61...])
>>> reg.score(X_test, y_test)
0.4...


For a detailed example of utilizing
GradientBoostingRegressor
to fit an ensemble of weak predictive models, please refer to
Gradient Boosting regression.


apply(X)[source]#
Apply trees in the ensemble to X, return leaf indices.

Added in version 0.17.


Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, its dtype will be converted to
dtype=np.float32. If a sparse matrix is provided, it will
be converted to a sparse csr_matrix.



Returns:

X_leavesarray-like of shape (n_samples, n_estimators)For each datapoint x in X and for each tree in the ensemble,
return the index of the leaf x ends up in each estimator.







property feature_importances_#
The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.
Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
sklearn.inspection.permutation_importance as an alternative.

Returns:

feature_importances_ndarray of shape (n_features,)The values of this array sum to 1, unless all trees are single node
trees consisting of only the root node, in which case it will be an
array of zeros.







fit(X, y, sample_weight=None, monitor=None)[source]#
Fit the gradient boosting model.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.

yarray-like of shape (n_samples,)Target values (strings or integers in classification, real numbers
in regression)
For classification, labels must correspond to classes.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. In the case of
classification, splits are also ignored if they would result in any
single class carrying a negative weight in either child node.

monitorcallable, default=NoneThe monitor is called after each iteration with the current
iteration, a reference to the estimator and the local variables of
_fit_stages as keyword arguments callable(i, self,
locals()). If the callable returns True the fitting procedure
is stopped. The monitor can be used for various things such as
computing held-out estimates, early stopping, model introspect, and
snapshotting.



Returns:

selfobjectFitted estimator.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict regression target for X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

yndarray of shape (n_samples,)The predicted values.







score(X, y, sample_weight=None)[source]#
Return the coefficient of determination of the prediction.
The coefficient of determination \(R^2\) is defined as
\((1 - \frac{u}{v})\), where \(u\) is the residual
sum of squares ((y_true - y_pred)** 2).sum() and \(v\)
is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of y, disregarding the input features, would get
a \(R^2\) score of 0.0.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples. For some estimators this may be a precomputed
kernel matrix or a list of generic objects instead with shape
(n_samples, n_samples_fitted), where n_samples_fitted
is the number of samples used in the fitting for the estimator.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True values for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloat\(R^2\) of self.predict(X) w.r.t. y.




Notes
The \(R^2\) score used when calling score on a regressor uses
multioutput='uniform_average' from version 0.23 to keep consistent
with default value of r2_score.
This influences the score method of all the multioutput
regressors (except for
MultiOutputRegressor).



set_fit_request(*, monitor: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → GradientBoostingRegressor[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

monitorstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for monitor parameter in fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → GradientBoostingRegressor[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







staged_predict(X)[source]#
Predict regression target at each stage for X.
This method allows monitoring (i.e. determine error on testing set)
after each stage.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Yields:

ygenerator of ndarray of shape (n_samples,)The predicted value of the input samples.







Gallery examples#

Early stopping in Gradient Boosting
Early stopping in Gradient Boosting

Gradient Boosting regression
Gradient Boosting regression

Plot individual and voting regression predictions
Plot individual and voting regression predictions

Prediction Intervals for Gradient Boosting Regression
Prediction Intervals for Gradient Boosting Regression

Model Complexity Influence
Model Complexity Influence










previous
GradientBoostingClassifier




next
HistGradientBoostingClassifier










 On this page
  


GradientBoostingRegressor
apply
feature_importances_
fit
get_metadata_routing
get_params
predict
score
set_fit_request
set_params
set_score_request
staged_predict


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gradient Boosted Trees Quantile Regressor,Gradient Boosting Quantile Regressor (scikit-learn). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Frequency-Severity Light Gradient Boosted Trees,no url,Frequency-Severity Light Gradient Boosted Trees,FSLL,Regression,no documentation retrieved,NUM,Frequency-Severity Light Gradient Boosted Trees,Frequency-Severity Light Gradient Boosted Trees
Gradient Boosting Quantile Regressor (scikit-learn) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor,Gradient Boosting Quantile Regressor (scikit-learn) with Early-Stopping,QESGBR2,Regression,"













GradientBoostingRegressor — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.ensemble
GradientBoos...









GradientBoostingRegressor#


class sklearn.ensemble.GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)[source]#
Gradient Boosting for regression.
This estimator builds an additive model in a forward stage-wise fashion; it
allows for the optimization of arbitrary differentiable loss functions. In
each stage a regression tree is fit on the negative gradient of the given
loss function.
HistGradientBoostingRegressor is a much faster variant
of this algorithm for intermediate and large datasets (n_samples >= 10_000) and
supports monotonic constraints.
Read more in the User Guide.

Parameters:

loss{‘squared_error’, ‘absolute_error’, ‘huber’, ‘quantile’},             default=’squared_error’Loss function to be optimized. ‘squared_error’ refers to the squared
error for regression. ‘absolute_error’ refers to the absolute error of
regression and is a robust loss function. ‘huber’ is a
combination of the two. ‘quantile’ allows quantile regression (use
alpha to specify the quantile).

learning_ratefloat, default=0.1Learning rate shrinks the contribution of each tree by learning_rate.
There is a trade-off between learning_rate and n_estimators.
Values must be in the range [0.0, inf).

n_estimatorsint, default=100The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.
Values must be in the range [1, inf).

subsamplefloat, default=1.0The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. subsample interacts with the parameter n_estimators.
Choosing subsample < 1.0 leads to a reduction of variance
and an increase in bias.
Values must be in the range (0.0, 1.0].

criterion{‘friedman_mse’, ‘squared_error’}, default=’friedman_mse’The function to measure the quality of a split. Supported criteria are
“friedman_mse” for the mean squared error with improvement score by
Friedman, “squared_error” for mean squared error. The default value of
“friedman_mse” is generally the best as it can provide a better
approximation in some cases.

Added in version 0.18.


min_samples_splitint or float, default=2The minimum number of samples required to split an internal node:

If int, values must be in the range [2, inf).
If float, values must be in the range (0.0, 1.0] and min_samples_split
will be ceil(min_samples_split * n_samples).


Changed in version 0.18: Added float values for fractions.


min_samples_leafint or float, default=1The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least min_samples_leaf training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.

If int, values must be in the range [1, inf).
If float, values must be in the range (0.0, 1.0) and min_samples_leaf
will be ceil(min_samples_leaf * n_samples).


Changed in version 0.18: Added float values for fractions.


min_weight_fraction_leaffloat, default=0.0The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.
Values must be in the range [0.0, 0.5].

max_depthint or None, default=3Maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
If int, values must be in the range [1, inf).

min_impurity_decreasefloat, default=0.0A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.
Values must be in the range [0.0, inf).
The weighted impurity decrease equation is the following:
N_t / N * (impurity - N_t_R / N_t * right_impurity
                    - N_t_L / N_t * left_impurity)


where N is the total number of samples, N_t is the number of
samples at the current node, N_t_L is the number of samples in the
left child, and N_t_R is the number of samples in the right child.
N, N_t, N_t_R and N_t_L all refer to the weighted sum,
if sample_weight is passed.

Added in version 0.19.


initestimator or ‘zero’, default=NoneAn estimator object that is used to compute the initial predictions.
init has to provide fit and predict. If ‘zero’, the
initial raw predictions are set to zero. By default a
DummyEstimator is used, predicting either the average target value
(for loss=’squared_error’), or a quantile for the other losses.

random_stateint, RandomState instance or None, default=NoneControls the random seed given to each Tree estimator at each
boosting iteration.
In addition, it controls the random permutation of the features at
each split (see Notes for more details).
It also controls the random splitting of the training data to obtain a
validation set if n_iter_no_change is not None.
Pass an int for reproducible output across multiple function calls.
See Glossary.

max_features{‘sqrt’, ‘log2’}, int or float, default=NoneThe number of features to consider when looking for the best split:

If int, values must be in the range [1, inf).
If float, values must be in the range (0.0, 1.0] and the features
considered at each split will be max(1, int(max_features * n_features_in_)).
If “sqrt”, then max_features=sqrt(n_features).
If “log2”, then max_features=log2(n_features).
If None, then max_features=n_features.

Choosing max_features < n_features leads to a reduction of variance
and an increase in bias.
Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than max_features features.

alphafloat, default=0.9The alpha-quantile of the huber loss function and the quantile
loss function. Only if loss='huber' or loss='quantile'.
Values must be in the range (0.0, 1.0).

verboseint, default=0Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.
Values must be in the range [0, inf).

max_leaf_nodesint, default=NoneGrow trees with max_leaf_nodes in best-first fashion.
Best nodes are defined as relative reduction in impurity.
Values must be in the range [2, inf).
If None, then unlimited number of leaf nodes.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See the Glossary.

validation_fractionfloat, default=0.1The proportion of training data to set aside as validation set for
early stopping. Values must be in the range (0.0, 1.0).
Only used if n_iter_no_change is set to an integer.

Added in version 0.20.


n_iter_no_changeint, default=Nonen_iter_no_change is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside validation_fraction size of the training
data as validation and terminate training when validation score is not
improving in all of the previous n_iter_no_change numbers of
iterations.
Values must be in the range [1, inf).
See
Early stopping in Gradient Boosting.

Added in version 0.20.


tolfloat, default=1e-4Tolerance for the early stopping. When the loss is not improving
by at least tol for n_iter_no_change iterations (if set to a
number), the training stops.
Values must be in the range [0.0, inf).

Added in version 0.20.


ccp_alphanon-negative float, default=0.0Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
ccp_alpha will be chosen. By default, no pruning is performed.
Values must be in the range [0.0, inf).
See Minimal Cost-Complexity Pruning for details.

Added in version 0.22.




Attributes:

n_estimators_intThe number of estimators as selected by early stopping (if
n_iter_no_change is specified). Otherwise it is set to
n_estimators.

n_trees_per_iteration_intThe number of trees that are built at each iteration. For regressors, this is
always 1.

Added in version 1.4.0.


feature_importances_ndarray of shape (n_features,)The impurity-based feature importances.

oob_improvement_ndarray of shape (n_estimators,)The improvement in loss on the out-of-bag samples
relative to the previous iteration.
oob_improvement_[0] is the improvement in
loss of the first stage over the init estimator.
Only available if subsample < 1.0.

oob_scores_ndarray of shape (n_estimators,)The full history of the loss values on the out-of-bag
samples. Only available if subsample < 1.0.

Added in version 1.3.


oob_score_floatThe last value of the loss on the out-of-bag samples. It is
the same as oob_scores_[-1]. Only available if subsample < 1.0.

Added in version 1.3.


train_score_ndarray of shape (n_estimators,)The i-th score train_score_[i] is the loss of the
model at iteration i on the in-bag sample.
If subsample == 1 this is the loss on the training data.

init_estimatorThe estimator that provides the initial predictions. Set via the init
argument.

estimators_ndarray of DecisionTreeRegressor of shape (n_estimators, 1)The collection of fitted sub-estimators.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


max_features_intThe inferred value of max_features.





See also

HistGradientBoostingRegressorHistogram-based Gradient Boosting Classification Tree.

sklearn.tree.DecisionTreeRegressorA decision tree regressor.

sklearn.ensemble.RandomForestRegressorA random forest regressor.



Notes
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
max_features=n_features, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
random_state has to be fixed.
References
J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.

Friedman, Stochastic Gradient Boosting, 1999

T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.
Examples
>>> from sklearn.datasets import make_regression
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> from sklearn.model_selection import train_test_split
>>> X, y = make_regression(random_state=0)
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=0)
>>> reg = GradientBoostingRegressor(random_state=0)
>>> reg.fit(X_train, y_train)
GradientBoostingRegressor(random_state=0)
>>> reg.predict(X_test[1:2])
array([-61...])
>>> reg.score(X_test, y_test)
0.4...


For a detailed example of utilizing
GradientBoostingRegressor
to fit an ensemble of weak predictive models, please refer to
Gradient Boosting regression.


apply(X)[source]#
Apply trees in the ensemble to X, return leaf indices.

Added in version 0.17.


Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, its dtype will be converted to
dtype=np.float32. If a sparse matrix is provided, it will
be converted to a sparse csr_matrix.



Returns:

X_leavesarray-like of shape (n_samples, n_estimators)For each datapoint x in X and for each tree in the ensemble,
return the index of the leaf x ends up in each estimator.







property feature_importances_#
The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.
Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
sklearn.inspection.permutation_importance as an alternative.

Returns:

feature_importances_ndarray of shape (n_features,)The values of this array sum to 1, unless all trees are single node
trees consisting of only the root node, in which case it will be an
array of zeros.







fit(X, y, sample_weight=None, monitor=None)[source]#
Fit the gradient boosting model.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.

yarray-like of shape (n_samples,)Target values (strings or integers in classification, real numbers
in regression)
For classification, labels must correspond to classes.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. In the case of
classification, splits are also ignored if they would result in any
single class carrying a negative weight in either child node.

monitorcallable, default=NoneThe monitor is called after each iteration with the current
iteration, a reference to the estimator and the local variables of
_fit_stages as keyword arguments callable(i, self,
locals()). If the callable returns True the fitting procedure
is stopped. The monitor can be used for various things such as
computing held-out estimates, early stopping, model introspect, and
snapshotting.



Returns:

selfobjectFitted estimator.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict regression target for X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

yndarray of shape (n_samples,)The predicted values.







score(X, y, sample_weight=None)[source]#
Return the coefficient of determination of the prediction.
The coefficient of determination \(R^2\) is defined as
\((1 - \frac{u}{v})\), where \(u\) is the residual
sum of squares ((y_true - y_pred)** 2).sum() and \(v\)
is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of y, disregarding the input features, would get
a \(R^2\) score of 0.0.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples. For some estimators this may be a precomputed
kernel matrix or a list of generic objects instead with shape
(n_samples, n_samples_fitted), where n_samples_fitted
is the number of samples used in the fitting for the estimator.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True values for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloat\(R^2\) of self.predict(X) w.r.t. y.




Notes
The \(R^2\) score used when calling score on a regressor uses
multioutput='uniform_average' from version 0.23 to keep consistent
with default value of r2_score.
This influences the score method of all the multioutput
regressors (except for
MultiOutputRegressor).



set_fit_request(*, monitor: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → GradientBoostingRegressor[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

monitorstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for monitor parameter in fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → GradientBoostingRegressor[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







staged_predict(X)[source]#
Predict regression target at each stage for X.
This method allows monitoring (i.e. determine error on testing set)
after each stage.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Yields:

ygenerator of ndarray of shape (n_samples,)The predicted value of the input samples.







Gallery examples#

Early stopping in Gradient Boosting
Early stopping in Gradient Boosting

Gradient Boosting regression
Gradient Boosting regression

Plot individual and voting regression predictions
Plot individual and voting regression predictions

Prediction Intervals for Gradient Boosting Regression
Prediction Intervals for Gradient Boosting Regression

Model Complexity Influence
Model Complexity Influence










previous
GradientBoostingClassifier




next
HistGradientBoostingClassifier










 On this page
  


GradientBoostingRegressor
apply
feature_importances_
fit
get_metadata_routing
get_params
predict
score
set_fit_request
set_params
set_score_request
staged_predict


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gradient Boosted Trees Quantile Regressor with Early Stopping,Gradient Boosting Quantile Regressor (scikit-learn) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Gradient Boosting Regressor (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Regressor (xgboost),XGBR2,Regression,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Regressor,Gradient Boosting Regressor (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Frequency-Cost ElasticNet,no url,Frequency-Cost ElasticNet,FCEE,Regression,no documentation retrieved,NUM,Frequency-Cost ElasticNet,Frequency-Cost ElasticNet
"Gaussian Process Regression with RBF Kernel. The RBF kernel is a stationary kernel. It is also known as the ""squared exponential"" kernel. It is parameterized by a length-scale parameter length_scale > 0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel).",https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html,Gaussian Process Regression with RBF Kernel,GPRRBF,Regression,"













RBF — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.gaussian_process
RBF









RBF#


class sklearn.gaussian_process.kernels.RBF(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0))[source]#
Radial basis function kernel (aka squared-exponential kernel).
The RBF kernel is a stationary kernel. It is also known as the
“squared exponential” kernel. It is parameterized by a length scale
parameter \(l>0\), which can either be a scalar (isotropic variant
of the kernel) or a vector with the same number of dimensions as the inputs
X (anisotropic variant of the kernel). The kernel is given by:

\[k(x_i, x_j) = \exp\left(- \frac{d(x_i, x_j)^2}{2l^2} \right)\]
where \(l\) is the length scale of the kernel and
\(d(\cdot,\cdot)\) is the Euclidean distance.
For advice on how to set the length scale parameter, see e.g. [1].
This kernel is infinitely differentiable, which implies that GPs with this
kernel as covariance function have mean square derivatives of all orders,
and are thus very smooth.
See [2], Chapter 4, Section 4.2, for further details of the RBF kernel.
Read more in the User Guide.

Added in version 0.18.


Parameters:

length_scalefloat or ndarray of shape (n_features,), default=1.0The length scale of the kernel. If a float, an isotropic kernel is
used. If an array, an anisotropic kernel is used where each dimension
of l defines the length-scale of the respective feature dimension.

length_scale_boundspair of floats >= 0 or “fixed”, default=(1e-5, 1e5)The lower and upper bound on ‘length_scale’.
If set to “fixed”, ‘length_scale’ cannot be changed during
hyperparameter tuning.




References


[1]
David Duvenaud (2014). “The Kernel Cookbook:
Advice on Covariance functions”.


[2]
Carl Edward Rasmussen, Christopher K. I. Williams (2006).
“Gaussian Processes for Machine Learning”. The MIT Press.


Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn.gaussian_process import GaussianProcessClassifier
>>> from sklearn.gaussian_process.kernels import RBF
>>> X, y = load_iris(return_X_y=True)
>>> kernel = 1.0 * RBF(1.0)
>>> gpc = GaussianProcessClassifier(kernel=kernel,
...         random_state=0).fit(X, y)
>>> gpc.score(X, y)
0.9866...
>>> gpc.predict_proba(X[:2,:])
array([[0.8354..., 0.03228..., 0.1322...],
       [0.7906..., 0.0652..., 0.1441...]])




__call__(X, Y=None, eval_gradient=False)[source]#
Return the kernel k(X, Y) and optionally its gradient.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)

Yndarray of shape (n_samples_Y, n_features), default=NoneRight argument of the returned kernel k(X, Y). If None, k(X, X)
if evaluated instead.

eval_gradientbool, default=FalseDetermines whether the gradient with respect to the log of
the kernel hyperparameter is computed.
Only supported when Y is None.



Returns:

Kndarray of shape (n_samples_X, n_samples_Y)Kernel k(X, Y)

K_gradientndarray of shape (n_samples_X, n_samples_X, n_dims),                 optionalThe gradient of the kernel k(X, X) with respect to the log of the
hyperparameter of the kernel. Only returned when eval_gradient
is True.







property bounds#
Returns the log-transformed bounds on the theta.

Returns:

boundsndarray of shape (n_dims, 2)The log-transformed bounds on the kernel’s hyperparameters theta







clone_with_theta(theta)[source]#
Returns a clone of self with given hyperparameters theta.

Parameters:

thetandarray of shape (n_dims,)The hyperparameters







diag(X)[source]#
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however,
it can be evaluated more efficiently since only the diagonal is
evaluated.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)



Returns:

K_diagndarray of shape (n_samples_X,)Diagonal of kernel k(X, X)







get_params(deep=True)[source]#
Get parameters of this kernel.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







property hyperparameters#
Returns a list of all hyperparameter specifications.



is_stationary()[source]#
Returns whether the kernel is stationary.



property n_dims#
Returns the number of non-fixed hyperparameters of the kernel.



property requires_vector_input#
Returns whether the kernel is defined on fixed-length feature
vectors or generic objects. Defaults to True for backward
compatibility.



set_params(**params)[source]#
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels.
The latter have parameters of the form <component>__<parameter>
so that it’s possible to update each component of a nested object.

Returns:

self






property theta#
Returns the (flattened, log-transformed) non-fixed hyperparameters.
Note that theta are typically the log-transformed values of the
kernel’s hyperparameters as this representation of the search space
is more amenable for hyperparameter search, as hyperparameters like
length-scales naturally live on a log-scale.

Returns:

thetandarray of shape (n_dims,)The non-fixed, log-transformed hyperparameters of the kernel







Gallery examples#

Classifier comparison
Classifier comparison

Plot classification probability
Plot classification probability

Ability of Gaussian process regression (GPR) to estimate data noise-level
Ability of Gaussian process regression (GPR) to estimate data noise-level

Comparison of kernel ridge and Gaussian process regression
Comparison of kernel ridge and Gaussian process regression

Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)
Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)

Gaussian Processes regression: basic introductory example
Gaussian Processes regression: basic introductory example

Gaussian process classification (GPC) on iris dataset
Gaussian process classification (GPC) on iris dataset

Illustration of Gaussian process classification (GPC) on the XOR dataset
Illustration of Gaussian process classification (GPC) on the XOR dataset

Illustration of prior and posterior Gaussian process for different kernels
Illustration of prior and posterior Gaussian process for different kernels

Probabilistic predictions with Gaussian process classification (GPC)
Probabilistic predictions with Gaussian process classification (GPC)










previous
Product




next
RationalQuadratic










 On this page
  


RBF
__call__
bounds
clone_with_theta
diag
get_params
hyperparameters
is_stationary
n_dims
requires_vector_input
set_params
theta


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gaussian Process Regressor with Radial Basis Function Kernel,"Gaussian Process Regression with RBF Kernel. The RBF kernel is a stationary kernel. It is also known as the ""squared exponential"" kernel. It is parameterized by a length-scale parameter length_scale > 0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel)."
Gradient Boosting Quantile Regressor (xgboost) with early stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Quantile Regressor (xgboost) with early stopping,ESQUANTXGBR,Regression,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Quantile Regressor with Early Stopping,Gradient Boosting Quantile Regressor (xgboost) with early stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Support Vector Regressor (scikit-learn). Support vector machines are a class of maximum margin models. They seek to maximize the separation they find between classes and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations.,http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC,Support Vector Regressor (scikit-learn),SVMR2,Regression,"













SVC — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.svm
SVC









SVC#


class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)[source]#
C-Support Vector Classification.
The implementation is based on libsvm. The fit time scales at least
quadratically with the number of samples and may be impractical
beyond tens of thousands of samples. For large datasets
consider using LinearSVC or
SGDClassifier instead, possibly after a
Nystroem transformer or
other Kernel Approximation.
The multiclass support is handled according to a one-vs-one scheme.
For details on the precise mathematical formulation of the provided
kernel functions and how gamma, coef0 and degree affect each
other, see the corresponding section in the narrative documentation:
Kernel functions.
To learn how to tune SVC’s hyperparameters, see the following example:
Nested versus non-nested cross-validation
Read more in the User Guide.

Parameters:

Cfloat, default=1.0Regularization parameter. The strength of the regularization is
inversely proportional to C. Must be strictly positive. The penalty
is a squared l2 penalty. For an intuitive visualization of the effects
of scaling the regularization parameter C, see
Scaling the regularization parameter for SVCs.

kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable,          default=’rbf’Specifies the kernel type to be used in the algorithm. If
none is given, ‘rbf’ will be used. If a callable is given it is used to
pre-compute the kernel matrix from data matrices; that matrix should be
an array of shape (n_samples, n_samples). For an intuitive
visualization of different kernel types see
Plot classification boundaries with different SVM Kernels.

degreeint, default=3Degree of the polynomial kernel function (‘poly’).
Must be non-negative. Ignored by all other kernels.

gamma{‘scale’, ‘auto’} or float, default=’scale’Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.

if gamma='scale' (default) is passed then it uses
1 / (n_features * X.var()) as value of gamma,
if ‘auto’, uses 1 / n_features
if float, must be non-negative.


Changed in version 0.22: The default value of gamma changed from ‘auto’ to ‘scale’.


coef0float, default=0.0Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.

shrinkingbool, default=TrueWhether to use the shrinking heuristic.
See the User Guide.

probabilitybool, default=FalseWhether to enable probability estimates. This must be enabled prior
to calling fit, will slow down that method as it internally uses
5-fold cross-validation, and predict_proba may be inconsistent with
predict. Read more in the User Guide.

tolfloat, default=1e-3Tolerance for stopping criterion.

cache_sizefloat, default=200Specify the size of the kernel cache (in MB).

class_weightdict or ‘balanced’, default=NoneSet the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as n_samples / (n_classes * np.bincount(y)).

verbosebool, default=FalseEnable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.

max_iterint, default=-1Hard limit on iterations within solver, or -1 for no limit.

decision_function_shape{‘ovo’, ‘ovr’}, default=’ovr’Whether to return a one-vs-rest (‘ovr’) decision function of shape
(n_samples, n_classes) as all other classifiers, or the original
one-vs-one (‘ovo’) decision function of libsvm which has shape
(n_samples, n_classes * (n_classes - 1) / 2). However, note that
internally, one-vs-one (‘ovo’) is always used as a multi-class strategy
to train models; an ovr matrix is only constructed from the ovo matrix.
The parameter is ignored for binary classification.

Changed in version 0.19: decision_function_shape is ‘ovr’ by default.


Added in version 0.17: decision_function_shape=’ovr’ is recommended.


Changed in version 0.17: Deprecated decision_function_shape=’ovo’ and None.


break_tiesbool, default=FalseIf true, decision_function_shape='ovr', and number of classes > 2,
predict will break ties according to the confidence values of
decision_function; otherwise the first class among the tied
classes is returned. Please note that breaking ties comes at a
relatively high computational cost compared to a simple predict.

Added in version 0.22.


random_stateint, RandomState instance or None, default=NoneControls the pseudo random number generation for shuffling the data for
probability estimates. Ignored when probability is False.
Pass an int for reproducible output across multiple function calls.
See Glossary.



Attributes:

class_weight_ndarray of shape (n_classes,)Multipliers of parameter C for each class.
Computed based on the class_weight parameter.

classes_ndarray of shape (n_classes,)The classes labels.

coef_ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)Weights assigned to the features when kernel=""linear"".

dual_coef_ndarray of shape (n_classes -1, n_SV)Dual coefficients of the support vector in the decision
function (see Mathematical formulation), multiplied by
their targets.
For multiclass, coefficient for all 1-vs-1 classifiers.
The layout of the coefficients in the multiclass case is somewhat
non-trivial. See the multi-class section of the User Guide for details.

fit_status_int0 if correctly fitted, 1 otherwise (will raise warning)

intercept_ndarray of shape (n_classes * (n_classes - 1) / 2,)Constants in decision function.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


n_iter_ndarray of shape (n_classes * (n_classes - 1) // 2,)Number of iterations run by the optimization routine to fit the model.
The shape of this attribute depends on the number of models optimized
which in turn depends on the number of classes.

Added in version 1.1.


support_ndarray of shape (n_SV)Indices of support vectors.

support_vectors_ndarray of shape (n_SV, n_features)Support vectors. An empty array if kernel is precomputed.

n_support_ndarray of shape (n_classes,), dtype=int32Number of support vectors for each class.

probA_ndarray of shape (n_classes * (n_classes - 1) / 2)Parameter learned in Platt scaling when probability=True.

probB_ndarray of shape (n_classes * (n_classes - 1) / 2)Parameter learned in Platt scaling when probability=True.

shape_fit_tuple of int of shape (n_dimensions_of_X,)Array dimensions of training vector X.





See also

SVRSupport Vector Machine for Regression implemented using libsvm.

LinearSVCScalable Linear Support Vector Machine for classification implemented using liblinear. Check the See Also section of LinearSVC for more comparison element.



References


[1]
LIBSVM: A Library for Support Vector Machines


[2]
Platt, John (1999). “Probabilistic Outputs for Support Vector
Machines and Comparisons to Regularized Likelihood Methods”


Examples
>>> import numpy as np
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> y = np.array([1, 1, 2, 2])
>>> from sklearn.svm import SVC
>>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
>>> clf.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('svc', SVC(gamma='auto'))])


>>> print(clf.predict([[-0.8, -1]]))
[1]




property coef_#
Weights assigned to the features when kernel=""linear"".

Returns:

ndarray of shape (n_features, n_classes)






decision_function(X)[source]#
Evaluate the decision function for the samples in X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Xndarray of shape (n_samples, n_classes * (n_classes-1) / 2)Returns the decision function of the sample for each class
in the model.
If decision_function_shape=’ovr’, the shape is (n_samples,
n_classes).




Notes
If decision_function_shape=’ovo’, the function values are proportional
to the distance of the samples X to the separating hyperplane. If the
exact distances are required, divide the function values by the norm of
the weight vector (coef_). See also this question for further details.
If decision_function_shape=’ovr’, the decision function is a monotonic
transformation of ovo decision function.



fit(X, y, sample_weight=None)[source]#
Fit the SVM model according to the given training data.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)Training vectors, where n_samples is the number of samples
and n_features is the number of features.
For kernel=”precomputed”, the expected shape of X is
(n_samples, n_samples).

yarray-like of shape (n_samples,)Target values (class labels in classification, real numbers in
regression).

sample_weightarray-like of shape (n_samples,), default=NonePer-sample weights. Rescale C per sample. Higher weights
force the classifier to put more emphasis on these points.



Returns:

selfobjectFitted estimator.




Notes
If X and y are not C-ordered and contiguous arrays of np.float64 and
X is not a scipy.sparse.csr_matrix, X and/or y may be copied.
If X is a dense array, then the other methods will not support sparse
matrices as input.



get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







property n_support_#
Number of support vectors for each class.



predict(X)[source]#
Perform classification on samples in X.
For an one-class model, +1 or -1 is returned.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)For kernel=”precomputed”, the expected shape of X is
(n_samples_test, n_samples_train).



Returns:

y_predndarray of shape (n_samples,)Class labels for samples in X.







predict_log_proba(X)[source]#
Compute log probabilities of possible outcomes for samples in X.
The model need to have probability information computed at training
time: fit with attribute probability set to True.

Parameters:

Xarray-like of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)For kernel=”precomputed”, the expected shape of X is
(n_samples_test, n_samples_train).



Returns:

Tndarray of shape (n_samples, n_classes)Returns the log-probabilities of the sample for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.




Notes
The probability model is created using cross validation, so
the results can be slightly different than those obtained by
predict. Also, it will produce meaningless results on very small
datasets.



predict_proba(X)[source]#
Compute probabilities of possible outcomes for samples in X.
The model needs to have probability information computed at training
time: fit with attribute probability set to True.

Parameters:

Xarray-like of shape (n_samples, n_features)For kernel=”precomputed”, the expected shape of X is
(n_samples_test, n_samples_train).



Returns:

Tndarray of shape (n_samples, n_classes)Returns the probability of the sample for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.




Notes
The probability model is created using cross validation, so
the results can be slightly different than those obtained by
predict. Also, it will produce meaningless results on very small
datasets.



property probA_#
Parameter learned in Platt scaling when probability=True.

Returns:

ndarray of shape  (n_classes * (n_classes - 1) / 2)






property probB_#
Parameter learned in Platt scaling when probability=True.

Returns:

ndarray of shape  (n_classes * (n_classes - 1) / 2)






score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → SVC[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → SVC[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







Gallery examples#

Release Highlights for scikit-learn 0.24
Release Highlights for scikit-learn 0.24

Release Highlights for scikit-learn 0.22
Release Highlights for scikit-learn 0.22

Classifier comparison
Classifier comparison

Plot classification probability
Plot classification probability

Recognizing hand-written digits
Recognizing hand-written digits

Plot the decision boundaries of a VotingClassifier
Plot the decision boundaries of a VotingClassifier

Faces recognition example using eigenfaces and SVMs
Faces recognition example using eigenfaces and SVMs

Scalable learning with polynomial kernel approximation
Scalable learning with polynomial kernel approximation

Displaying Pipelines
Displaying Pipelines

Explicit feature map approximation for RBF kernels
Explicit feature map approximation for RBF kernels

Multilabel classification
Multilabel classification

ROC Curve with Visualization API
ROC Curve with Visualization API

Comparison between grid search and successive halving
Comparison between grid search and successive halving

Confusion matrix
Confusion matrix

Custom refit strategy of a grid search with cross-validation
Custom refit strategy of a grid search with cross-validation

Nested versus non-nested cross-validation
Nested versus non-nested cross-validation

Plotting Learning Curves and Checking Models’ Scalability
Plotting Learning Curves and Checking Models' Scalability

Plotting Validation Curves
Plotting Validation Curves

Receiver Operating Characteristic (ROC) with cross validation
Receiver Operating Characteristic (ROC) with cross validation

Statistical comparison of models using grid search
Statistical comparison of models using grid search

Test with permutations the significance of a classification score
Test with permutations the significance of a classification score

Concatenating multiple feature extraction methods
Concatenating multiple feature extraction methods

Feature discretization
Feature discretization

Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset
Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset

Effect of varying threshold for self-training
Effect of varying threshold for self-training

Plot classification boundaries with different SVM Kernels
Plot classification boundaries with different SVM Kernels

Plot different SVM classifiers in the iris dataset
Plot different SVM classifiers in the iris dataset

RBF SVM parameters
RBF SVM parameters

SVM Margins Example
SVM Margins Example

SVM Tie Breaking Example
SVM Tie Breaking Example

SVM with custom kernel
SVM with custom kernel

SVM-Anova: SVM with univariate feature selection
SVM-Anova: SVM with univariate feature selection

SVM: Maximum margin separating hyperplane
SVM: Maximum margin separating hyperplane

SVM: Separating hyperplane for unbalanced classes
SVM: Separating hyperplane for unbalanced classes

SVM: Weighted samples
SVM: Weighted samples

SVM Exercise
SVM Exercise










previous
OneClassSVM




next
SVR










 On this page
  


SVC
coef_
decision_function
fit
get_metadata_routing
get_params
n_support_
predict
predict_log_proba
predict_proba
probA_
probB_
score
set_fit_request
set_params
set_score_request


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Support Vector Regressor (Radial Kernel),Support Vector Regressor (scikit-learn). Support vector machines are a class of maximum margin models. They seek to maximize the separation they find between classes and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations.
"Approximate Kernel Support Vector Regressor using sklearn Ridge. Support vector machines are a class of maximum margin models. They seek to maximize the separation they find between classes, and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations.",http://scikit-learn.org/stable/modules/kernel_approximation.html,Approximate Kernel Support Vector Regressor using sklearn Ridge,ASVMSKR,Regression,"













6.7. Kernel Approximation — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

1. Supervised learning
1.1. Linear Models
1.2. Linear and Quadratic Discriminant Analysis
1.3. Kernel ridge regression
1.4. Support Vector Machines
1.5. Stochastic Gradient Descent
1.6. Nearest Neighbors
1.7. Gaussian Processes
1.8. Cross decomposition
1.9. Naive Bayes
1.10. Decision Trees
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking
1.12. Multiclass and multioutput algorithms
1.13. Feature selection
1.14. Semi-supervised learning
1.15. Isotonic regression
1.16. Probability calibration
1.17. Neural network models (supervised)


2. Unsupervised learning
2.1. Gaussian mixture models
2.2. Manifold learning
2.3. Clustering
2.4. Biclustering
2.5. Decomposing signals in components (matrix factorization problems)
2.6. Covariance estimation
2.7. Novelty and Outlier Detection
2.8. Density Estimation
2.9. Neural network models (unsupervised)


3. Model selection and evaluation
3.1. Cross-validation: evaluating estimator performance
3.2. Tuning the hyper-parameters of an estimator
3.3. Tuning the decision threshold for class prediction
3.4. Metrics and scoring: quantifying the quality of predictions
3.5. Validation curves: plotting scores to evaluate models


4. Inspection
4.1. Partial Dependence and Individual Conditional Expectation plots
4.2. Permutation feature importance


5. Visualizations
6. Dataset transformations
6.1. Pipelines and composite estimators
6.2. Feature extraction
6.3. Preprocessing data
6.4. Imputation of missing values
6.5. Unsupervised dimensionality reduction
6.6. Random Projection
6.7. Kernel Approximation
6.8. Pairwise metrics, Affinities and Kernels
6.9. Transforming the prediction target (y)


7. Dataset loading utilities
7.1. Toy datasets
7.2. Real world datasets
7.3. Generated datasets
7.4. Loading other datasets


8. Computing with scikit-learn
8.1. Strategies to scale computationally: bigger data
8.2. Computational Performance
8.3. Parallelism, resource management, and configuration


9. Model persistence
10. Common pitfalls and recommended practices
11. Dispatching
11.1. Array API support (experimental)


12. Choosing the right estimator
13. External Resources, Videos and Talks






















User Guide
6. Dataset transformations










6.7. Kernel Approximation#
This submodule contains functions that approximate the feature mappings that
correspond to certain kernels, as they are used for example in support vector
machines (see Support Vector Machines).
The following feature functions perform non-linear transformations of the
input, which can serve as a basis for linear classification or other
algorithms.
The advantage of using approximate explicit feature maps compared to the
kernel trick,
which makes use of feature maps implicitly, is that explicit mappings
can be better suited for online learning and can significantly reduce the cost
of learning with very large datasets.
Standard kernelized SVMs do not scale well to large datasets, but using an
approximate kernel map it is possible to use much more efficient linear SVMs.
In particular, the combination of kernel map approximations with
SGDClassifier can make non-linear learning on large datasets possible.
Since there has not been much empirical work using approximate embeddings, it
is advisable to compare results against exact kernel methods when possible.

See also
Polynomial regression: extending linear models with basis functions for an exact polynomial transformation.


6.7.1. Nystroem Method for Kernel Approximation#
The Nystroem method, as implemented in Nystroem is a general method for
reduced rank approximations of kernels. It achieves this by subsampling without
replacement rows/columns of the data on which the kernel is evaluated. While the
computational complexity of the exact method is
\(\mathcal{O}(n^3_{\text{samples}})\), the complexity of the approximation
is \(\mathcal{O}(n^2_{\text{components}} \cdot n_{\text{samples}})\), where
one can set \(n_{\text{components}} \ll n_{\text{samples}}\) without a
significative decrease in performance [WS2001].
We can construct the eigendecomposition of the kernel matrix \(K\), based
on the features of the data, and then split it into sampled and unsampled data
points.

\[\begin{split}K = U \Lambda U^T
= \begin{bmatrix} U_1 \\ U_2\end{bmatrix} \Lambda \begin{bmatrix} U_1 \\ U_2 \end{bmatrix}^T
= \begin{bmatrix} U_1 \Lambda U_1^T & U_1 \Lambda U_2^T \\ U_2 \Lambda U_1^T & U_2 \Lambda U_2^T \end{bmatrix}
\equiv \begin{bmatrix} K_{11} & K_{12} \\ K_{21} & K_{22} \end{bmatrix}\end{split}\]
where:

\(U\) is orthonormal
\(\Lambda\) is diagonal matrix of eigenvalues
\(U_1\) is orthonormal matrix of samples that were chosen
\(U_2\) is orthonormal matrix of samples that were not chosen

Given that \(U_1 \Lambda U_1^T\) can be obtained by orthonormalization of
the matrix \(K_{11}\), and \(U_2 \Lambda U_1^T\) can be evaluated (as
well as its transpose), the only remaining term to elucidate is
\(U_2 \Lambda U_2^T\). To do this we can express it in terms of the already
evaluated matrices:

\[\begin{split}\begin{align} U_2 \Lambda U_2^T &= \left(K_{21} U_1 \Lambda^{-1}\right) \Lambda \left(K_{21} U_1 \Lambda^{-1}\right)^T
\\&= K_{21} U_1 (\Lambda^{-1} \Lambda) \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} U_1 \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} K_{11}^{-1} K_{21}^T
\\&= \left( K_{21} K_{11}^{-\frac12} \right) \left( K_{21} K_{11}^{-\frac12} \right)^T
.\end{align}\end{split}\]
During fit, the class Nystroem evaluates the basis \(U_1\), and
computes the normalization constant, \(K_{11}^{-\frac12}\). Later, during
transform, the kernel matrix is determined between the basis (given by the
components_ attribute) and the new data points, X. This matrix is then
multiplied by the normalization_ matrix for the final result.
By default Nystroem uses the rbf kernel, but it can use any kernel
function or a precomputed kernel matrix. The number of samples used - which is
also the dimensionality of the features computed - is given by the parameter
n_components.
Examples

See the example entitled
Time-related feature engineering,
that shows an efficient machine learning pipeline that uses a
Nystroem kernel.



6.7.2. Radial Basis Function Kernel#
The RBFSampler constructs an approximate mapping for the radial basis
function kernel, also known as Random Kitchen Sinks [RR2007]. This
transformation can be used to explicitly model a kernel map, prior to applying
a linear algorithm, for example a linear SVM:
>>> from sklearn.kernel_approximation import RBFSampler
>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
>>> y = [0, 0, 1, 1]
>>> rbf_feature = RBFSampler(gamma=1, random_state=1)
>>> X_features = rbf_feature.fit_transform(X)
>>> clf = SGDClassifier(max_iter=5)
>>> clf.fit(X_features, y)
SGDClassifier(max_iter=5)
>>> clf.score(X_features, y)
1.0


The mapping relies on a Monte Carlo approximation to the
kernel values. The fit function performs the Monte Carlo sampling, whereas
the transform method performs the mapping of the data.  Because of the
inherent randomness of the process, results may vary between different calls to
the fit function.
The fit function takes two arguments:
n_components, which is the target dimensionality of the feature transform,
and gamma, the parameter of the RBF-kernel.  A higher n_components will
result in a better approximation of the kernel and will yield results more
similar to those produced by a kernel SVM. Note that “fitting” the feature
function does not actually depend on the data given to the fit function.
Only the dimensionality of the data is used.
Details on the method can be found in [RR2007].
For a given value of n_components RBFSampler is often less accurate
as Nystroem. RBFSampler is cheaper to compute, though, making
use of larger feature spaces more efficient.




Comparing an exact RBF kernel (left) with the approximation (right)#


Examples

Explicit feature map approximation for RBF kernels



6.7.3. Additive Chi Squared Kernel#
The additive chi squared kernel is a kernel on histograms, often used in computer vision.
The additive chi squared kernel as used here is given by

\[k(x, y) = \sum_i \frac{2x_iy_i}{x_i+y_i}\]
This is not exactly the same as sklearn.metrics.pairwise.additive_chi2_kernel.
The authors of [VZ2010] prefer the version above as it is always positive
definite.
Since the kernel is additive, it is possible to treat all components
\(x_i\) separately for embedding. This makes it possible to sample
the Fourier transform in regular intervals, instead of approximating
using Monte Carlo sampling.
The class AdditiveChi2Sampler implements this component wise
deterministic sampling. Each component is sampled \(n\) times, yielding
\(2n+1\) dimensions per input dimension (the multiple of two stems
from the real and complex part of the Fourier transform).
In the literature, \(n\) is usually chosen to be 1 or 2, transforming
the dataset to size n_samples * 5 * n_features (in the case of \(n=2\)).
The approximate feature map provided by AdditiveChi2Sampler can be combined
with the approximate feature map provided by RBFSampler to yield an approximate
feature map for the exponentiated chi squared kernel.
See the [VZ2010] for details and [VVZ2010] for combination with the RBFSampler.


6.7.4. Skewed Chi Squared Kernel#
The skewed chi squared kernel is given by:

\[k(x,y) = \prod_i \frac{2\sqrt{x_i+c}\sqrt{y_i+c}}{x_i + y_i + 2c}\]
It has properties that are similar to the exponentiated chi squared kernel
often used in computer vision, but allows for a simple Monte Carlo
approximation of the feature map.
The usage of the SkewedChi2Sampler is the same as the usage described
above for the RBFSampler. The only difference is in the free
parameter, that is called \(c\).
For a motivation for this mapping and the mathematical details see [LS2010].


6.7.5. Polynomial Kernel Approximation via Tensor Sketch#
The polynomial kernel is a popular type of kernel
function given by:

\[k(x, y) = (\gamma x^\top y +c_0)^d\]
where:

x, y are the input vectors
d is the kernel degree

Intuitively, the feature space of the polynomial kernel of degree d
consists of all possible degree-d products among input features, which enables
learning algorithms using this kernel to account for interactions between features.
The TensorSketch [PP2013] method, as implemented in PolynomialCountSketch, is a
scalable, input data independent method for polynomial kernel approximation.
It is based on the concept of Count sketch [WIKICS] [CCF2002] , a dimensionality
reduction technique similar to feature hashing, which instead uses several
independent hash functions. TensorSketch obtains a Count Sketch of the outer product
of two vectors (or a vector with itself), which can be used as an approximation of the
polynomial kernel feature space. In particular, instead of explicitly computing
the outer product, TensorSketch computes the Count Sketch of the vectors and then
uses polynomial multiplication via the Fast Fourier Transform to compute the
Count Sketch of their outer product.
Conveniently, the training phase of TensorSketch simply consists of initializing
some random variables. It is thus independent of the input data, i.e. it only
depends on the number of input features, but not the data values.
In addition, this method can transform samples in
\(\mathcal{O}(n_{\text{samples}}(n_{\text{features}} + n_{\text{components}} \log(n_{\text{components}})))\)
time, where \(n_{\text{components}}\) is the desired output dimension,
determined by n_components.
Examples

Scalable learning with polynomial kernel approximation



6.7.6. Mathematical Details#
Kernel methods like support vector machines or kernelized
PCA rely on a property of reproducing kernel Hilbert spaces.
For any positive definite kernel function \(k\) (a so called Mercer kernel),
it is guaranteed that there exists a mapping \(\phi\)
into a Hilbert space \(\mathcal{H}\), such that

\[k(x,y) = \langle \phi(x), \phi(y) \rangle\]
Where \(\langle \cdot, \cdot \rangle\) denotes the inner product in the
Hilbert space.
If an algorithm, such as a linear support vector machine or PCA,
relies only on the scalar product of data points \(x_i\), one may use
the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm
to the mapped data points \(\phi(x_i)\).
The advantage of using \(k\) is that the mapping \(\phi\) never has
to be calculated explicitly, allowing for arbitrary large
features (even infinite).
One drawback of kernel methods is, that it might be necessary
to store many kernel values \(k(x_i, x_j)\) during optimization.
If a kernelized classifier is applied to new data \(y_j\),
\(k(x_i, y_j)\) needs to be computed to make predictions,
possibly for many different \(x_i\) in the training set.
The classes in this submodule allow to approximate the embedding
\(\phi\), thereby working explicitly with the representations
\(\phi(x_i)\), which obviates the need to apply the kernel
or store training examples.
References


[WS2001]
“Using the Nyström method to speed up kernel machines”
Williams, C.K.I.; Seeger, M. - 2001.


[RR2007]
(1,2)
“Random features for large-scale kernel machines”
Rahimi, A. and Recht, B. - Advances in neural information processing 2007,


[LS2010]
“Random Fourier approximations for skewed multiplicative histogram kernels”
Li, F., Ionescu, C., and Sminchisescu, C.
- Pattern Recognition,  DAGM 2010, Lecture Notes in Computer Science.


[VZ2010]
(1,2)
“Efficient additive kernels via explicit feature maps”
Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010


[VVZ2010]
“Generalized RBF feature maps for Efficient Detection”
Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010


[PP2013]
“Fast and scalable polynomial kernels via explicit feature maps”
Pham, N., & Pagh, R. - 2013


[CCF2002]
“Finding frequent items in data streams”
Charikar, M., Chen, K., & Farach-Colton - 2002


[WIKICS]
“Wikipedia: Count sketch”












previous
6.6. Random Projection




next
6.8. Pairwise metrics, Affinities and Kernels










 On this page
  


6.7.1. Nystroem Method for Kernel Approximation
6.7.2. Radial Basis Function Kernel
6.7.3. Additive Chi Squared Kernel
6.7.4. Skewed Chi Squared Kernel
6.7.5. Polynomial Kernel Approximation via Tensor Sketch
6.7.6. Mathematical Details





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Nystroem Kernel SVM Regressor using Logistic Regression,"Approximate Kernel Support Vector Regressor using sklearn Ridge. Support vector machines are a class of maximum margin models. They seek to maximize the separation they find between classes, and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations."
Light GBM Regressor with GBDT with Boosting on Residuals,https://github.com/Microsoft/LightGBM/blob/master/docs/README.md,Light GBM Regressor with GBDT with Boosting on Residuals,RES_PLGBMTR,Boosted Regression,no documentation retrieved,NUM,Light Gradient Boosting on ElasticNet Predictions,Light GBM Regressor with GBDT with Boosting on Residuals
Gradient Boosting Regressor (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Regressor (xgboost) with Early-Stopping,XL_XGBR2,Boosted Regression,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Regressor (Boosted Predictions),Gradient Boosting Regressor (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Gradient Boosting Regressor (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Regressor (xgboost) with Early-Stopping,XL_ESXGBR2,Boosted Regression,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Regressor with Early Stopping,Gradient Boosting Regressor (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Text fit on Residuals. A ElasticNet Regressor is trained on text features with as offset the raw predictions of the main model,no url,Text fit on Residuals,XL_ENETCDWC,Boosted Regression,no documentation retrieved,NUM,Text fit on Residuals (L1 /  Least-Squares Loss),Text fit on Residuals. A ElasticNet Regressor is trained on text features with as offset the raw predictions of the main model
Gradient Boosting Regressor (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Regressor (xgboost) with Early-Stopping,XL_PXGBR2,Boosted Regression,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Regressor,Gradient Boosting Regressor (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Light GBM Regressor with Early Stopping with GBDT and Boosting on Residuals,https://github.com/Microsoft/LightGBM/blob/master/docs/README.md,Light GBM Regressor with Early Stopping with GBDT and Boosting on Residuals,RES_ESLGBMTR,Boosted Regression,no documentation retrieved,NUM,Light Gradient Boosting Regression on ElasticNet Predictions,Light GBM Regressor with Early Stopping with GBDT and Boosting on Residuals
Boosting on ElasticNet Predictions. A Extreme Gradient Boosting Machine is trained with as offset the raw predictions of the main model,no url,Boosting on ElasticNet Predictions,RES_ESXGBR2,Boosted Regression,no documentation retrieved,NUM,eXtreme Gradient Boosting Regression on ElasticNet Predictions,Boosting on ElasticNet Predictions. A Extreme Gradient Boosting Machine is trained with as offset the raw predictions of the main model
Boosting on ElasticNet Predictions. A Extreme Gradient Boosting Machine is trained with as offset the raw predictions of the main model,no url,Boosting on ElasticNet Predictions,RES_XGBR2,Boosted Regression,no documentation retrieved,NUM,eXtreme Gradient Boosting on ElasticNet Predictions,Boosting on ElasticNet Predictions. A Extreme Gradient Boosting Machine is trained with as offset the raw predictions of the main model
Text fit on Residuals. A ElasticNet Regressor is trained on text features with as offset the raw predictions of the main model,no url,Text fit on Residuals,XL_ENETCD,Boosted Regression,no documentation retrieved,NUM,Text fit on Residuals (L1 /  Least-Squares Loss),Text fit on Residuals. A ElasticNet Regressor is trained on text features with as offset the raw predictions of the main model
Tunes word n-grams and generates out-of-sample predictions,no url,Tunes word n-grams and generates out-of-sample predictions,WNGEC2,Binary Classification,no documentation retrieved,NUM,Auto-Tuned N-Gram Text Classifier using token counts,Tunes word n-grams and generates out-of-sample predictions
"Keras-based Image Fine-Tune Regressor
    
    The module uses Convolutional Neural Networks (CNNs) to employ transfer learning (""fine-tuning"") for
    image variable types. It initializes the CNNs using pretrained weights and proceeds to train and
    fine-tune those weights based on the dataset.
    
    Transfer learning methodologies rely on different elements, the two most
    essential of which are task size and similarity to the pretrained image dataset. Becaise Deep CNN
    (DCNN) features are more generic in early layers and more dataset-specific in later layers, the
    module is capable of only fine-tuning the higher level layers of the network.
    This is driven by the understanding that the prior features of a DCNN contain more generic image
    features that are useful to many tasks, but later layers of the Deep CNN become progressively more
    specific to the exact elements of the classes relevant in the given task.
    
    Parameters
    ----------
    Pretrained model (model_name): select (default=squeezenet)
        Model architecture trained on a pretrained dataset.
    
        ``values: ['squeezenet', 'resnet50', 'xception', 'efficientnet-b0', 'efficientnet-b4']``
    Batch size (batch_size): int (default=32)
        Mini-batch size for training.
    
        ``values: [0, 32]``
    Number of epoch (epoch): int (default=1000)
        Number of epochs for training.
    
        ``values: [0, 1000]``
    Early Stop parameter (earlystop_patience): int (default=5)
        Early stopping window. Number of epochs that stops the model from training when the validation
        score consecutively fails to improve
    
        ``values: [0, 1000]``
    reduce_lr_on_plateau (reduce_lr_on_plateau): selectgrid (default=False)
        When True, reduces the learning rate when a metric has stopped improving.
    reduce_lr_patience (reduce_lr_patience): intgrid (default=3)
        Number of epochs to wait before reducing the learning rate.
    reduce_lr_factor (reduce_lr_factor): float (default=0.2)
        Factor by which the learning rate will be reduced. Specifically `new_lr = lr * factor`.
    
        ``values: [0, 1]``
    weights_initialization (weights_initialization): select (default=pretrained)
        Whether to use a randomly initialized model or pretrained weights as a starting point before
        fine-tuning the model base.
    
        ``values: ['random', 'pretrained']``
    optimizer (optimizer): select (default=rmsprop)
        Name of the optimizer.
    
        ``values: ['rmsprop', 'adadelta', 'adagrad', 'adam', 'momentum', 'sgd', 'adam_tf']``
    use_discriminative_learning_rate (use_discriminative_learning_rate): selectgrid (default=False)
        When True (default is False), uses a different learning rate for each trainable layer. By
        default, DataRobot uses cosine learning rate decay structure (decayed to 0.0 learning rate) to
        decay learning rate by layer. If not all layers are trainable, learning rate decay proceeds by
        layer number. If all the layers are trainable, learning rate decays by convolutional
        blocks/groups specific to the model architecture chosen.
    
        ``values: ['True', 'False']``
    learning rate (learning_rate_init): float (default='auto')
        Initial learning rate. When value is set to auto, we set the initial learning rate is based on
        the chosen optimizer.
    
        ``values: {'float': [0, 1], 'select': ['auto']}``
    Trainable scope (trainable_scope): multi (default='all')
        Number of layers to enable training the weights of the base CNN model in fine-tune modelers.
        Either:
    
        integers: enable the last convolutional layers of the chosen network to be trainable
    
        all: All learnable layers are trainable
    
        chain_thaw: First, fine-tunes any new layers (often only a Softmax layer)
        to the target task until convergence on a validation set. Then, fine-tunes each
        layer individually starting from the first layer in the network. Last, the entire model is
        trained with all layers. Each time the model converges as measured on the validation set, the
        weights are reloaded to the best setting, thereby preventing overfitting to any layer.
    
        ``values: {'int': [0, 100], 'select': ['all', 'chain_thaw']}``
    featurizer_pool (featurizer_pool): select (default=avg)
        Type of summarizer to use to squash the multi-dimensional CNN features
        applied on initial, intermediate, and top convolutional layers of the network.
    
        ``values: ['avg', 'gem', 'max']``
    image_aug_list_id (image_aug_list_id): select (default=None)
        ID of the augmentation list used to control the transformations applied to the images
    
        ``values: [None]``
    loss (loss): select (default=mean_squared_error)
        The type of loss used for tuning the model.
    
        ``values: ['gaussian', 'poisson', 'tweedie', 'mean_absolute_error', 'mean_squared_error',
        'root_mean_squared_error']``
    
    References
    ----------
    .. [1] Jeremy Howard and Sebastian Ruder.
       ""Universal language  model  fine-tuning  for  text  classification"".
       arXiv preprint arXiv:1801.06146.
       `[link]
       <https://arxiv.org/pdf/1801.06146v5.pdf>`__
    .. [2] Pan, S. J., and Yang, Q.
       ""A Survey on Transfer Learning""
       Knowledge and Data Engineering, IEEE Transactionson 22 (10): 1345-1359.
       `[link]
       <https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf>`__
    
    See Also
    --------
    Source:
        `Convolutional Neural Network wikipedia
        <https://en.wikipedia.org/wiki/Convolutional_neural_network>`_",no url,Keras-based Image Fine-Tune Regressor,IMGFTR,Binary Classification,no documentation retrieved,NUM,Fine-Tuned Image Regressor (All Layers),Image Regression using pre-trained deep neural network models.
Naive Bayes Classifier for numeric inputs (scikit-learn). Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes theorem with the “naive” assumption of independence between every pair of features.,http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB,Naive Bayes Classifier for numeric inputs (scikit-learn),BNBC,Binary Classification,"













BernoulliNB — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.naive_bayes
BernoulliNB









BernoulliNB#


class sklearn.naive_bayes.BernoulliNB(*, alpha=1.0, force_alpha=True, binarize=0.0, fit_prior=True, class_prior=None)[source]#
Naive Bayes classifier for multivariate Bernoulli models.
Like MultinomialNB, this classifier is suitable for discrete data. The
difference is that while MultinomialNB works with occurrence counts,
BernoulliNB is designed for binary/boolean features.
Read more in the User Guide.

Parameters:

alphafloat or array-like of shape (n_features,), default=1.0Additive (Laplace/Lidstone) smoothing parameter
(set alpha=0 and force_alpha=True, for no smoothing).

force_alphabool, default=TrueIf False and alpha is less than 1e-10, it will set alpha to
1e-10. If True, alpha will remain unchanged. This may cause
numerical errors if alpha is too close to 0.

Added in version 1.2.


Changed in version 1.4: The default value of force_alpha changed to True.


binarizefloat or None, default=0.0Threshold for binarizing (mapping to booleans) of sample features.
If None, input is presumed to already consist of binary vectors.

fit_priorbool, default=TrueWhether to learn class prior probabilities or not.
If false, a uniform prior will be used.

class_priorarray-like of shape (n_classes,), default=NonePrior probabilities of the classes. If specified, the priors are not
adjusted according to the data.



Attributes:

class_count_ndarray of shape (n_classes,)Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.

class_log_prior_ndarray of shape (n_classes,)Log probability of each class (smoothed).

classes_ndarray of shape (n_classes,)Class labels known to the classifier

feature_count_ndarray of shape (n_classes, n_features)Number of samples encountered for each (class, feature)
during fitting. This value is weighted by the sample weight when
provided.

feature_log_prob_ndarray of shape (n_classes, n_features)Empirical log probability of features given a class, P(x_i|y).

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

CategoricalNBNaive Bayes classifier for categorical features.

ComplementNBThe Complement Naive Bayes classifier described in Rennie et al. (2003).

GaussianNBGaussian Naive Bayes (GaussianNB).

MultinomialNBNaive Bayes classifier for multinomial models.



References
C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html
A. McCallum and K. Nigam (1998). A comparison of event models for naive
Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for
Text Categorization, pp. 41-48.
V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with
naive Bayes – Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).
Examples
>>> import numpy as np
>>> rng = np.random.RandomState(1)
>>> X = rng.randint(5, size=(6, 100))
>>> Y = np.array([1, 2, 3, 4, 4, 5])
>>> from sklearn.naive_bayes import BernoulliNB
>>> clf = BernoulliNB()
>>> clf.fit(X, Y)
BernoulliNB()
>>> print(clf.predict(X[2:3]))
[3]




fit(X, y, sample_weight=None)[source]#
Fit Naive Bayes classifier according to X, y.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training vectors, where n_samples is the number of samples and
n_features is the number of features.

yarray-like of shape (n_samples,)Target values.

sample_weightarray-like of shape (n_samples,), default=NoneWeights applied to individual samples (1. for unweighted).



Returns:

selfobjectReturns the instance itself.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







partial_fit(X, y, classes=None, sample_weight=None)[source]#
Incremental fit on a batch of samples.
This method is expected to be called several times consecutively
on different chunks of a dataset so as to implement out-of-core
or online learning.
This is especially useful when the whole dataset is too big to fit in
memory at once.
This method has some performance overhead hence it is better to call
partial_fit on chunks of data that are as large as possible
(as long as fitting in the memory budget) to hide the overhead.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training vectors, where n_samples is the number of samples and
n_features is the number of features.

yarray-like of shape (n_samples,)Target values.

classesarray-like of shape (n_classes,), default=NoneList of all the classes that can possibly appear in the y vector.
Must be provided at the first call to partial_fit, can be omitted
in subsequent calls.

sample_weightarray-like of shape (n_samples,), default=NoneWeights applied to individual samples (1. for unweighted).



Returns:

selfobjectReturns the instance itself.







predict(X)[source]#
Perform classification on an array of test vectors X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Cndarray of shape (n_samples,)Predicted target values for X.







predict_joint_log_proba(X)[source]#
Return joint log probability estimates for the test vector X.
For each row x of X and class y, the joint log probability is given by
log P(x, y) = log P(y) + log P(x|y),
where log P(y) is the class prior probability and log P(x|y) is
the class-conditional probability.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Cndarray of shape (n_samples, n_classes)Returns the joint log-probability of the samples for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.







predict_log_proba(X)[source]#
Return log-probability estimates for the test vector X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Carray-like of shape (n_samples, n_classes)Returns the log-probability of the samples for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.







predict_proba(X)[source]#
Return probability estimates for the test vector X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Carray-like of shape (n_samples, n_classes)Returns the probability of the samples for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → BernoulliNB[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_partial_fit_request(*, classes: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → BernoulliNB[source]#
Request metadata passed to the partial_fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to partial_fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to partial_fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

classesstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for classes parameter in partial_fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in partial_fit.



Returns:

selfobjectThe updated object.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → BernoulliNB[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







Gallery examples#

Hashing feature transformation using Totally Random Trees
Hashing feature transformation using Totally Random Trees










previous
sklearn.naive_bayes




next
CategoricalNB










 On this page
  


BernoulliNB
fit
get_metadata_routing
get_params
partial_fit
predict
predict_joint_log_proba
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_partial_fit_request
set_score_request


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Bernoulli Naive Bayes classifier (scikit-learn),Naive Bayes Classifier for numeric inputs (scikit-learn). Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes theorem with the “naive” assumption of independence between every pair of features.
Stochastic Gradient Descent Classifier. Based on scikit-learn. Stochastic Gradient Descent is a extremely scalable method for fitting linear regression models to big data.,http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier,Stochastic Gradient Descent Classifier,SGDC,Binary Classification,"













SGDClassifier — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.linear_model
SGDClassifier









SGDClassifier#


class sklearn.linear_model.SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)[source]#
Linear classifiers (SVM, logistic regression, etc.) with SGD training.
This estimator implements regularized linear models with stochastic
gradient descent (SGD) learning: the gradient of the loss is estimated
each sample at a time and the model is updated along the way with a
decreasing strength schedule (aka learning rate). SGD allows minibatch
(online/out-of-core) learning via the partial_fit method.
For best results using the default learning rate schedule, the data should
have zero mean and unit variance.
This implementation works with data represented as dense or sparse arrays
of floating point values for the features. The model it fits can be
controlled with the loss parameter; by default, it fits a linear support
vector machine (SVM).
The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.
Read more in the User Guide.

Parameters:

loss{‘hinge’, ‘log_loss’, ‘modified_huber’, ‘squared_hinge’,        ‘perceptron’, ‘squared_error’, ‘huber’, ‘epsilon_insensitive’,        ‘squared_epsilon_insensitive’}, default=’hinge’The loss function to be used.

‘hinge’ gives a linear SVM.
‘log_loss’ gives logistic regression, a probabilistic classifier.
‘modified_huber’ is another smooth loss that brings tolerance to
outliers as well as probability estimates.
‘squared_hinge’ is like hinge but is quadratically penalized.
‘perceptron’ is the linear loss used by the perceptron algorithm.
The other losses, ‘squared_error’, ‘huber’, ‘epsilon_insensitive’ and
‘squared_epsilon_insensitive’ are designed for regression but can be useful
in classification as well; see
SGDRegressor for a description.

More details about the losses formulas can be found in the
User Guide.

penalty{‘l2’, ‘l1’, ‘elasticnet’, None}, default=’l2’The penalty (aka regularization term) to be used. Defaults to ‘l2’
which is the standard regularizer for linear SVM models. ‘l1’ and
‘elasticnet’ might bring sparsity to the model (feature selection)
not achievable with ‘l2’. No penalty is added when set to None.

alphafloat, default=0.0001Constant that multiplies the regularization term. The higher the
value, the stronger the regularization. Also used to compute the
learning rate when learning_rate is set to ‘optimal’.
Values must be in the range [0.0, inf).

l1_ratiofloat, default=0.15The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Only used if penalty is ‘elasticnet’.
Values must be in the range [0.0, 1.0].

fit_interceptbool, default=TrueWhether the intercept should be estimated or not. If False, the
data is assumed to be already centered.

max_iterint, default=1000The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the fit method, and not the
partial_fit method.
Values must be in the range [1, inf).

Added in version 0.19.


tolfloat or None, default=1e-3The stopping criterion. If it is not None, training will stop
when (loss > best_loss - tol) for n_iter_no_change consecutive
epochs.
Convergence is checked against the training loss or the
validation loss depending on the early_stopping parameter.
Values must be in the range [0.0, inf).

Added in version 0.19.


shufflebool, default=TrueWhether or not the training data should be shuffled after each epoch.

verboseint, default=0The verbosity level.
Values must be in the range [0, inf).

epsilonfloat, default=0.1Epsilon in the epsilon-insensitive loss functions; only if loss is
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.
For ‘huber’, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.
Values must be in the range [0.0, inf).

n_jobsint, default=NoneThe number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation.
None means 1 unless in a joblib.parallel_backend context.
-1 means using all processors. See Glossary
for more details.

random_stateint, RandomState instance, default=NoneUsed for shuffling the data, when shuffle is set to True.
Pass an int for reproducible output across multiple function calls.
See Glossary.
Integer values must be in the range [0, 2**32 - 1].

learning_ratestr, default=’optimal’The learning rate schedule:

‘constant’: eta = eta0
‘optimal’: eta = 1.0 / (alpha * (t + t0))
where t0 is chosen by a heuristic proposed by Leon Bottou.
‘invscaling’: eta = eta0 / pow(t, power_t)
‘adaptive’: eta = eta0, as long as the training keeps decreasing.
Each time n_iter_no_change consecutive epochs fail to decrease the
training loss by tol or fail to increase validation score by tol if
early_stopping is True, the current learning rate is divided by 5.


Added in version 0.20: Added ‘adaptive’ option





eta0float, default=0.0The initial learning rate for the ‘constant’, ‘invscaling’ or
‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by
the default schedule ‘optimal’.
Values must be in the range [0.0, inf).

power_tfloat, default=0.5The exponent for inverse scaling learning rate.
Values must be in the range (-inf, inf).

early_stoppingbool, default=FalseWhether to use early stopping to terminate training when validation
score is not improving. If set to True, it will automatically set aside
a stratified fraction of training data as validation and terminate
training when validation score returned by the score method is not
improving by at least tol for n_iter_no_change consecutive epochs.

Added in version 0.20: Added ‘early_stopping’ option


validation_fractionfloat, default=0.1The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.
Values must be in the range (0.0, 1.0).

Added in version 0.20: Added ‘validation_fraction’ option


n_iter_no_changeint, default=5Number of iterations with no improvement to wait before stopping
fitting.
Convergence is checked against the training loss or the
validation loss depending on the early_stopping parameter.
Integer values must be in the range [1, max_iter).

Added in version 0.20: Added ‘n_iter_no_change’ option


class_weightdict, {class_label: weight} or “balanced”, default=NonePreset for the class_weight fit parameter.
Weights associated with classes. If not given, all classes
are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as n_samples / (n_classes * np.bincount(y)).

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See the Glossary.
Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.
If a dynamic learning rate is used, the learning rate is adapted
depending on the number of samples already seen. Calling fit resets
this counter, while partial_fit will result in increasing the
existing counter.

averagebool or int, default=FalseWhen set to True, computes the averaged SGD weights across all
updates and stores the result in the coef_ attribute. If set to
an int greater than 1, averaging will begin once the total number of
samples seen reaches average. So average=10 will begin
averaging after seeing 10 samples.
Integer values must be in the range [1, n_samples].



Attributes:

coef_ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)Weights assigned to the features.

intercept_ndarray of shape (1,) if n_classes == 2 else (n_classes,)Constants in decision function.

n_iter_intThe actual number of iterations before reaching the stopping criterion.
For multiclass fits, it is the maximum over every binary fit.

loss_function_concrete LossFunction
Deprecated since version 1.4: Attribute loss_function_ was deprecated in version 1.4 and will be
removed in 1.6.


classes_array of shape (n_classes,)
t_intNumber of weight updates performed during training.
Same as (n_iter_ * n_samples + 1).

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

sklearn.svm.LinearSVCLinear support vector classification.

LogisticRegressionLogistic regression.

PerceptronInherits from SGDClassifier. Perceptron() is equivalent to SGDClassifier(loss=""perceptron"", eta0=1, learning_rate=""constant"", penalty=None).



Examples
>>> import numpy as np
>>> from sklearn.linear_model import SGDClassifier
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.pipeline import make_pipeline
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> Y = np.array([1, 1, 2, 2])
>>> # Always scale the input. The most convenient way is to use a pipeline.
>>> clf = make_pipeline(StandardScaler(),
...                     SGDClassifier(max_iter=1000, tol=1e-3))
>>> clf.fit(X, Y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('sgdclassifier', SGDClassifier())])
>>> print(clf.predict([[-0.8, -1]]))
[1]




decision_function(X)[source]#
Predict confidence scores for samples.
The confidence score for a sample is proportional to the signed
distance of that sample to the hyperplane.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the confidence scores.



Returns:

scoresndarray of shape (n_samples,) or (n_samples, n_classes)Confidence scores per (n_samples, n_classes) combination. In the
binary case, confidence score for self.classes_[1] where >0 means
this class would be predicted.







densify()[source]#
Convert coefficient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the
default format of coef_ and is required for fitting, so calling
this method is only required on models that have previously been
sparsified; otherwise, it is a no-op.

Returns:

selfFitted estimator.







fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)[source]#
Fit linear model with Stochastic Gradient Descent.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Training data.

yndarray of shape (n_samples,)Target values.

coef_initndarray of shape (n_classes, n_features), default=NoneThe initial coefficients to warm-start the optimization.

intercept_initndarray of shape (n_classes,), default=NoneThe initial intercept to warm-start the optimization.

sample_weightarray-like, shape (n_samples,), default=NoneWeights applied to individual samples.
If not provided, uniform weights are assumed. These weights will
be multiplied with class_weight (passed through the
constructor) if class_weight is specified.



Returns:

selfobjectReturns an instance of self.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







partial_fit(X, y, classes=None, sample_weight=None)[source]#
Perform one epoch of stochastic gradient descent on given samples.
Internally, this method uses max_iter = 1. Therefore, it is not
guaranteed that a minimum of the cost function is reached after calling
it once. Matters such as objective convergence, early stopping, and
learning rate adjustments should be handled by the user.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Subset of the training data.

yndarray of shape (n_samples,)Subset of the target values.

classesndarray of shape (n_classes,), default=NoneClasses across all calls to partial_fit.
Can be obtained by via np.unique(y_all), where y_all is the
target vector of the entire dataset.
This argument is required for the first call to partial_fit
and can be omitted in the subsequent calls.
Note that y doesn’t need to contain all labels in classes.

sample_weightarray-like, shape (n_samples,), default=NoneWeights applied to individual samples.
If not provided, uniform weights are assumed.



Returns:

selfobjectReturns an instance of self.







predict(X)[source]#
Predict class labels for samples in X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the predictions.



Returns:

y_predndarray of shape (n_samples,)Vector containing the class labels for each sample.







predict_log_proba(X)[source]#
Log of probability estimates.
This method is only available for log loss and modified Huber loss.
When loss=”modified_huber”, probability estimates may be hard zeros
and ones, so taking the logarithm is not possible.
See predict_proba for details.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Input data for prediction.



Returns:

Tarray-like, shape (n_samples, n_classes)Returns the log-probability of the sample for each class in the
model, where classes are ordered as they are in
self.classes_.







predict_proba(X)[source]#
Probability estimates.
This method is only available for log loss and modified Huber loss.
Multiclass probability estimates are derived from binary (one-vs.-rest)
estimates by simple normalization, as recommended by Zadrozny and
Elkan.
Binary probability estimates for loss=”modified_huber” are given by
(clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions
it is necessary to perform proper probability calibration by wrapping
the classifier with
CalibratedClassifierCV instead.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Input data for prediction.



Returns:

ndarray of shape (n_samples, n_classes)Returns the probability of the sample for each class in the model,
where classes are ordered as they are in self.classes_.




References
Zadrozny and Elkan, “Transforming classifier scores into multiclass
probability estimates”, SIGKDD’02,
https://dl.acm.org/doi/pdf/10.1145/775047.775151
The justification for the formula in the loss=”modified_huber”
case is in the appendix B in:
http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf



score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, coef_init: bool | None | str = '$UNCHANGED$', intercept_init: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → SGDClassifier[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

coef_initstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for coef_init parameter in fit.

intercept_initstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for intercept_init parameter in fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_partial_fit_request(*, classes: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → SGDClassifier[source]#
Request metadata passed to the partial_fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to partial_fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to partial_fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

classesstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for classes parameter in partial_fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in partial_fit.



Returns:

selfobjectThe updated object.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → SGDClassifier[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







sparsify()[source]#
Convert coefficient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for
L1-regularized models can be much more memory- and storage-efficient
than the usual numpy.ndarray representation.
The intercept_ member is not converted.

Returns:

selfFitted estimator.




Notes
For non-sparse models, i.e. when there are not many zeros in coef_,
this may actually increase memory usage, so use this method with
care. A rule of thumb is that the number of zero elements, which can
be computed with (coef_ == 0).sum(), must be more than 50% for this
to provide significant benefits.
After calling this method, further fitting with the partial_fit
method (if any) will not work until you call densify.



Gallery examples#

Model Complexity Influence
Model Complexity Influence

Out-of-core classification of text documents
Out-of-core classification of text documents

Comparing various online solvers
Comparing various online solvers

Early stopping of Stochastic Gradient Descent
Early stopping of Stochastic Gradient Descent

Plot multi-class SGD on the iris dataset
Plot multi-class SGD on the iris dataset

SGD: Maximum margin separating hyperplane
SGD: Maximum margin separating hyperplane

SGD: Penalties
SGD: Penalties

SGD: Weighted samples
SGD: Weighted samples

SGD: convex loss functions
SGD: convex loss functions

Explicit feature map approximation for RBF kernels
Explicit feature map approximation for RBF kernels

Comparing randomized search and grid search for hyperparameter estimation
Comparing randomized search and grid search for hyperparameter estimation

Semi-supervised Classification on a Text Dataset
Semi-supervised Classification on a Text Dataset

Classification of text documents using sparse features
Classification of text documents using sparse features










previous
RidgeClassifierCV




next
SGDOneClassSVM










 On this page
  


SGDClassifier
decision_function
densify
fit
get_metadata_routing
get_params
partial_fit
predict
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_partial_fit_request
set_score_request
sparsify


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Stochastic Gradient Descent Classifier,Stochastic Gradient Descent Classifier. Based on scikit-learn. Stochastic Gradient Descent is a extremely scalable method for fitting linear regression models to big data.
Logistic Regression with no penalty.  Logistic regression is a generalized linear model that uses a logistic link function. Based on lightning CDClassifier,http://contrib.scikit-learn.org/lightning/,Logistic Regression with no penalty,LRCD,Binary Classification,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Logistic Regression,Logistic Regression with no penalty.  Logistic regression is a generalized linear model that uses a logistic link function. Based on lightning CDClassifier
Logistic Regression with L1 or L2 penalty. The L2 (or ridge) penalty tends to shrink regression coefficients while the L1 (or lasso) penalty selects variables. Based on scikit-learn,http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression,Logistic Regression with L1 or L2 penalty,LR1,Binary Classification,"













LogisticRegression — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.linear_model
LogisticRegression









LogisticRegression#


class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)[source]#
Logistic Regression (aka logit, MaxEnt) classifier.
In the multiclass case, the training algorithm uses the one-vs-rest (OvR)
scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the
cross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’.
(Currently the ‘multinomial’ option is supported only by the ‘lbfgs’,
‘sag’, ‘saga’ and ‘newton-cg’ solvers.)
This class implements regularized logistic regression using the
‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers. Note
that regularization is applied by default. It can handle both dense
and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit
floats for optimal performance; any other input format will be converted
(and copied).
The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization
with primal formulation, or no regularization. The ‘liblinear’ solver
supports both L1 and L2 regularization, with a dual formulation only for
the L2 penalty. The Elastic-Net regularization is only supported by the
‘saga’ solver.
Read more in the User Guide.

Parameters:

penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’Specify the norm of the penalty:

None: no penalty is added;
'l2': add a L2 penalty term and it is the default choice;
'l1': add a L1 penalty term;
'elasticnet': both L1 and L2 penalty terms are added.


Warning
Some penalties may not work with some solvers. See the parameter
solver below, to know the compatibility between the penalty and
solver.


Added in version 0.19: l1 penalty with SAGA solver (allowing ‘multinomial’ + L1)


dualbool, default=FalseDual (constrained) or primal (regularized, see also
this equation) formulation. Dual formulation
is only implemented for l2 penalty with liblinear solver. Prefer dual=False when
n_samples > n_features.

tolfloat, default=1e-4Tolerance for stopping criteria.

Cfloat, default=1.0Inverse of regularization strength; must be a positive float.
Like in support vector machines, smaller values specify stronger
regularization.

fit_interceptbool, default=TrueSpecifies if a constant (a.k.a. bias or intercept) should be
added to the decision function.

intercept_scalingfloat, default=1Useful only when the solver ‘liblinear’ is used
and self.fit_intercept is set to True. In this case, x becomes
[x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equal to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic_feature_weight.
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.

class_weightdict or ‘balanced’, default=NoneWeights associated with classes in the form {class_label: weight}.
If not given, all classes are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as n_samples / (n_classes * np.bincount(y)).
Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.

Added in version 0.17: class_weight=’balanced’


random_stateint, RandomState instance, default=NoneUsed when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the
data. See Glossary for details.

solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’},             default=’lbfgs’Algorithm to use in the optimization problem. Default is ‘lbfgs’.
To choose a solver, you might want to consider the following aspects:

For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’
and ‘saga’ are faster for large ones;
For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and
‘lbfgs’ handle multinomial loss;
‘liblinear’ and ‘newton-cholesky’ can only handle binary classification
by default. To apply a one-versus-rest scheme for the multiclass setting
one can wrapt it with the OneVsRestClassifier.
‘newton-cholesky’ is a good choice for n_samples >> n_features,
especially with one-hot encoded categorical features with rare
categories. Be aware that the memory usage of this solver has a quadratic
dependency on n_features because it explicitly computes the Hessian
matrix.


Warning
The choice of the algorithm depends on the penalty chosen and on
(multinomial) multiclass support:


solver
penalty
multinomial multiclass



‘lbfgs’
‘l2’, None
yes

‘liblinear’
‘l1’, ‘l2’
no

‘newton-cg’
‘l2’, None
yes

‘newton-cholesky’
‘l2’, None
no

‘sag’
‘l2’, None
yes

‘saga’
‘elasticnet’, ‘l1’, ‘l2’, None
yes






Note
‘sag’ and ‘saga’ fast convergence is only guaranteed on features
with approximately the same scale. You can preprocess the data with
a scaler from sklearn.preprocessing.


See also
Refer to the User Guide for more information regarding
LogisticRegression and more specifically the
Table
summarizing solver/penalty supports.


Added in version 0.17: Stochastic Average Gradient descent solver.


Added in version 0.19: SAGA solver.


Changed in version 0.22: The default solver changed from ‘liblinear’ to ‘lbfgs’ in 0.22.


Added in version 1.2: newton-cholesky solver.


max_iterint, default=100Maximum number of iterations taken for the solvers to converge.

multi_class{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’If the option chosen is ‘ovr’, then a binary problem is fit for each
label. For ‘multinomial’ the loss minimised is the multinomial loss fit
across the entire probability distribution, even when the data is
binary. ‘multinomial’ is unavailable when solver=’liblinear’.
‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,
and otherwise selects ‘multinomial’.

Added in version 0.18: Stochastic Average Gradient descent solver for ‘multinomial’ case.


Changed in version 0.22: Default changed from ‘ovr’ to ‘auto’ in 0.22.


Deprecated since version 1.5: multi_class was deprecated in version 1.5 and will be removed in 1.7.
From then on, the recommended ‘multinomial’ will always be used for
n_classes >= 3.
Solvers that do not support ‘multinomial’ will raise an error.
Use sklearn.multiclass.OneVsRestClassifier(LogisticRegression()) if you
still want to use OvR.


verboseint, default=0For the liblinear and lbfgs solvers set verbose to any positive
number for verbosity.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
Useless for liblinear solver. See the Glossary.

Added in version 0.17: warm_start to support lbfgs, newton-cg, sag, saga solvers.


n_jobsint, default=NoneNumber of CPU cores used when parallelizing over classes if
multi_class=’ovr’”. This parameter is ignored when the solver is
set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or
not. None means 1 unless in a joblib.parallel_backend
context. -1 means using all processors.
See Glossary for more details.

l1_ratiofloat, default=NoneThe Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only
used if penalty='elasticnet'. Setting l1_ratio=0 is equivalent
to using penalty='l2', while setting l1_ratio=1 is equivalent
to using penalty='l1'. For 0 < l1_ratio <1, the penalty is a
combination of L1 and L2.



Attributes:

classes_ndarray of shape (n_classes, )A list of class labels known to the classifier.

coef_ndarray of shape (1, n_features) or (n_classes, n_features)Coefficient of the features in the decision function.
coef_ is of shape (1, n_features) when the given problem is binary.
In particular, when multi_class='multinomial', coef_ corresponds
to outcome 1 (True) and -coef_ corresponds to outcome 0 (False).

intercept_ndarray of shape (1,) or (n_classes,)Intercept (a.k.a. bias) added to the decision function.
If fit_intercept is set to False, the intercept is set to zero.
intercept_ is of shape (1,) when the given problem is binary.
In particular, when multi_class='multinomial', intercept_
corresponds to outcome 1 (True) and -intercept_ corresponds to
outcome 0 (False).

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


n_iter_ndarray of shape (n_classes,) or (1, )Actual number of iterations for all classes. If binary or multinomial,
it returns only 1 element. For liblinear solver, only the maximum
number of iteration across all classes is given.

Changed in version 0.20: In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
max_iter. n_iter_ will now report at most max_iter.






See also

SGDClassifierIncrementally trained logistic regression (when given the parameter loss=""log_loss"").

LogisticRegressionCVLogistic regression with built-in cross validation.



Notes
The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.
Predict output may not match that of standalone liblinear in certain
cases. See differences from liblinear
in the narrative documentation.
References

L-BFGS-B – Software for Large-scale Bound-constrained OptimizationCiyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.
http://users.iems.northwestern.edu/~nocedal/lbfgsb.html

LIBLINEAR – A Library for Large Linear Classificationhttps://www.csie.ntu.edu.tw/~cjlin/liblinear/

SAG – Mark Schmidt, Nicolas Le Roux, and Francis BachMinimizing Finite Sums with the Stochastic Average Gradient
https://hal.inria.fr/hal-00860051/document

SAGA – Defazio, A., Bach F. & Lacoste-Julien S. (2014).“SAGA: A Fast Incremental Gradient Method With Support
for Non-Strongly Convex Composite Objectives”

Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descentmethods for logistic regression and maximum entropy models.
Machine Learning 85(1-2):41-75.
https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf


Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import LogisticRegression
>>> X, y = load_iris(return_X_y=True)
>>> clf = LogisticRegression(random_state=0).fit(X, y)
>>> clf.predict(X[:2, :])
array([0, 0])
>>> clf.predict_proba(X[:2, :])
array([[9.8...e-01, 1.8...e-02, 1.4...e-08],
       [9.7...e-01, 2.8...e-02, ...e-08]])
>>> clf.score(X, y)
0.97...




decision_function(X)[source]#
Predict confidence scores for samples.
The confidence score for a sample is proportional to the signed
distance of that sample to the hyperplane.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the confidence scores.



Returns:

scoresndarray of shape (n_samples,) or (n_samples, n_classes)Confidence scores per (n_samples, n_classes) combination. In the
binary case, confidence score for self.classes_[1] where >0 means
this class would be predicted.







densify()[source]#
Convert coefficient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the
default format of coef_ and is required for fitting, so calling
this method is only required on models that have previously been
sparsified; otherwise, it is a no-op.

Returns:

selfFitted estimator.







fit(X, y, sample_weight=None)[source]#
Fit the model according to the given training data.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training vector, where n_samples is the number of samples and
n_features is the number of features.

yarray-like of shape (n_samples,)Target vector relative to X.

sample_weightarray-like of shape (n_samples,) default=NoneArray of weights that are assigned to individual samples.
If not provided, then each sample is given unit weight.

Added in version 0.17: sample_weight support to LogisticRegression.




Returns:

selfFitted estimator.




Notes
The SAGA solver supports both float64 and float32 bit arrays.



get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict class labels for samples in X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the predictions.



Returns:

y_predndarray of shape (n_samples,)Vector containing the class labels for each sample.







predict_log_proba(X)[source]#
Predict logarithm of probability estimates.
The returned estimates for all classes are ordered by the
label of classes.

Parameters:

Xarray-like of shape (n_samples, n_features)Vector to be scored, where n_samples is the number of samples and
n_features is the number of features.



Returns:

Tarray-like of shape (n_samples, n_classes)Returns the log-probability of the sample for each class in the
model, where classes are ordered as they are in self.classes_.







predict_proba(X)[source]#
Probability estimates.
The returned estimates for all classes are ordered by the
label of classes.
For a multi_class problem, if multi_class is set to be “multinomial”
the softmax function is used to find the predicted probability of
each class.
Else use a one-vs-rest approach, i.e. calculate the probability
of each class assuming it to be positive using the logistic function
and normalize these values across all the classes.

Parameters:

Xarray-like of shape (n_samples, n_features)Vector to be scored, where n_samples is the number of samples and
n_features is the number of features.



Returns:

Tarray-like of shape (n_samples, n_classes)Returns the probability of the sample for each class in the model,
where classes are ordered as they are in self.classes_.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → LogisticRegression[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → LogisticRegression[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







sparsify()[source]#
Convert coefficient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for
L1-regularized models can be much more memory- and storage-efficient
than the usual numpy.ndarray representation.
The intercept_ member is not converted.

Returns:

selfFitted estimator.




Notes
For non-sparse models, i.e. when there are not many zeros in coef_,
this may actually increase memory usage, so use this method with
care. A rule of thumb is that the number of zero elements, which can
be computed with (coef_ == 0).sum(), must be more than 50% for this
to provide significant benefits.
After calling this method, further fitting with the partial_fit
method (if any) will not work until you call densify.



Gallery examples#

Release Highlights for scikit-learn 1.5
Release Highlights for scikit-learn 1.5

Release Highlights for scikit-learn 1.3
Release Highlights for scikit-learn 1.3

Release Highlights for scikit-learn 1.1
Release Highlights for scikit-learn 1.1

Release Highlights for scikit-learn 1.0
Release Highlights for scikit-learn 1.0

Release Highlights for scikit-learn 0.24
Release Highlights for scikit-learn 0.24

Release Highlights for scikit-learn 0.23
Release Highlights for scikit-learn 0.23

Release Highlights for scikit-learn 0.22
Release Highlights for scikit-learn 0.22

Probability Calibration curves
Probability Calibration curves

Plot classification probability
Plot classification probability

Feature transformations with ensembles of trees
Feature transformations with ensembles of trees

Plot class probabilities calculated by the VotingClassifier
Plot class probabilities calculated by the VotingClassifier

Model-based and sequential feature selection
Model-based and sequential feature selection

Recursive feature elimination
Recursive feature elimination

Recursive feature elimination with cross-validation
Recursive feature elimination with cross-validation

Comparing various online solvers
Comparing various online solvers

L1 Penalty and Sparsity in Logistic Regression
L1 Penalty and Sparsity in Logistic Regression

Logistic Regression 3-class Classifier
Logistic Regression 3-class Classifier

Logistic function
Logistic function

MNIST classification using multinomial logistic + L1
MNIST classification using multinomial logistic + L1

Multiclass sparse logistic regression on 20newgroups
Multiclass sparse logistic regression on 20newgroups

Plot multinomial and One-vs-Rest Logistic Regression
Plot multinomial and One-vs-Rest Logistic Regression

Regularization path of L1- Logistic Regression
Regularization path of L1- Logistic Regression

Displaying Pipelines
Displaying Pipelines

Displaying estimators and complex pipelines
Displaying estimators and complex pipelines

Introducing the set_output API
Introducing the set_output API

Visualizations with Display Objects
Visualizations with Display Objects

Class Likelihood Ratios to measure classification performance
Class Likelihood Ratios to measure classification performance

Multiclass Receiver Operating Characteristic (ROC)
Multiclass Receiver Operating Characteristic (ROC)

Post-hoc tuning the cut-off point of decision function
Post-hoc tuning the cut-off point of decision function

Post-tuning the decision threshold for cost-sensitive learning
Post-tuning the decision threshold for cost-sensitive learning

Multilabel classification using a classifier chain
Multilabel classification using a classifier chain

Restricted Boltzmann Machine features for digit classification
Restricted Boltzmann Machine features for digit classification

Column Transformer with Mixed Types
Column Transformer with Mixed Types

Pipelining: chaining a PCA and a logistic regression
Pipelining: chaining a PCA and a logistic regression

Feature discretization
Feature discretization

Digits Classification Exercise
Digits Classification Exercise

Classification of text documents using sparse features
Classification of text documents using sparse features










previous
sklearn.linear_model




next
LogisticRegressionCV










 On this page
  


LogisticRegression
decision_function
densify
fit
get_metadata_routing
get_params
predict
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_score_request
sparsify


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Regularized Logistic Regression (L2),Logistic Regression with L1 or L2 penalty. The L2 (or ridge) penalty tends to shrink regression coefficients while the L1 (or lasso) penalty selects variables. Based on scikit-learn
Gradient Boosting Classifier (xgboost) with Early-Stopping and Unsupervised Learning features. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Classifier (xgboost) with Early-Stopping and Unsupervised Learning features,UESXGBC2,Binary Classification,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features,Gradient Boosting Classifier (xgboost) with Early-Stopping and Unsupervised Learning features. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Naive Bayes Classifier for numeric inputs (scikit-learn). Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes theorem with the “naive” assumption of independence between every pair of features.,http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB,Naive Bayes Classifier for numeric inputs (scikit-learn),MNBC,Binary Classification,"













MultinomialNB — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.naive_bayes
MultinomialNB









MultinomialNB#


class sklearn.naive_bayes.MultinomialNB(*, alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None)[source]#
Naive Bayes classifier for multinomial models.
The multinomial Naive Bayes classifier is suitable for classification with
discrete features (e.g., word counts for text classification). The
multinomial distribution normally requires integer feature counts. However,
in practice, fractional counts such as tf-idf may also work.
Read more in the User Guide.

Parameters:

alphafloat or array-like of shape (n_features,), default=1.0Additive (Laplace/Lidstone) smoothing parameter
(set alpha=0 and force_alpha=True, for no smoothing).

force_alphabool, default=TrueIf False and alpha is less than 1e-10, it will set alpha to
1e-10. If True, alpha will remain unchanged. This may cause
numerical errors if alpha is too close to 0.

Added in version 1.2.


Changed in version 1.4: The default value of force_alpha changed to True.


fit_priorbool, default=TrueWhether to learn class prior probabilities or not.
If false, a uniform prior will be used.

class_priorarray-like of shape (n_classes,), default=NonePrior probabilities of the classes. If specified, the priors are not
adjusted according to the data.



Attributes:

class_count_ndarray of shape (n_classes,)Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.

class_log_prior_ndarray of shape (n_classes,)Smoothed empirical log probability for each class.

classes_ndarray of shape (n_classes,)Class labels known to the classifier

feature_count_ndarray of shape (n_classes, n_features)Number of samples encountered for each (class, feature)
during fitting. This value is weighted by the sample weight when
provided.

feature_log_prob_ndarray of shape (n_classes, n_features)Empirical log probability of features
given a class, P(x_i|y).

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

BernoulliNBNaive Bayes classifier for multivariate Bernoulli models.

CategoricalNBNaive Bayes classifier for categorical features.

ComplementNBComplement Naive Bayes classifier.

GaussianNBGaussian Naive Bayes.



References
C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html
Examples
>>> import numpy as np
>>> rng = np.random.RandomState(1)
>>> X = rng.randint(5, size=(6, 100))
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB()
>>> clf.fit(X, y)
MultinomialNB()
>>> print(clf.predict(X[2:3]))
[3]




fit(X, y, sample_weight=None)[source]#
Fit Naive Bayes classifier according to X, y.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training vectors, where n_samples is the number of samples and
n_features is the number of features.

yarray-like of shape (n_samples,)Target values.

sample_weightarray-like of shape (n_samples,), default=NoneWeights applied to individual samples (1. for unweighted).



Returns:

selfobjectReturns the instance itself.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







partial_fit(X, y, classes=None, sample_weight=None)[source]#
Incremental fit on a batch of samples.
This method is expected to be called several times consecutively
on different chunks of a dataset so as to implement out-of-core
or online learning.
This is especially useful when the whole dataset is too big to fit in
memory at once.
This method has some performance overhead hence it is better to call
partial_fit on chunks of data that are as large as possible
(as long as fitting in the memory budget) to hide the overhead.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training vectors, where n_samples is the number of samples and
n_features is the number of features.

yarray-like of shape (n_samples,)Target values.

classesarray-like of shape (n_classes,), default=NoneList of all the classes that can possibly appear in the y vector.
Must be provided at the first call to partial_fit, can be omitted
in subsequent calls.

sample_weightarray-like of shape (n_samples,), default=NoneWeights applied to individual samples (1. for unweighted).



Returns:

selfobjectReturns the instance itself.







predict(X)[source]#
Perform classification on an array of test vectors X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Cndarray of shape (n_samples,)Predicted target values for X.







predict_joint_log_proba(X)[source]#
Return joint log probability estimates for the test vector X.
For each row x of X and class y, the joint log probability is given by
log P(x, y) = log P(y) + log P(x|y),
where log P(y) is the class prior probability and log P(x|y) is
the class-conditional probability.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Cndarray of shape (n_samples, n_classes)Returns the joint log-probability of the samples for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.







predict_log_proba(X)[source]#
Return log-probability estimates for the test vector X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Carray-like of shape (n_samples, n_classes)Returns the log-probability of the samples for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.







predict_proba(X)[source]#
Return probability estimates for the test vector X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Carray-like of shape (n_samples, n_classes)Returns the probability of the samples for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → MultinomialNB[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_partial_fit_request(*, classes: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → MultinomialNB[source]#
Request metadata passed to the partial_fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to partial_fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to partial_fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

classesstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for classes parameter in partial_fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in partial_fit.



Returns:

selfobjectThe updated object.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → MultinomialNB[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







Gallery examples#

Out-of-core classification of text documents
Out-of-core classification of text documents










previous
GaussianNB




next
sklearn.neighbors










 On this page
  


MultinomialNB
fit
get_metadata_routing
get_params
partial_fit
predict
predict_joint_log_proba
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_partial_fit_request
set_score_request


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Multinomial Naive Bayes classifier (scikit-learn),Naive Bayes Classifier for numeric inputs (scikit-learn). Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes theorem with the “naive” assumption of independence between every pair of features.
AdaBoost Classifier (scikit-learn). An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.,http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier,AdaBoost Classifier (scikit-learn),ABC,Binary Classification,"













AdaBoostClassifier — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.ensemble
AdaBoostClassifier









AdaBoostClassifier#


class sklearn.ensemble.AdaBoostClassifier(estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)[source]#
An AdaBoost classifier.
An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
classifier on the original dataset and then fits additional copies of the
classifier on the same dataset but where the weights of incorrectly
classified instances are adjusted such that subsequent classifiers focus
more on difficult cases.
This class implements the algorithm based on [2].
Read more in the User Guide.

Added in version 0.14.


Parameters:

estimatorobject, default=NoneThe base estimator from which the boosted ensemble is built.
Support for sample weighting is required, as well as proper
classes_ and n_classes_ attributes. If None, then
the base estimator is DecisionTreeClassifier
initialized with max_depth=1.

Added in version 1.2: base_estimator was renamed to estimator.


n_estimatorsint, default=50The maximum number of estimators at which boosting is terminated.
In case of perfect fit, the learning procedure is stopped early.
Values must be in the range [1, inf).

learning_ratefloat, default=1.0Weight applied to each classifier at each boosting iteration. A higher
learning rate increases the contribution of each classifier. There is
a trade-off between the learning_rate and n_estimators parameters.
Values must be in the range (0.0, inf).

algorithm{‘SAMME’, ‘SAMME.R’}, default=’SAMME.R’If ‘SAMME.R’ then use the SAMME.R real boosting algorithm.
estimator must support calculation of class probabilities.
If ‘SAMME’ then use the SAMME discrete boosting algorithm.
The SAMME.R algorithm typically converges faster than SAMME,
achieving a lower test error with fewer boosting iterations.

Deprecated since version 1.4: ""SAMME.R"" is deprecated and will be removed in version 1.6.
‘“SAMME”’ will become the default.


random_stateint, RandomState instance or None, default=NoneControls the random seed given at each estimator at each
boosting iteration.
Thus, it is only used when estimator exposes a random_state.
Pass an int for reproducible output across multiple function calls.
See Glossary.



Attributes:

estimator_estimatorThe base estimator from which the ensemble is grown.

Added in version 1.2: base_estimator_ was renamed to estimator_.


estimators_list of classifiersThe collection of fitted sub-estimators.

classes_ndarray of shape (n_classes,)The classes labels.

n_classes_intThe number of classes.

estimator_weights_ndarray of floatsWeights for each estimator in the boosted ensemble.

estimator_errors_ndarray of floatsClassification error for each estimator in the boosted
ensemble.

feature_importances_ndarray of shape (n_features,)The impurity-based feature importances.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

AdaBoostRegressorAn AdaBoost regressor that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction.

GradientBoostingClassifierGB builds an additive model in a forward stage-wise fashion. Regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.

sklearn.tree.DecisionTreeClassifierA non-parametric supervised learning method used for classification. Creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.



References


[1]
Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of
on-Line Learning and an Application to Boosting”, 1995.


[2]
J. Zhu, H. Zou, S. Rosset, T. Hastie, “Multi-class adaboost.”
Statistics and its Interface 2.3 (2009): 349-360.


Examples
>>> from sklearn.ensemble import AdaBoostClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = AdaBoostClassifier(n_estimators=100, algorithm=""SAMME"", random_state=0)
>>> clf.fit(X, y)
AdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)
>>> clf.predict([[0, 0, 0, 0]])
array([1])
>>> clf.score(X, y)
0.96...


For a detailed example of using AdaBoost to fit a sequence of DecisionTrees
as weaklearners, please refer to
Multi-class AdaBoosted Decision Trees.
For a detailed example of using AdaBoost to fit a non-linearly seperable
classification dataset composed of two Gaussian quantiles clusters, please
refer to Two-class AdaBoost.


decision_function(X)[source]#
Compute the decision function of X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.



Returns:

scorendarray of shape of (n_samples, k)The decision function of the input samples. The order of
outputs is the same as that of the classes_ attribute.
Binary classification is a special cases with k == 1,
otherwise k==n_classes. For binary classification,
values closer to -1 or 1 mean more like the first or second
class in classes_, respectively.







property feature_importances_#
The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.
Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
sklearn.inspection.permutation_importance as an alternative.

Returns:

feature_importances_ndarray of shape (n_features,)The feature importances.







fit(X, y, sample_weight=None)[source]#
Build a boosted classifier/regressor from the training set (X, y).

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.

yarray-like of shape (n_samples,)The target values.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights. If None, the sample weights are initialized to
1 / n_samples.



Returns:

selfobjectFitted estimator.







get_metadata_routing()[source]#
Raise NotImplementedError.
This estimator does not support metadata routing yet.



get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict classes for X.
The predicted class of an input sample is computed as the weighted mean
prediction of the classifiers in the ensemble.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.



Returns:

yndarray of shape (n_samples,)The predicted classes.







predict_log_proba(X)[source]#
Predict class log-probabilities for X.
The predicted class log-probabilities of an input sample is computed as
the weighted mean predicted class log-probabilities of the classifiers
in the ensemble.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.



Returns:

pndarray of shape (n_samples, n_classes)The class probabilities of the input samples. The order of
outputs is the same of that of the classes_ attribute.







predict_proba(X)[source]#
Predict class probabilities for X.
The predicted class probabilities of an input sample is computed as
the weighted mean predicted class probabilities of the classifiers
in the ensemble.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.



Returns:

pndarray of shape (n_samples, n_classes)The class probabilities of the input samples. The order of
outputs is the same of that of the classes_ attribute.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → AdaBoostClassifier[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → AdaBoostClassifier[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







staged_decision_function(X)[source]#
Compute decision function of X for each boosting iteration.
This method allows monitoring (i.e. determine error on testing set)
after each boosting iteration.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.



Yields:

scoregenerator of ndarray of shape (n_samples, k)The decision function of the input samples. The order of
outputs is the same of that of the classes_ attribute.
Binary classification is a special cases with k == 1,
otherwise k==n_classes. For binary classification,
values closer to -1 or 1 mean more like the first or second
class in classes_, respectively.







staged_predict(X)[source]#
Return staged predictions for X.
The predicted class of an input sample is computed as the weighted mean
prediction of the classifiers in the ensemble.
This generator method yields the ensemble prediction after each
iteration of boosting and therefore allows monitoring, such as to
determine the prediction on a test set after each boost.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.



Yields:

ygenerator of ndarray of shape (n_samples,)The predicted classes.







staged_predict_proba(X)[source]#
Predict class probabilities for X.
The predicted class probabilities of an input sample is computed as
the weighted mean predicted class probabilities of the classifiers
in the ensemble.
This generator method yields the ensemble predicted class probabilities
after each iteration of boosting and therefore allows monitoring, such
as to determine the predicted class probabilities on a test set after
each boost.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.



Yields:

pgenerator of ndarray of shape (n_samples,)The class probabilities of the input samples. The order of
outputs is the same of that of the classes_ attribute.







staged_score(X, y, sample_weight=None)[source]#
Return staged scores for X, y.
This generator method yields the ensemble score after each iteration of
boosting and therefore allows monitoring, such as to determine the
score on a test set after each boost.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.

yarray-like of shape (n_samples,)Labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Yields:

zfloat






Gallery examples#

Classifier comparison
Classifier comparison

Multi-class AdaBoosted Decision Trees
Multi-class AdaBoosted Decision Trees

Plot the decision surfaces of ensembles of trees on the iris dataset
Plot the decision surfaces of ensembles of trees on the iris dataset

Two-class AdaBoost
Two-class AdaBoost

Multi-class AdaBoosted Decision Trees
Multi-class AdaBoosted Decision Trees










previous
sklearn.ensemble




next
AdaBoostRegressor










 On this page
  


AdaBoostClassifier
decision_function
feature_importances_
fit
get_metadata_routing
get_params
predict
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_score_request
staged_decision_function
staged_predict
staged_predict_proba
staged_score


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Adaboost Classifier,AdaBoost Classifier (scikit-learn). An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.
Keras Neural Network Classifier,no url,Keras Neural Network Classifier,KERASC,Binary Classification,no documentation retrieved,NUM,Keras Neural Network Classifier,Keras Neural Network Classifier
Eureqa Classifier. Eureqa is a proprietary A.I.-powered modeling engine that automates much of the heavy lifting inherent in analytics and data science.,no url,Eureqa Classifier,EQC,Binary Classification,no documentation retrieved,NUM,Eureqa Classifier,Eureqa Classifier. Eureqa is a proprietary A.I.-powered modeling engine that automates much of the heavy lifting inherent in analytics and data science.
"LightGBM Dropout Additive Regression Trees Classifier
    
    LightGBM is a gradient boosting framework. It uses a tree-based algorithm and is designed to be
    distributed and efficient, providing the following advantages:
    
    #. Faster training speed and higher efficiency
    #. Lower memory usage
    #. Better accuracy
    #. Support for parallel learning
    #. Handling of large-scale data
    
    DART (Dropout Additive Regression Trees, proposed by Rasmi et al.) is a novel way of employing
    dropouts in Gradient Boosted Trees. It results in the Dropout Additive Regression Trees algorithm.
    By employing dropout techniques commonly used by deep neural nets, Rasmi et al. showed improve
    Gradient Boosted Trees results (in some situations).
    
    **Gradient Boosting Machines:**
    
    Gradient Boosting Machines (or Generalized Boosted Models, depending on who you
    ask to explain the acronym 'GBM') are an advanced algorithm for fitting
    extremely accurate predictive models. GBMs have won a number of recent predictive
    modeling competitions and are considered by many data scientists to be the
    most versatile and useful predictive modeling algorithm. GBMs require very
    little preprocessing, elegantly handle missing data, strike a good balance between
    bias and variance, and are typically able to find complicated interaction terms, making them a
    useful ""Swiss army knife"" of predictive models.
    
    GBMs are a generalization of Freund and Schapire's adaboost algorithm (1995) that handles
    arbitrary loss functions. They are very similar in concept to random forests, in that
    they fit individual decision trees to random re-samples of input data, where each
    tree sees a bootstrap sample of the rows of the dataset and N arbitrarily chosen
    columns, where N is a configurable parameter of the model. GBMs differ from random
    forests in a single major aspect: rather than fitting the trees independently, the
    GBM fits each successive tree to the residual errors from all the previous trees
    combined. This is advantageous, as the model focuses each iteration on the examples
    that are most difficult to predict (and therefore most useful to get correct).
    
    Due to their iterative nature, GBMs are almost guaranteed to overfit the training data,
    given enough iterations. Therefore, the 2 critical parameters of the algorithm are the
    learning rate (or how fast the model fits the data) and the number of trees the model
    is allowed to fit. It is critical to tune one of these 2 parameters, and
    when done correctly, GBMs are capable of finding the exact point in the training data
    where overfitting begins, and halt one iteration prior to that point. In this manner GBMs
    are usually capable of squeezing every last bit of information out of the training
    set and producing a model with the highest possible accuracy without overfitting.
    
    Parameters
    ----------
    learning_rate (lr): floatgrid (default='0.1')
        Shrink the contribution of each tree by `learning_rate`.
        There is a trade-off between learning_rate (lr) and n_estimators(n).
        In dart, it also affects normalization
        ``values: [1e-7, 1e2]``
    n_estimators (n): intgrid (default='10')
        Number of boosting stages to perform.
        Gradient boosting is fairly robust to overfitting so a large number usually results in better
        performance.
        ``values: [1, 1e6]``
    num_leaves (nl): intgrid (default='31')
        Number of leaves in one tree.
        ``values: [2, 1e4]``
    max_depth (md): intgrid (default='none')
        Maximum depth of the individual regression estimators.
        The maximum depth limits the number of nodes in the tree. Tune this parameter
        for best performance; the best value depends on the interaction
        of the input variables. Deeper the tree the more variable interactions
        the model can capture. Tree still grow by leaf-wise.
        <0 means no limit
        ``values: ['none', [1, 1e4]]``
    max_bin (mb): intgrid (default='255')
        Max number of bin that feature values will bucket in.
        Small bin may reduce training accuracy but may increase general power (deal with overfit).
        LightGBM will auto compress memory according max_bin. For example, LightGBM will use
        uint8_t for feature value if max_bin=255.
        ``values: [3, 1e4]``
    subsample_for_bin (ssfb): int (default=50000)
        Number of samples for constructing bins.
        ``values: [1, 1e6]``
    min_split_gain (msg): floatgrid (default='0')
        Minimum loss reduction required to make a further partition on a leaf node of the tree.
        ``values: [0, 100]``
    min_child_weight (mcw): intgrid (default='5')
        Minimum sum of instance weight(hessian) needed in a child(leaf).
        ``values: [0, 1e2]``
    min_child_samples (mcs): int (default='10')
        Minimum number of data need in a child(leaf).
        ``values: [0, 1e3]``
    subsample (ss): floatgrid (default='1.0')
        Subsample ratio of the training instance.
        ``values: [0.01, 1]``
    subsample_freq (ssf): intgrid (default='1')
        Frequency of subsample 'none' means it is not enabled.
        ``values: ['none', [1, 1e3]]``
    colsample_bytree (cbt): floatgrid (default='1.0')
        Subsample ratio of columns when constructing each tree.
        By default, the value of ``colsample_bytree`` for LightGBM classes is 1.0. However, based on the
        training data, DataRobot may choose a different initial value for this parameter.
        ``values: [0, 1]``
    reg_alpha (ra): floatgrid (default='0')
        L1 regularization term on weights.
        ``values: [0, 1e6]``
    reg_lambda (rl): floatgrid (default='0')
        L2 regularization term on weights.
        ``values: [0, 1e6]``
    sigmoid (s): floatgrid (default='1.0')
        Parameter for sigmoid function. Used in binary classification and LambdaRank.
        ``values: [1e-06, 1e03]``
    is_unbalance (iu): select (default=False)
        Set to true if training data are unbalanced. Used in binary classification.
        ``values: [True, False]``
    drop_rate (dr): floatgrid (default='0.1')
        Dropout rate.
        ``values: [0, 1]``
    skip_drop (sd): floatgrid (default='0.5')
        Probability of skipping drop.
        ``values: [0, 1]``
    max_drop (md): intgrid (default='50')
        Max number of dropped trees on one iteration. <=0 means no limit.
        ``values: ['auto', [1, 1e3]]``
    uniform_drop (ud): select (default=False)
        True if want to use uniform drop.
        ``values: [True, False]``
    xgboost_dart_mode (xdm): select (default=False)
        True if want to use xgboost dart mode.
        ``values: [True, False]``
    drop_seed (ds): intgrid (default='4')
        Used to random seed to choose dropping models.
        ``values: [0, 1e2]``
    
    References
    ----------
    .. [1] Chen, T, and He, T.
       Higgs Boson Discovery with Boosted Trees."" Cowan et al.,
       editor, JMLR: Workshop and Conference Proceedings. No. 42. 2015.
       `[link]
       <http://proceedings.mlr.press/v42/chen14.pdf>`__
    .. [2] Freund, Yoav, and Robert E. Schapire.
       ""A decision-theoretic generalization of on-line learning and an application to boosting.""
       Journal of computer and system sciences
       55.1 (1997): 119-139.
       `[link]
       <https://doi.org/10.1006/jcss.1997.1504>`__
    .. [3] Friedman, Jerome H.
       ""Greedy function approximation: a gradient boosting machine.""
       Annals of statistics (2001): 1189-1232.
       `[link]
       <https://statweb.stanford.edu/~jhf/ftp/trebst.pdf>`__
    .. [4] Breiman, Leo. Arcing the edge.
       Technical Report 486, Statistics Department,
       University of California at Berkeley, 1997.
       `[link]
       <https://www.stat.berkeley.edu/~breiman/arcing-the-edge.pdf>`__
    .. [5] Rashmi Korlakai Vinayak, Ran Gilad-Bachrach.
       ""DART: Dropouts meet Multiple Additive Regression Trees.""
       `[link]
       <http://proceedings.mlr.press/v38/korlakaivinayak15.pdf>`__
    
    See Also
    --------
    Source:
        `LightGBM on GitHub
        <https://github.com/Microsoft/LightGBM>`_
    Source:
        `Gradient boosting wikipedia
        <https://en.wikipedia.org/wiki/Gradient_boosting>`_",no url,LightGBM Dropout Additive Regression Trees Classifier,PLGBMDC,Binary Classification,no documentation retrieved,NUM,Dropout Additive Regression Trees Classifier,Dropout Additive Regression Trees Classifier
Hot / Cold Spots Classifier. The classifier implements the Patient Rule Induction method to generate hot / cold spots in the data,no url,Hot / Cold Spots Classifier,XPRIMC,Binary Classification,no documentation retrieved,NUM,Hot Spots,Hot / Cold Spots Classifier. The classifier implements the Patient Rule Induction method to generate hot / cold spots in the data
RuleFit Classifier,no url,RuleFit Classifier,RULEFITC,Binary Classification,no documentation retrieved,NUM,RuleFit Classifier,RuleFit Classifier
Partial Least Squares Classification. Similar to PCA classification. This method projects the variables into a new space. Based on scikit-learn Lasso,http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html,Partial Least Squares Classification,PLSC,Binary Classification,"













PLSRegression — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.cross_decomposition
PLSRegression









PLSRegression#


class sklearn.cross_decomposition.PLSRegression(n_components=2, *, scale=True, max_iter=500, tol=1e-06, copy=True)[source]#
PLS regression.
PLSRegression is also known as PLS2 or PLS1, depending on the number of
targets.
For a comparison between other cross decomposition algorithms, see
Compare cross decomposition methods.
Read more in the User Guide.

Added in version 0.8.


Parameters:

n_componentsint, default=2Number of components to keep. Should be in [1, n_features].

scalebool, default=TrueWhether to scale X and Y.

max_iterint, default=500The maximum number of iterations of the power method when
algorithm='nipals'. Ignored otherwise.

tolfloat, default=1e-06The tolerance used as convergence criteria in the power method: the
algorithm stops whenever the squared norm of u_i - u_{i-1} is less
than tol, where u corresponds to the left singular vector.

copybool, default=TrueWhether to copy X and Y in fit before applying centering,
and potentially scaling. If False, these operations will be done
inplace, modifying both arrays.



Attributes:

x_weights_ndarray of shape (n_features, n_components)The left singular vectors of the cross-covariance matrices of each
iteration.

y_weights_ndarray of shape (n_targets, n_components)The right singular vectors of the cross-covariance matrices of each
iteration.

x_loadings_ndarray of shape (n_features, n_components)The loadings of X.

y_loadings_ndarray of shape (n_targets, n_components)The loadings of Y.

x_scores_ndarray of shape (n_samples, n_components)The transformed training samples.

y_scores_ndarray of shape (n_samples, n_components)The transformed training targets.

x_rotations_ndarray of shape (n_features, n_components)The projection matrix used to transform X.

y_rotations_ndarray of shape (n_targets, n_components)The projection matrix used to transform Y.

coef_ndarray of shape (n_target, n_features)The coefficients of the linear model such that Y is approximated as
Y = X @ coef_.T + intercept_.

intercept_ndarray of shape (n_targets,)The intercepts of the linear model such that Y is approximated as
Y = X @ coef_.T + intercept_.

Added in version 1.1.


n_iter_list of shape (n_components,)Number of iterations of the power method, for each
component.

n_features_in_intNumber of features seen during fit.

feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

PLSCanonicalPartial Least Squares transformer and regressor.



Examples
>>> from sklearn.cross_decomposition import PLSRegression
>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]
>>> y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
>>> pls2 = PLSRegression(n_components=2)
>>> pls2.fit(X, y)
PLSRegression()
>>> Y_pred = pls2.predict(X)


For a comparison between PLS Regression and PCA, see
Principal Component Regression vs Partial Least Squares Regression.


fit(X, y=None, Y=None)[source]#
Fit model to data.

Parameters:

Xarray-like of shape (n_samples, n_features)Training vectors, where n_samples is the number of samples and
n_features is the number of predictors.

yarray-like of shape (n_samples,) or (n_samples, n_targets)Target vectors, where n_samples is the number of samples and
n_targets is the number of response variables.

Yarray-like of shape (n_samples,) or (n_samples, n_targets)Target vectors, where n_samples is the number of samples and
n_targets is the number of response variables.

Deprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.




Returns:

selfobjectFitted model.







fit_transform(X, y=None)[source]#
Learn and apply the dimension reduction on the train data.

Parameters:

Xarray-like of shape (n_samples, n_features)Training vectors, where n_samples is the number of samples and
n_features is the number of predictors.

yarray-like of shape (n_samples, n_targets), default=NoneTarget vectors, where n_samples is the number of samples and
n_targets is the number of response variables.



Returns:

selfndarray of shape (n_samples, n_components)Return x_scores if Y is not given, (x_scores, y_scores) otherwise.







get_feature_names_out(input_features=None)[source]#
Get output feature names for transformation.
The feature names out will prefixed by the lowercased class name. For
example, if the transformer outputs 3 features, then the feature names
out are: [""class_name0"", ""class_name1"", ""class_name2""].

Parameters:

input_featuresarray-like of str or None, default=NoneOnly used to validate feature names with the names seen in fit.



Returns:

feature_names_outndarray of str objectsTransformed feature names.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







inverse_transform(X, y=None, Y=None)[source]#
Transform data back to its original space.

Parameters:

Xarray-like of shape (n_samples, n_components)New data, where n_samples is the number of samples
and n_components is the number of pls components.

yarray-like of shape (n_samples,) or (n_samples, n_components)New target, where n_samples is the number of samples
and n_components is the number of pls components.

Yarray-like of shape (n_samples, n_components)New target, where n_samples is the number of samples
and n_components is the number of pls components.

Deprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.




Returns:

X_reconstructedndarray of shape (n_samples, n_features)Return the reconstructed X data.

y_reconstructedndarray of shape (n_samples, n_targets)Return the reconstructed X target. Only returned when y is given.




Notes
This transformation will only be exact if n_components=n_features.



predict(X, copy=True)[source]#
Predict targets of given samples.

Parameters:

Xarray-like of shape (n_samples, n_features)Samples.

copybool, default=TrueWhether to copy X and Y, or perform in-place normalization.



Returns:

y_predndarray of shape (n_samples,) or (n_samples, n_targets)Returns predicted values.




Notes
This call requires the estimation of a matrix of shape
(n_features, n_targets), which may be an issue in high dimensional
space.



score(X, y, sample_weight=None)[source]#
Return the coefficient of determination of the prediction.
The coefficient of determination \(R^2\) is defined as
\((1 - \frac{u}{v})\), where \(u\) is the residual
sum of squares ((y_true - y_pred)** 2).sum() and \(v\)
is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of y, disregarding the input features, would get
a \(R^2\) score of 0.0.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples. For some estimators this may be a precomputed
kernel matrix or a list of generic objects instead with shape
(n_samples, n_samples_fitted), where n_samples_fitted
is the number of samples used in the fitting for the estimator.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True values for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloat\(R^2\) of self.predict(X) w.r.t. y.




Notes
The \(R^2\) score used when calling score on a regressor uses
multioutput='uniform_average' from version 0.23 to keep consistent
with default value of r2_score.
This influences the score method of all the multioutput
regressors (except for
MultiOutputRegressor).



set_output(*, transform=None)[source]#
Set output container.
See Introducing the set_output API
for an example on how to use the API.

Parameters:

transform{“default”, “pandas”, “polars”}, default=NoneConfigure output of transform and fit_transform.

""default"": Default output format of a transformer
""pandas"": DataFrame output
""polars"": Polars output
None: Transform configuration is unchanged


Added in version 1.4: ""polars"" option was added.




Returns:

selfestimator instanceEstimator instance.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_predict_request(*, copy: bool | None | str = '$UNCHANGED$') → PLSRegression[source]#
Request metadata passed to the predict method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to predict if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to predict.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

copystr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for copy parameter in predict.



Returns:

selfobjectThe updated object.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → PLSRegression[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







set_transform_request(*, copy: bool | None | str = '$UNCHANGED$') → PLSRegression[source]#
Request metadata passed to the transform method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to transform if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to transform.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

copystr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for copy parameter in transform.



Returns:

selfobjectThe updated object.







transform(X, y=None, Y=None, copy=True)[source]#
Apply the dimension reduction.

Parameters:

Xarray-like of shape (n_samples, n_features)Samples to transform.

yarray-like of shape (n_samples, n_targets), default=NoneTarget vectors.

Yarray-like of shape (n_samples, n_targets), default=NoneTarget vectors.

Deprecated since version 1.5: Y is deprecated in 1.5 and will be removed in 1.7. Use y instead.


copybool, default=TrueWhether to copy X and Y, or perform in-place normalization.



Returns:

x_scores, y_scoresarray-like or tuple of array-likeReturn x_scores if Y is not given, (x_scores, y_scores) otherwise.







Gallery examples#

Compare cross decomposition methods
Compare cross decomposition methods

Principal Component Regression vs Partial Least Squares Regression
Principal Component Regression vs Partial Least Squares Regression










previous
PLSCanonical




next
PLSSVD










 On this page
  


PLSRegression
fit
fit_transform
get_feature_names_out
get_metadata_routing
get_params
inverse_transform
predict
score
set_output
set_params
set_predict_request
set_score_request
set_transform_request
transform


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Partial Least-Squares Classification,Partial Least Squares Classification. Similar to PCA classification. This method projects the variables into a new space. Based on scikit-learn Lasso
"Keras-based Variable Length Multi Image Fine-Tune Classification.
    
    The module uses Convolutional Neural Networks (CNNs) to employ transfer learning (""fine-tuning"") for
    image variable types. It initializes the CNNs using pretrained weights and proceeds to train and
    fine-tune those weights based on the dataset.
    
    Transfer learning methodologies rely on different elements, the two most
    essential of which are task size and similarity to the pretrained image dataset. Becaise Deep CNN
    (DCNN) features are more generic in early layers and more dataset-specific in later layers, the
    module is capable of only fine-tuning the higher level layers of the network.
    This is driven by the understanding that the prior features of a DCNN contain more generic image
    features that are useful to many tasks, but later layers of the Deep CNN become progressively more
    specific to the exact elements of the classes relevant in the given task.
    
    Multiple-image data introduces a new set of challenges because the images can either be
    ordered or unordered, empty or not empty. CNNs, and Deep Learning in general, can proceed
    independent of the exact ordering on the data. Additionally, common practices of
    padding data before feeding it to neural networks has long been proven to work in various
    applications. DataRobot apply the same methodology when there is no image by adding in black images.
    When images are unordered, these act as ""no information"" data and are used as placeholders to
    randomize the image order while training. In a multiple-image input model, the base model networks
    share the same weights even in fine-tuning, to further ensure independence from ordering.
    
    Parameters
    ----------
    Pretrained model (model_name): select (default=squeezenet)
        Model architecture trained on a pretrained dataset.
    
        ``values: ['squeezenet', 'resnet50', 'xception', 'efficientnet-b0', 'efficientnet-b4']``
    Batch size (batch_size): int (default=32)
        Mini-batch size for training.
    
        ``values: [0, 32]``
    Number of epoch (epoch): int (default=1000)
        Number of epochs for training.
    
        ``values: [0, 1000]``
    Early Stop parameter (earlystop_patience): int (default=5)
        Early stopping window. Number of epochs that stops the model from training when the validation
        score consecutively fails to improve
    
        ``values: [0, 1000]``
    reduce_lr_on_plateau (reduce_lr_on_plateau): selectgrid (default=False)
        When True, reduces the learning rate when a metric has stopped improving.
    reduce_lr_patience (reduce_lr_patience): intgrid (default=3)
        Number of epochs to wait before reducing the learning rate.
    reduce_lr_factor (reduce_lr_factor): float (default=0.2)
        Factor by which the learning rate will be reduced. Specifically `new_lr = lr * factor`.
    
        ``values: [0, 1]``
    weights_initialization (weights_initialization): select (default=pretrained)
        Whether to use a randomly initialized model or pretrained weights as a starting point before
        fine-tuning the model base.
    
        ``values: ['random', 'pretrained']``
    optimizer (optimizer): select (default=rmsprop)
        Name of the optimizer.
    
        ``values: ['rmsprop', 'adadelta', 'adagrad', 'adam', 'momentum', 'sgd', 'adam_tf']``
    use_discriminative_learning_rate (use_discriminative_learning_rate): selectgrid (default=False)
        When True (default is False), uses a different learning rate for each trainable layer. By
        default, DataRobot uses cosine learning rate decay structure (decayed to 0.0 learning rate) to
        decay learning rate by layer. If not all layers are trainable, learning rate decay proceeds by
        layer number. If all the layers are trainable, learning rate decays by convolutional
        blocks/groups specific to the model architecture chosen.
    
        ``values: ['True', 'False']``
    learning rate (learning_rate_init): float (default='auto')
        Initial learning rate. When value is set to auto, we set the initial learning rate is based on
        the chosen optimizer.
    
        ``values: {'float': [0, 1], 'select': ['auto']}``
    Trainable scope (trainable_scope): multi (default='all')
        Number of layers to enable training the weights of the base CNN model in fine-tune modelers.
        Either:
    
        integers: enable the last convolutional layers of the chosen network to be trainable
    
        all: All learnable layers are trainable
    
        chain_thaw: First, fine-tunes any new layers (often only a Softmax layer)
        to the target task until convergence on a validation set. Then, fine-tunes each
        layer individually starting from the first layer in the network. Last, the entire model is
        trained with all layers. Each time the model converges as measured on the validation set, the
        weights are reloaded to the best setting, thereby preventing overfitting to any layer.
    
        ``values: {'int': [0, 100], 'select': ['all', 'chain_thaw']}``
    featurizer_pool (featurizer_pool): select (default=avg)
        Type of summarizer to use to squash the multi-dimensional CNN features
        applied on initial, intermediate, and top convolutional layers of the network.
    
        ``values: ['avg', 'gem', 'max']``
    image_aug_list_id (image_aug_list_id): select (default=None)
        ID of the augmentation list used to control the transformations applied to the images
    
        ``values: [None]``
    loss (loss): select (default=crossentropy)
        The type of loss used for tuning the model:
    
        focal_loss: penalizes hard-to-classify examples more heavily relative to easy-to-classify
        examples.
    
        crossentropy: is the basic log loss and is used by default.
    
        blend_loss: uses a weighted average between crossentropy and focal loss.
    
        ``values: ['crossentropy', 'focal_loss', 'blend_loss']``
    Multi Image Training Type (variable_num_of_images_train_mode): select (default='MULTI_INPUT_CNN')
        Determines model and training type for multi-image data, either:
    
        ``values: ['MULTI_INPUT_CNN', 'SINGLE_INPUT_CNN']``
    
        Multi: A multi input CNN model with shared base architecture (shared weights) which
        trains on all of the available image columns and handles variable length image rows
    
        Single: A single input CNN model which trains on all available training images
        and based on labels, averages the predictions at testing time. It handles variable length image
        rows.
    
    References
    ----------
    .. [1] Jeremy Howard and Sebastian Ruder.
       ""Universal language  model  fine-tuning  for  text  classification"".
       arXiv preprint arXiv:1801.06146.
       `[link]
       <https://arxiv.org/pdf/1801.06146v5.pdf>`__
    .. [2] Pan, S. J., and Yang, Q.
       ""A Survey on Transfer Learning""
       Knowledge and Data Engineering, IEEE Transactionson 22 (10): 1345-1359.
       `[link]
       <https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf>`__
    
    See Also
    --------
    Source:
        `Convolutional Neural Network wikipedia
        <https://en.wikipedia.org/wiki/Convolutional_neural_network>`_",no url,Keras-based Variable Length Multi Image Fine-Tune Classification.,MULTIIMGFTC,Binary Classification,no documentation retrieved,NUM,Fine-Tuned Multi-Image Classifier (All Layers),Multi Image Classification using pre-trained deep neural network models
RuleFit Classifier. A rulefit model is an advanced ensemble method that combines the best qualities of tree-based and linear models.,no url,RuleFit Classifier,XRULEFITC,Binary Classification,no documentation retrieved,NUM,XRuleFit Classifier,RuleFit Classifier. A rulefit model is an advanced ensemble method that combines the best qualities of tree-based and linear models.
Random Forests based on scikit-learn. These are built shallowly for TreeShap performance,no url,Random Forests based on scikit-learn,SHAPRFC,Binary Classification,no documentation retrieved,NUM,ExtraTrees Classifier (Gini),Random Forests based on scikit-learn. These are built shallowly for TreeShap performance
Support Vector Classifier (scikit-learn). Support vector machines are a class of maximum margin models. They seek to maximize the separation they find between classes and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations.,http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC,Support Vector Classifier (scikit-learn),SVMC2,Binary Classification,"













SVC — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.svm
SVC









SVC#


class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)[source]#
C-Support Vector Classification.
The implementation is based on libsvm. The fit time scales at least
quadratically with the number of samples and may be impractical
beyond tens of thousands of samples. For large datasets
consider using LinearSVC or
SGDClassifier instead, possibly after a
Nystroem transformer or
other Kernel Approximation.
The multiclass support is handled according to a one-vs-one scheme.
For details on the precise mathematical formulation of the provided
kernel functions and how gamma, coef0 and degree affect each
other, see the corresponding section in the narrative documentation:
Kernel functions.
To learn how to tune SVC’s hyperparameters, see the following example:
Nested versus non-nested cross-validation
Read more in the User Guide.

Parameters:

Cfloat, default=1.0Regularization parameter. The strength of the regularization is
inversely proportional to C. Must be strictly positive. The penalty
is a squared l2 penalty. For an intuitive visualization of the effects
of scaling the regularization parameter C, see
Scaling the regularization parameter for SVCs.

kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable,          default=’rbf’Specifies the kernel type to be used in the algorithm. If
none is given, ‘rbf’ will be used. If a callable is given it is used to
pre-compute the kernel matrix from data matrices; that matrix should be
an array of shape (n_samples, n_samples). For an intuitive
visualization of different kernel types see
Plot classification boundaries with different SVM Kernels.

degreeint, default=3Degree of the polynomial kernel function (‘poly’).
Must be non-negative. Ignored by all other kernels.

gamma{‘scale’, ‘auto’} or float, default=’scale’Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.

if gamma='scale' (default) is passed then it uses
1 / (n_features * X.var()) as value of gamma,
if ‘auto’, uses 1 / n_features
if float, must be non-negative.


Changed in version 0.22: The default value of gamma changed from ‘auto’ to ‘scale’.


coef0float, default=0.0Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.

shrinkingbool, default=TrueWhether to use the shrinking heuristic.
See the User Guide.

probabilitybool, default=FalseWhether to enable probability estimates. This must be enabled prior
to calling fit, will slow down that method as it internally uses
5-fold cross-validation, and predict_proba may be inconsistent with
predict. Read more in the User Guide.

tolfloat, default=1e-3Tolerance for stopping criterion.

cache_sizefloat, default=200Specify the size of the kernel cache (in MB).

class_weightdict or ‘balanced’, default=NoneSet the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as n_samples / (n_classes * np.bincount(y)).

verbosebool, default=FalseEnable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.

max_iterint, default=-1Hard limit on iterations within solver, or -1 for no limit.

decision_function_shape{‘ovo’, ‘ovr’}, default=’ovr’Whether to return a one-vs-rest (‘ovr’) decision function of shape
(n_samples, n_classes) as all other classifiers, or the original
one-vs-one (‘ovo’) decision function of libsvm which has shape
(n_samples, n_classes * (n_classes - 1) / 2). However, note that
internally, one-vs-one (‘ovo’) is always used as a multi-class strategy
to train models; an ovr matrix is only constructed from the ovo matrix.
The parameter is ignored for binary classification.

Changed in version 0.19: decision_function_shape is ‘ovr’ by default.


Added in version 0.17: decision_function_shape=’ovr’ is recommended.


Changed in version 0.17: Deprecated decision_function_shape=’ovo’ and None.


break_tiesbool, default=FalseIf true, decision_function_shape='ovr', and number of classes > 2,
predict will break ties according to the confidence values of
decision_function; otherwise the first class among the tied
classes is returned. Please note that breaking ties comes at a
relatively high computational cost compared to a simple predict.

Added in version 0.22.


random_stateint, RandomState instance or None, default=NoneControls the pseudo random number generation for shuffling the data for
probability estimates. Ignored when probability is False.
Pass an int for reproducible output across multiple function calls.
See Glossary.



Attributes:

class_weight_ndarray of shape (n_classes,)Multipliers of parameter C for each class.
Computed based on the class_weight parameter.

classes_ndarray of shape (n_classes,)The classes labels.

coef_ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)Weights assigned to the features when kernel=""linear"".

dual_coef_ndarray of shape (n_classes -1, n_SV)Dual coefficients of the support vector in the decision
function (see Mathematical formulation), multiplied by
their targets.
For multiclass, coefficient for all 1-vs-1 classifiers.
The layout of the coefficients in the multiclass case is somewhat
non-trivial. See the multi-class section of the User Guide for details.

fit_status_int0 if correctly fitted, 1 otherwise (will raise warning)

intercept_ndarray of shape (n_classes * (n_classes - 1) / 2,)Constants in decision function.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


n_iter_ndarray of shape (n_classes * (n_classes - 1) // 2,)Number of iterations run by the optimization routine to fit the model.
The shape of this attribute depends on the number of models optimized
which in turn depends on the number of classes.

Added in version 1.1.


support_ndarray of shape (n_SV)Indices of support vectors.

support_vectors_ndarray of shape (n_SV, n_features)Support vectors. An empty array if kernel is precomputed.

n_support_ndarray of shape (n_classes,), dtype=int32Number of support vectors for each class.

probA_ndarray of shape (n_classes * (n_classes - 1) / 2)Parameter learned in Platt scaling when probability=True.

probB_ndarray of shape (n_classes * (n_classes - 1) / 2)Parameter learned in Platt scaling when probability=True.

shape_fit_tuple of int of shape (n_dimensions_of_X,)Array dimensions of training vector X.





See also

SVRSupport Vector Machine for Regression implemented using libsvm.

LinearSVCScalable Linear Support Vector Machine for classification implemented using liblinear. Check the See Also section of LinearSVC for more comparison element.



References


[1]
LIBSVM: A Library for Support Vector Machines


[2]
Platt, John (1999). “Probabilistic Outputs for Support Vector
Machines and Comparisons to Regularized Likelihood Methods”


Examples
>>> import numpy as np
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> y = np.array([1, 1, 2, 2])
>>> from sklearn.svm import SVC
>>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
>>> clf.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('svc', SVC(gamma='auto'))])


>>> print(clf.predict([[-0.8, -1]]))
[1]




property coef_#
Weights assigned to the features when kernel=""linear"".

Returns:

ndarray of shape (n_features, n_classes)






decision_function(X)[source]#
Evaluate the decision function for the samples in X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Xndarray of shape (n_samples, n_classes * (n_classes-1) / 2)Returns the decision function of the sample for each class
in the model.
If decision_function_shape=’ovr’, the shape is (n_samples,
n_classes).




Notes
If decision_function_shape=’ovo’, the function values are proportional
to the distance of the samples X to the separating hyperplane. If the
exact distances are required, divide the function values by the norm of
the weight vector (coef_). See also this question for further details.
If decision_function_shape=’ovr’, the decision function is a monotonic
transformation of ovo decision function.



fit(X, y, sample_weight=None)[source]#
Fit the SVM model according to the given training data.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)Training vectors, where n_samples is the number of samples
and n_features is the number of features.
For kernel=”precomputed”, the expected shape of X is
(n_samples, n_samples).

yarray-like of shape (n_samples,)Target values (class labels in classification, real numbers in
regression).

sample_weightarray-like of shape (n_samples,), default=NonePer-sample weights. Rescale C per sample. Higher weights
force the classifier to put more emphasis on these points.



Returns:

selfobjectFitted estimator.




Notes
If X and y are not C-ordered and contiguous arrays of np.float64 and
X is not a scipy.sparse.csr_matrix, X and/or y may be copied.
If X is a dense array, then the other methods will not support sparse
matrices as input.



get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







property n_support_#
Number of support vectors for each class.



predict(X)[source]#
Perform classification on samples in X.
For an one-class model, +1 or -1 is returned.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)For kernel=”precomputed”, the expected shape of X is
(n_samples_test, n_samples_train).



Returns:

y_predndarray of shape (n_samples,)Class labels for samples in X.







predict_log_proba(X)[source]#
Compute log probabilities of possible outcomes for samples in X.
The model need to have probability information computed at training
time: fit with attribute probability set to True.

Parameters:

Xarray-like of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)For kernel=”precomputed”, the expected shape of X is
(n_samples_test, n_samples_train).



Returns:

Tndarray of shape (n_samples, n_classes)Returns the log-probabilities of the sample for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.




Notes
The probability model is created using cross validation, so
the results can be slightly different than those obtained by
predict. Also, it will produce meaningless results on very small
datasets.



predict_proba(X)[source]#
Compute probabilities of possible outcomes for samples in X.
The model needs to have probability information computed at training
time: fit with attribute probability set to True.

Parameters:

Xarray-like of shape (n_samples, n_features)For kernel=”precomputed”, the expected shape of X is
(n_samples_test, n_samples_train).



Returns:

Tndarray of shape (n_samples, n_classes)Returns the probability of the sample for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.




Notes
The probability model is created using cross validation, so
the results can be slightly different than those obtained by
predict. Also, it will produce meaningless results on very small
datasets.



property probA_#
Parameter learned in Platt scaling when probability=True.

Returns:

ndarray of shape  (n_classes * (n_classes - 1) / 2)






property probB_#
Parameter learned in Platt scaling when probability=True.

Returns:

ndarray of shape  (n_classes * (n_classes - 1) / 2)






score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → SVC[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → SVC[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







Gallery examples#

Release Highlights for scikit-learn 0.24
Release Highlights for scikit-learn 0.24

Release Highlights for scikit-learn 0.22
Release Highlights for scikit-learn 0.22

Classifier comparison
Classifier comparison

Plot classification probability
Plot classification probability

Recognizing hand-written digits
Recognizing hand-written digits

Plot the decision boundaries of a VotingClassifier
Plot the decision boundaries of a VotingClassifier

Faces recognition example using eigenfaces and SVMs
Faces recognition example using eigenfaces and SVMs

Scalable learning with polynomial kernel approximation
Scalable learning with polynomial kernel approximation

Displaying Pipelines
Displaying Pipelines

Explicit feature map approximation for RBF kernels
Explicit feature map approximation for RBF kernels

Multilabel classification
Multilabel classification

ROC Curve with Visualization API
ROC Curve with Visualization API

Comparison between grid search and successive halving
Comparison between grid search and successive halving

Confusion matrix
Confusion matrix

Custom refit strategy of a grid search with cross-validation
Custom refit strategy of a grid search with cross-validation

Nested versus non-nested cross-validation
Nested versus non-nested cross-validation

Plotting Learning Curves and Checking Models’ Scalability
Plotting Learning Curves and Checking Models' Scalability

Plotting Validation Curves
Plotting Validation Curves

Receiver Operating Characteristic (ROC) with cross validation
Receiver Operating Characteristic (ROC) with cross validation

Statistical comparison of models using grid search
Statistical comparison of models using grid search

Test with permutations the significance of a classification score
Test with permutations the significance of a classification score

Concatenating multiple feature extraction methods
Concatenating multiple feature extraction methods

Feature discretization
Feature discretization

Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset
Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset

Effect of varying threshold for self-training
Effect of varying threshold for self-training

Plot classification boundaries with different SVM Kernels
Plot classification boundaries with different SVM Kernels

Plot different SVM classifiers in the iris dataset
Plot different SVM classifiers in the iris dataset

RBF SVM parameters
RBF SVM parameters

SVM Margins Example
SVM Margins Example

SVM Tie Breaking Example
SVM Tie Breaking Example

SVM with custom kernel
SVM with custom kernel

SVM-Anova: SVM with univariate feature selection
SVM-Anova: SVM with univariate feature selection

SVM: Maximum margin separating hyperplane
SVM: Maximum margin separating hyperplane

SVM: Separating hyperplane for unbalanced classes
SVM: Separating hyperplane for unbalanced classes

SVM: Weighted samples
SVM: Weighted samples

SVM Exercise
SVM Exercise










previous
OneClassSVM




next
SVR










 On this page
  


SVC
coef_
decision_function
fit
get_metadata_routing
get_params
n_support_
predict
predict_log_proba
predict_proba
probA_
probB_
score
set_fit_request
set_params
set_score_request


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Support Vector Classifier (Radial Kernel),Support Vector Classifier (scikit-learn). Support vector machines are a class of maximum margin models. They seek to maximize the separation they find between classes and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations.
Keras Convolutional Neural Network Classifier for Text,no url,Keras Convolutional Neural Network Classifier for Text,KERAS_CNN_TEXT_C,Binary Classification,no documentation retrieved,NUM,Keras Text Convolutional Neural Network Binary Classifier,Keras Convolutional Neural Network Classifier for Text
"Gaussian Process Classification with RBF Kernel. The RBF kernel is a stationary kernel. It is also known as the ""squared exponential"" kernel. It is parameterized by a length-scale parameter length_scale > 0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel).",https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html,Gaussian Process Classification with RBF Kernel,GPCRBF,Binary Classification,"













RBF — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.gaussian_process
RBF









RBF#


class sklearn.gaussian_process.kernels.RBF(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0))[source]#
Radial basis function kernel (aka squared-exponential kernel).
The RBF kernel is a stationary kernel. It is also known as the
“squared exponential” kernel. It is parameterized by a length scale
parameter \(l>0\), which can either be a scalar (isotropic variant
of the kernel) or a vector with the same number of dimensions as the inputs
X (anisotropic variant of the kernel). The kernel is given by:

\[k(x_i, x_j) = \exp\left(- \frac{d(x_i, x_j)^2}{2l^2} \right)\]
where \(l\) is the length scale of the kernel and
\(d(\cdot,\cdot)\) is the Euclidean distance.
For advice on how to set the length scale parameter, see e.g. [1].
This kernel is infinitely differentiable, which implies that GPs with this
kernel as covariance function have mean square derivatives of all orders,
and are thus very smooth.
See [2], Chapter 4, Section 4.2, for further details of the RBF kernel.
Read more in the User Guide.

Added in version 0.18.


Parameters:

length_scalefloat or ndarray of shape (n_features,), default=1.0The length scale of the kernel. If a float, an isotropic kernel is
used. If an array, an anisotropic kernel is used where each dimension
of l defines the length-scale of the respective feature dimension.

length_scale_boundspair of floats >= 0 or “fixed”, default=(1e-5, 1e5)The lower and upper bound on ‘length_scale’.
If set to “fixed”, ‘length_scale’ cannot be changed during
hyperparameter tuning.




References


[1]
David Duvenaud (2014). “The Kernel Cookbook:
Advice on Covariance functions”.


[2]
Carl Edward Rasmussen, Christopher K. I. Williams (2006).
“Gaussian Processes for Machine Learning”. The MIT Press.


Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn.gaussian_process import GaussianProcessClassifier
>>> from sklearn.gaussian_process.kernels import RBF
>>> X, y = load_iris(return_X_y=True)
>>> kernel = 1.0 * RBF(1.0)
>>> gpc = GaussianProcessClassifier(kernel=kernel,
...         random_state=0).fit(X, y)
>>> gpc.score(X, y)
0.9866...
>>> gpc.predict_proba(X[:2,:])
array([[0.8354..., 0.03228..., 0.1322...],
       [0.7906..., 0.0652..., 0.1441...]])




__call__(X, Y=None, eval_gradient=False)[source]#
Return the kernel k(X, Y) and optionally its gradient.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)

Yndarray of shape (n_samples_Y, n_features), default=NoneRight argument of the returned kernel k(X, Y). If None, k(X, X)
if evaluated instead.

eval_gradientbool, default=FalseDetermines whether the gradient with respect to the log of
the kernel hyperparameter is computed.
Only supported when Y is None.



Returns:

Kndarray of shape (n_samples_X, n_samples_Y)Kernel k(X, Y)

K_gradientndarray of shape (n_samples_X, n_samples_X, n_dims),                 optionalThe gradient of the kernel k(X, X) with respect to the log of the
hyperparameter of the kernel. Only returned when eval_gradient
is True.







property bounds#
Returns the log-transformed bounds on the theta.

Returns:

boundsndarray of shape (n_dims, 2)The log-transformed bounds on the kernel’s hyperparameters theta







clone_with_theta(theta)[source]#
Returns a clone of self with given hyperparameters theta.

Parameters:

thetandarray of shape (n_dims,)The hyperparameters







diag(X)[source]#
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however,
it can be evaluated more efficiently since only the diagonal is
evaluated.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)



Returns:

K_diagndarray of shape (n_samples_X,)Diagonal of kernel k(X, X)







get_params(deep=True)[source]#
Get parameters of this kernel.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







property hyperparameters#
Returns a list of all hyperparameter specifications.



is_stationary()[source]#
Returns whether the kernel is stationary.



property n_dims#
Returns the number of non-fixed hyperparameters of the kernel.



property requires_vector_input#
Returns whether the kernel is defined on fixed-length feature
vectors or generic objects. Defaults to True for backward
compatibility.



set_params(**params)[source]#
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels.
The latter have parameters of the form <component>__<parameter>
so that it’s possible to update each component of a nested object.

Returns:

self






property theta#
Returns the (flattened, log-transformed) non-fixed hyperparameters.
Note that theta are typically the log-transformed values of the
kernel’s hyperparameters as this representation of the search space
is more amenable for hyperparameter search, as hyperparameters like
length-scales naturally live on a log-scale.

Returns:

thetandarray of shape (n_dims,)The non-fixed, log-transformed hyperparameters of the kernel







Gallery examples#

Classifier comparison
Classifier comparison

Plot classification probability
Plot classification probability

Ability of Gaussian process regression (GPR) to estimate data noise-level
Ability of Gaussian process regression (GPR) to estimate data noise-level

Comparison of kernel ridge and Gaussian process regression
Comparison of kernel ridge and Gaussian process regression

Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)
Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)

Gaussian Processes regression: basic introductory example
Gaussian Processes regression: basic introductory example

Gaussian process classification (GPC) on iris dataset
Gaussian process classification (GPC) on iris dataset

Illustration of Gaussian process classification (GPC) on the XOR dataset
Illustration of Gaussian process classification (GPC) on the XOR dataset

Illustration of prior and posterior Gaussian process for different kernels
Illustration of prior and posterior Gaussian process for different kernels

Probabilistic predictions with Gaussian process classification (GPC)
Probabilistic predictions with Gaussian process classification (GPC)










previous
Product




next
RationalQuadratic










 On this page
  


RBF
__call__
bounds
clone_with_theta
diag
get_params
hyperparameters
is_stationary
n_dims
requires_vector_input
set_params
theta


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gaussian Process Classifier with Radial Basis Function Kernel,"Gaussian Process Classification with RBF Kernel. The RBF kernel is a stationary kernel. It is also known as the ""squared exponential"" kernel. It is parameterized by a length-scale parameter length_scale > 0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel)."
Eureqa Generalized Additive Model. The Classifier is a surrogate model that approximates Gradient Boosting Machine predictions using Eureqa modeling engine. Eureqa is a proprietary A.I.-powered modeling engine that automates much of the heavy lifting inherent in analytics and data science.,no url,Eureqa Generalized Additive Model,EQ_ESXGBC,Binary Classification,no documentation retrieved,NUM,Eureqa Generalized Additive Model Classifier,Eureqa Generalized Additive Model. The Classifier is a surrogate model that approximates Gradient Boosting Machine predictions using Eureqa modeling engine. Eureqa is a proprietary A.I.-powered modeling engine that automates much of the heavy lifting inherent in analytics and data science.
"LightGBM Random Forest Classifier
    
    LightGBM is a gradient boosting framework. It uses a tree-based algorithm and is designed to be
    distributed and efficient, providing the following advantages:
    
    #. Faster training speed and higher efficiency
    #. Lower memory usage
    #. Better accuracy
    #. Support for parallel learning
    #. Handling of large-scale data
    
    LightGBM implements both the gradient boosted trees and random forest algorithms. Random forests are
    an ensemble method where hundreds (or thousands) of individual decision trees are fit to bootstrap
    re-samples of the original dataset, with each tree being allowed to use a random selection of N
    variables, where N is the major configurable parameter of this algorithm.
    
    **Gradient Boosting Machines:**
    
    Gradient Boosting Machines (or Generalized Boosted Models, depending on who you
    ask to explain the acronym 'GBM') are an advanced algorithm for fitting
    extremely accurate predictive models. GBMs have won a number of recent predictive
    modeling competitions and are considered by many data scientists to be the
    most versatile and useful predictive modeling algorithm. GBMs require very
    little preprocessing, elegantly handle missing data, strike a good balance between
    bias and variance, and are typically able to find complicated interaction terms, making them a
    useful ""Swiss army knife"" of predictive models.
    
    GBMs are a generalization of Freund and Schapire's adaboost algorithm (1995) that handles
    arbitrary loss functions. They are very similar in concept to random forests, in that
    they fit individual decision trees to random re-samples of input data, where each
    tree sees a bootstrap sample of the rows of the dataset and N arbitrarily chosen
    columns, where N is a configurable parameter of the model. GBMs differ from random
    forests in a single major aspect: rather than fitting the trees independently, the
    GBM fits each successive tree to the residual errors from all the previous trees
    combined. This is advantageous, as the model focuses each iteration on the examples
    that are most difficult to predict (and therefore most useful to get correct).
    
    Due to their iterative nature, GBMs are almost guaranteed to overfit the training data,
    given enough iterations. Therefore, the 2 critical parameters of the algorithm are the
    learning rate (or how fast the model fits the data) and the number of trees the model
    is allowed to fit. It is critical to tune one of these 2 parameters, and
    when done correctly, GBMs are capable of finding the exact point in the training data
    where overfitting begins, and halt one iteration prior to that point. In this manner GBMs
    are usually capable of squeezing every last bit of information out of the training
    set and producing a model with the highest possible accuracy without overfitting.
    
    Parameters
    ----------
    learning_rate (lr): floatgrid (default='0.1')
        Not used in random forest mode.
        ``values: [1e-7, 1e2]``
    n_estimators (n): intgrid (default='10')
        Number of boosting stages to perform.
        Gradient boosting is fairly robust to overfitting so a large number usually results in better
        performance.
        ``values: [1, 1e6]``
    num_leaves (nl): intgrid (default='31')
        Number of leaves in one tree.
        ``values: [2, 1e4]``
    max_depth (md): intgrid (default='none')
        Maximum depth of the individual regression estimators.
        The maximum depth limits the number of nodes in the tree. Tune this parameter
        for best performance; the best value depends on the interaction
        of the input variables. Deeper the tree the more variable interactions
        the model can capture. Tree still grow by leaf-wise.
        <0 means no limit
        ``values: ['none', [1, 1e4]]``
    max_bin (mb): intgrid (default='255')
        Max number of bin that feature values will bucket in.
        Small bin may reduce training accuracy but may increase general power (deal with overfit).
        LightGBM will auto compress memory according max_bin. For example, LightGBM will use
        uint8_t for feature value if max_bin=255.
        ``values: [3, 1e4]``
    subsample_for_bin (ssfb): int (default=50000)
        Number of samples for constructing bins.
        ``values: [1, 1e6]``
    min_split_gain (msg): floatgrid (default='0')
        Minimum loss reduction required to make a further partition on a leaf node of the tree.
        ``values: [0, 100]``
    min_child_weight (mcw): intgrid (default='5')
        Minimum sum of instance weight(hessian) needed in a child(leaf).
        ``values: [0, 1e2]``
    min_child_samples (mcs): int (default='10')
        Minimum number of data need in a child(leaf).
        ``values: [0, 1e3]``
    subsample (ss): floatgrid (default='1.0')
        Subsample ratio of the training instance.
        ``values: [0.01, 1]``
    subsample_freq (ssf): intgrid (default='1')
        Frequency of subsample 'none' means it is not enabled.
        ``values: ['none', [1, 1e3]]``
    colsample_bytree (cbt): floatgrid (default='1.0')
        Subsample ratio of columns when constructing each tree.
        By default, the value of ``colsample_bytree`` for LightGBM classes is 1.0. However, based on the
        training data, DataRobot may choose a different initial value for this parameter.
        ``values: [0, 1]``
    reg_alpha (ra): floatgrid (default='0')
        L1 regularization term on weights.
        ``values: [0, 1e6]``
    reg_lambda (rl): floatgrid (default='0')
        L2 regularization term on weights.
        ``values: [0, 1e6]``
    sigmoid (s): floatgrid (default='1.0')
        Parameter for sigmoid function. Used in binary classification and LambdaRank.
        ``values: [1e-06, 1e03]``
    is_unbalance (iu): select (default=False)
        Set to true if training data are unbalanced. Used in binary classification.
        ``values: [True, False]``
    
    References
    ----------
    .. [1] Breiman, Leo.
       ""Random forests.""
       Machine learning 45.1 (2001): 5-32.
       `[link]
       <http://machinelearning202.pbworks.com/w/file/fetch/60606349/breiman_randomforests.pdf>`__
    .. [2] Liaw, Andy, and Matthew Wiener.
       ""Classification and regression by randomForest.""
       R news 2.3 (2002): 18-22.
       `[link]
       <https://cogns.northwestern.edu/cbmg/LiawAndWiener2002.pdf>`__
    .. [3] Ho, Tin Kam.
       ""Random decision forests."" Document Analysis and Recognition, 1995.,
       Proceedings of the Third International Conference on. Vol. 1. IEEE, 1995.
       `[link]
       <https://doi.org/10.1109/ICDAR.1995.598994>`__
    .. [4] Geurts, Pierre, Damien Ernst, and Louis Wehenkel.
       ""Extremely randomized trees."" Machine learning 63.1 (2006): 3-42.
       `[link]
       <https://doi.org/10.1007/s10994-006-6226-1>`__
    
    See Also
    --------
    Source:
        `sklearn.ensemble.RandomForestClassifier
        <http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>`_
    
    Source:
        `sklearn ensemble methods: Random Forests
        <http://scikit-learn.org/stable/modules/ensemble.html>`_
    
    Source:
        `Random Forests wikipedia
        <https://en.wikipedia.org/wiki/Random_forest>`_
    
    Source:
        `sklearn.ensemble.ExtraTreesClassifier
        <http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html>`_",no url,LightGBM Random Forest Classifier,PLGBMRFC,Binary Classification,no documentation retrieved,NUM,LightGBM Random Forest Classifier,Light GBM Random Forest Classifier
Light GBM Classifier with Early Stopping with GBDT using Unsupervised Learning features.,https://github.com/Microsoft/LightGBM/blob/master/docs/README.md,Light GBM Classifier with Early Stopping with GBDT using Unsupervised Learning features,UESLGBMTC,Binary Classification,no documentation retrieved,NUM,Light Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features,Light GBM Classifier with Early Stopping with GBDT using Unsupervised Learning features.
Auto Tuned Elasticnet Classifier for Summarized categorical with Category cloud,no url,Auto Tuned Elasticnet Classifier for Summarized categorical with Category cloud,SCLENETC,Binary Classification,no documentation retrieved,NUM,Auto-Tuned Summarized Categorical Classifier,Auto Tuned Elasticnet Classifier for Summarized categorical with Category cloud
Gradient Boosting Classifier (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Classifier (xgboost),PXGBC2,Binary Classification,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Classifier,Gradient Boosting Classifier (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
"Gaussian Process Classification with Matern Kernel. The class of Matern kernels is a generalization of the RBF parameterized by a parameter nu that controls exponentiation of the kernel. The smaller the nu used, the less smooth the approximated function is. For nu=0.5, the Matern kernel is equivalent to the Absolute Exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).",https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html,Gaussian Process Classification with Matern Kernel,GPCM,Binary Classification,"













Matern — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.gaussian_process
Matern









Matern#


class sklearn.gaussian_process.kernels.Matern(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0), nu=1.5)[source]#
Matern kernel.
The class of Matern kernels is a generalization of the RBF.
It has an additional parameter \(\nu\) which controls the
smoothness of the resulting function. The smaller \(\nu\),
the less smooth the approximated function is.
As \(\nu\rightarrow\infty\), the kernel becomes equivalent to
the RBF kernel. When \(\nu = 1/2\), the Matérn kernel
becomes identical to the absolute exponential kernel.
Important intermediate values are
\(\nu=1.5\) (once differentiable functions)
and \(\nu=2.5\) (twice differentiable functions).
The kernel is given by:

\[k(x_i, x_j) =  \frac{1}{\Gamma(\nu)2^{\nu-1}}\Bigg(
\frac{\sqrt{2\nu}}{l} d(x_i , x_j )
\Bigg)^\nu K_\nu\Bigg(
\frac{\sqrt{2\nu}}{l} d(x_i , x_j )\Bigg)\]
where \(d(\cdot,\cdot)\) is the Euclidean distance,
\(K_{\nu}(\cdot)\) is a modified Bessel function and
\(\Gamma(\cdot)\) is the gamma function.
See [1], Chapter 4, Section 4.2, for details regarding the different
variants of the Matern kernel.
Read more in the User Guide.

Added in version 0.18.


Parameters:

length_scalefloat or ndarray of shape (n_features,), default=1.0The length scale of the kernel. If a float, an isotropic kernel is
used. If an array, an anisotropic kernel is used where each dimension
of l defines the length-scale of the respective feature dimension.

length_scale_boundspair of floats >= 0 or “fixed”, default=(1e-5, 1e5)The lower and upper bound on ‘length_scale’.
If set to “fixed”, ‘length_scale’ cannot be changed during
hyperparameter tuning.

nufloat, default=1.5The parameter nu controlling the smoothness of the learned function.
The smaller nu, the less smooth the approximated function is.
For nu=inf, the kernel becomes equivalent to the RBF kernel and for
nu=0.5 to the absolute exponential kernel. Important intermediate
values are nu=1.5 (once differentiable functions) and nu=2.5
(twice differentiable functions). Note that values of nu not in
[0.5, 1.5, 2.5, inf] incur a considerably higher computational cost
(appr. 10 times higher) since they require to evaluate the modified
Bessel function. Furthermore, in contrast to l, nu is kept fixed to
its initial value and not optimized.




References


[1]
Carl Edward Rasmussen, Christopher K. I. Williams (2006).
“Gaussian Processes for Machine Learning”. The MIT Press.


Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn.gaussian_process import GaussianProcessClassifier
>>> from sklearn.gaussian_process.kernels import Matern
>>> X, y = load_iris(return_X_y=True)
>>> kernel = 1.0 * Matern(length_scale=1.0, nu=1.5)
>>> gpc = GaussianProcessClassifier(kernel=kernel,
...         random_state=0).fit(X, y)
>>> gpc.score(X, y)
0.9866...
>>> gpc.predict_proba(X[:2,:])
array([[0.8513..., 0.0368..., 0.1117...],
        [0.8086..., 0.0693..., 0.1220...]])




__call__(X, Y=None, eval_gradient=False)[source]#
Return the kernel k(X, Y) and optionally its gradient.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)

Yndarray of shape (n_samples_Y, n_features), default=NoneRight argument of the returned kernel k(X, Y). If None, k(X, X)
if evaluated instead.

eval_gradientbool, default=FalseDetermines whether the gradient with respect to the log of
the kernel hyperparameter is computed.
Only supported when Y is None.



Returns:

Kndarray of shape (n_samples_X, n_samples_Y)Kernel k(X, Y)

K_gradientndarray of shape (n_samples_X, n_samples_X, n_dims),                 optionalThe gradient of the kernel k(X, X) with respect to the log of the
hyperparameter of the kernel. Only returned when eval_gradient
is True.







property bounds#
Returns the log-transformed bounds on the theta.

Returns:

boundsndarray of shape (n_dims, 2)The log-transformed bounds on the kernel’s hyperparameters theta







clone_with_theta(theta)[source]#
Returns a clone of self with given hyperparameters theta.

Parameters:

thetandarray of shape (n_dims,)The hyperparameters







diag(X)[source]#
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however,
it can be evaluated more efficiently since only the diagonal is
evaluated.

Parameters:

Xndarray of shape (n_samples_X, n_features)Left argument of the returned kernel k(X, Y)



Returns:

K_diagndarray of shape (n_samples_X,)Diagonal of kernel k(X, X)







get_params(deep=True)[source]#
Get parameters of this kernel.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







property hyperparameters#
Returns a list of all hyperparameter specifications.



is_stationary()[source]#
Returns whether the kernel is stationary.



property n_dims#
Returns the number of non-fixed hyperparameters of the kernel.



property requires_vector_input#
Returns whether the kernel is defined on fixed-length feature
vectors or generic objects. Defaults to True for backward
compatibility.



set_params(**params)[source]#
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels.
The latter have parameters of the form <component>__<parameter>
so that it’s possible to update each component of a nested object.

Returns:

self






property theta#
Returns the (flattened, log-transformed) non-fixed hyperparameters.
Note that theta are typically the log-transformed values of the
kernel’s hyperparameters as this representation of the search space
is more amenable for hyperparameter search, as hyperparameters like
length-scales naturally live on a log-scale.

Returns:

thetandarray of shape (n_dims,)The non-fixed, log-transformed hyperparameters of the kernel







Gallery examples#

Illustration of prior and posterior Gaussian process for different kernels
Illustration of prior and posterior Gaussian process for different kernels










previous
Kernel




next
PairwiseKernel










 On this page
  


Matern
__call__
bounds
clone_with_theta
diag
get_params
hyperparameters
is_stationary
n_dims
requires_vector_input
set_params
theta


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gaussian Process Classifier with Matern Kernel,"Gaussian Process Classification with Matern Kernel. The class of Matern kernels is a generalization of the RBF parameterized by a parameter nu that controls exponentiation of the kernel. The smaller the nu used, the less smooth the approximated function is. For nu=0.5, the Matern kernel is equivalent to the Absolute Exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions)."
Gradient Boosting Classifier (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Classifier (xgboost),XGBC2,Binary Classification,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Classifier,Gradient Boosting Classifier (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
"Keras-based Variable Length Multi Image Fine-Tune Regression.
    
    The module uses Convolutional Neural Networks (CNNs) to employ transfer learning (""fine-tuning"") for
    image variable types. It initializes the CNNs using pretrained weights and proceeds to train and
    fine-tune those weights based on the dataset.
    
    Transfer learning methodologies rely on different elements, the two most
    essential of which are task size and similarity to the pretrained image dataset. Becaise Deep CNN
    (DCNN) features are more generic in early layers and more dataset-specific in later layers, the
    module is capable of only fine-tuning the higher level layers of the network.
    This is driven by the understanding that the prior features of a DCNN contain more generic image
    features that are useful to many tasks, but later layers of the Deep CNN become progressively more
    specific to the exact elements of the classes relevant in the given task.
    
    Multiple-image data introduces a new set of challenges because the images can either be
    ordered or unordered, empty or not empty. CNNs, and Deep Learning in general, can proceed
    independent of the exact ordering on the data. Additionally, common practices of
    padding data before feeding it to neural networks has long been proven to work in various
    applications. DataRobot apply the same methodology when there is no image by adding in black images.
    When images are unordered, these act as ""no information"" data and are used as placeholders to
    randomize the image order while training. In a multiple-image input model, the base model networks
    share the same weights even in fine-tuning, to further ensure independence from ordering.
    
    Parameters
    ----------
    Pretrained model (model_name): select (default=squeezenet)
        Model architecture trained on a pretrained dataset.
    
        ``values: ['squeezenet', 'resnet50', 'xception', 'efficientnet-b0', 'efficientnet-b4']``
    Batch size (batch_size): int (default=32)
        Mini-batch size for training.
    
        ``values: [0, 32]``
    Number of epoch (epoch): int (default=1000)
        Number of epochs for training.
    
        ``values: [0, 1000]``
    Early Stop parameter (earlystop_patience): int (default=5)
        Early stopping window. Number of epochs that stops the model from training when the validation
        score consecutively fails to improve
    
        ``values: [0, 1000]``
    reduce_lr_on_plateau (reduce_lr_on_plateau): selectgrid (default=False)
        When True, reduces the learning rate when a metric has stopped improving.
    reduce_lr_patience (reduce_lr_patience): intgrid (default=3)
        Number of epochs to wait before reducing the learning rate.
    reduce_lr_factor (reduce_lr_factor): float (default=0.2)
        Factor by which the learning rate will be reduced. Specifically `new_lr = lr * factor`.
    
        ``values: [0, 1]``
    weights_initialization (weights_initialization): select (default=pretrained)
        Whether to use a randomly initialized model or pretrained weights as a starting point before
        fine-tuning the model base.
    
        ``values: ['random', 'pretrained']``
    optimizer (optimizer): select (default=rmsprop)
        Name of the optimizer.
    
        ``values: ['rmsprop', 'adadelta', 'adagrad', 'adam', 'momentum', 'sgd', 'adam_tf']``
    use_discriminative_learning_rate (use_discriminative_learning_rate): selectgrid (default=False)
        When True (default is False), uses a different learning rate for each trainable layer. By
        default, DataRobot uses cosine learning rate decay structure (decayed to 0.0 learning rate) to
        decay learning rate by layer. If not all layers are trainable, learning rate decay proceeds by
        layer number. If all the layers are trainable, learning rate decays by convolutional
        blocks/groups specific to the model architecture chosen.
    
        ``values: ['True', 'False']``
    learning rate (learning_rate_init): float (default='auto')
        Initial learning rate. When value is set to auto, we set the initial learning rate is based on
        the chosen optimizer.
    
        ``values: {'float': [0, 1], 'select': ['auto']}``
    Trainable scope (trainable_scope): multi (default='all')
        Number of layers to enable training the weights of the base CNN model in fine-tune modelers.
        Either:
    
        integers: enable the last convolutional layers of the chosen network to be trainable
    
        all: All learnable layers are trainable
    
        chain_thaw: First, fine-tunes any new layers (often only a Softmax layer)
        to the target task until convergence on a validation set. Then, fine-tunes each
        layer individually starting from the first layer in the network. Last, the entire model is
        trained with all layers. Each time the model converges as measured on the validation set, the
        weights are reloaded to the best setting, thereby preventing overfitting to any layer.
    
        ``values: {'int': [0, 100], 'select': ['all', 'chain_thaw']}``
    featurizer_pool (featurizer_pool): select (default=avg)
        Type of summarizer to use to squash the multi-dimensional CNN features
        applied on initial, intermediate, and top convolutional layers of the network.
    
        ``values: ['avg', 'gem', 'max']``
    image_aug_list_id (image_aug_list_id): select (default=None)
        ID of the augmentation list used to control the transformations applied to the images
    
        ``values: [None]``
    loss (loss): select (default=mean_squared_error)
        The type of loss used for tuning the model.
    
        ``values: ['gaussian', 'poisson', 'tweedie', 'mean_absolute_error', 'mean_squared_error',
        'root_mean_squared_error']``
    Multi Image Training Type (variable_num_of_images_train_mode): select (default='MULTI_INPUT_CNN')
        Determines model and training type for multi-image data, either:
    
        ``values: ['MULTI_INPUT_CNN', 'SINGLE_INPUT_CNN']``
    
        Multi: A multi input CNN model with shared base architecture (shared weights) which
        trains on all of the available image columns and handles variable length image rows
    
        Single: A single input CNN model which trains on all available training images
        and based on labels, averages the predictions at testing time. It handles variable length image
        rows.
    
    References
    ----------
    .. [1] Jeremy Howard and Sebastian Ruder.
       ""Universal language  model  fine-tuning  for  text  classification"".
       arXiv preprint arXiv:1801.06146.
       `[link]
       <https://arxiv.org/pdf/1801.06146v5.pdf>`__
    .. [2] Pan, S. J., and Yang, Q.
       ""A Survey on Transfer Learning""
       Knowledge and Data Engineering, IEEE Transactionson 22 (10): 1345-1359.
       `[link]
       <https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf>`__
    
    See Also
    --------
    Source:
        `Convolutional Neural Network wikipedia
        <https://en.wikipedia.org/wiki/Convolutional_neural_network>`_",no url,Keras-based Variable Length Multi Image Fine-Tune Regression.,MULTIIMGFTR,Binary Classification,no documentation retrieved,NUM,Fine-Tuned Multi-Image Regressor (All Layers),Multi Image Regression using pre-trained deep neural network models
"Cluster distances based on K-Means Clustering, followed by an Elasticnet Classifier model based on block coordinate descent.",http://contrib.scikit-learn.org/lightning/,"Cluster distances based on K-Means Clustering, followed by an Elasticnet Classifier model based on block coordinate descent",KMDLENETCD,Binary Classification,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Elastic-Net Classifier (L1 / Binomial Deviance) with K-Means Distance Features,"Cluster distances based on K-Means Clustering, followed by an Elasticnet Classifier model based on block coordinate descent."
"Elasticnet Classifier. ElasticNet is a linear model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. Based on lightning CDClassifier",http://contrib.scikit-learn.org/lightning/,Elasticnet Classifier,LENETCD,Binary Classification,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Elastic-Net Classifier (L1 / Binomial Deviance),"Elasticnet Classifier. ElasticNet is a linear model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. Based on lightning CDClassifier"
Gradient Boosting Classifier (scikit-learn) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier,Gradient Boosting Classifier (scikit-learn) with Early-Stopping,ESGBC,Binary Classification,"













GradientBoostingClassifier — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.ensemble
GradientBoos...









GradientBoostingClassifier#


class sklearn.ensemble.GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)[source]#
Gradient Boosting for classification.
This algorithm builds an additive model in a forward stage-wise fashion; it
allows for the optimization of arbitrary differentiable loss functions. In
each stage n_classes_ regression trees are fit on the negative gradient
of the loss function, e.g. binary or multiclass log loss. Binary
classification is a special case where only a single regression tree is
induced.
HistGradientBoostingClassifier is a much faster variant
of this algorithm for intermediate and large datasets (n_samples >= 10_000) and
supports monotonic constraints.
Read more in the User Guide.

Parameters:

loss{‘log_loss’, ‘exponential’}, default=’log_loss’The loss function to be optimized. ‘log_loss’ refers to binomial and
multinomial deviance, the same as used in logistic regression.
It is a good choice for classification with probabilistic outputs.
For loss ‘exponential’, gradient boosting recovers the AdaBoost algorithm.

learning_ratefloat, default=0.1Learning rate shrinks the contribution of each tree by learning_rate.
There is a trade-off between learning_rate and n_estimators.
Values must be in the range [0.0, inf).

n_estimatorsint, default=100The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.
Values must be in the range [1, inf).

subsamplefloat, default=1.0The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. subsample interacts with the parameter n_estimators.
Choosing subsample < 1.0 leads to a reduction of variance
and an increase in bias.
Values must be in the range (0.0, 1.0].

criterion{‘friedman_mse’, ‘squared_error’}, default=’friedman_mse’The function to measure the quality of a split. Supported criteria are
‘friedman_mse’ for the mean squared error with improvement score by
Friedman, ‘squared_error’ for mean squared error. The default value of
‘friedman_mse’ is generally the best as it can provide a better
approximation in some cases.

Added in version 0.18.


min_samples_splitint or float, default=2The minimum number of samples required to split an internal node:

If int, values must be in the range [2, inf).
If float, values must be in the range (0.0, 1.0] and min_samples_split
will be ceil(min_samples_split * n_samples).


Changed in version 0.18: Added float values for fractions.


min_samples_leafint or float, default=1The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least min_samples_leaf training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.

If int, values must be in the range [1, inf).
If float, values must be in the range (0.0, 1.0) and min_samples_leaf
will be ceil(min_samples_leaf * n_samples).


Changed in version 0.18: Added float values for fractions.


min_weight_fraction_leaffloat, default=0.0The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.
Values must be in the range [0.0, 0.5].

max_depthint or None, default=3Maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
If int, values must be in the range [1, inf).

min_impurity_decreasefloat, default=0.0A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.
Values must be in the range [0.0, inf).
The weighted impurity decrease equation is the following:
N_t / N * (impurity - N_t_R / N_t * right_impurity
                    - N_t_L / N_t * left_impurity)


where N is the total number of samples, N_t is the number of
samples at the current node, N_t_L is the number of samples in the
left child, and N_t_R is the number of samples in the right child.
N, N_t, N_t_R and N_t_L all refer to the weighted sum,
if sample_weight is passed.

Added in version 0.19.


initestimator or ‘zero’, default=NoneAn estimator object that is used to compute the initial predictions.
init has to provide fit and predict_proba. If
‘zero’, the initial raw predictions are set to zero. By default, a
DummyEstimator predicting the classes priors is used.

random_stateint, RandomState instance or None, default=NoneControls the random seed given to each Tree estimator at each
boosting iteration.
In addition, it controls the random permutation of the features at
each split (see Notes for more details).
It also controls the random splitting of the training data to obtain a
validation set if n_iter_no_change is not None.
Pass an int for reproducible output across multiple function calls.
See Glossary.

max_features{‘sqrt’, ‘log2’}, int or float, default=NoneThe number of features to consider when looking for the best split:

If int, values must be in the range [1, inf).
If float, values must be in the range (0.0, 1.0] and the features
considered at each split will be max(1, int(max_features * n_features_in_)).
If ‘sqrt’, then max_features=sqrt(n_features).
If ‘log2’, then max_features=log2(n_features).
If None, then max_features=n_features.

Choosing max_features < n_features leads to a reduction of variance
and an increase in bias.
Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than max_features features.

verboseint, default=0Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.
Values must be in the range [0, inf).

max_leaf_nodesint, default=NoneGrow trees with max_leaf_nodes in best-first fashion.
Best nodes are defined as relative reduction in impurity.
Values must be in the range [2, inf).
If None, then unlimited number of leaf nodes.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See the Glossary.

validation_fractionfloat, default=0.1The proportion of training data to set aside as validation set for
early stopping. Values must be in the range (0.0, 1.0).
Only used if n_iter_no_change is set to an integer.

Added in version 0.20.


n_iter_no_changeint, default=Nonen_iter_no_change is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside validation_fraction size of the training
data as validation and terminate training when validation score is not
improving in all of the previous n_iter_no_change numbers of
iterations. The split is stratified.
Values must be in the range [1, inf).
See
Early stopping in Gradient Boosting.

Added in version 0.20.


tolfloat, default=1e-4Tolerance for the early stopping. When the loss is not improving
by at least tol for n_iter_no_change iterations (if set to a
number), the training stops.
Values must be in the range [0.0, inf).

Added in version 0.20.


ccp_alphanon-negative float, default=0.0Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
ccp_alpha will be chosen. By default, no pruning is performed.
Values must be in the range [0.0, inf).
See Minimal Cost-Complexity Pruning for details.

Added in version 0.22.




Attributes:

n_estimators_intThe number of estimators as selected by early stopping (if
n_iter_no_change is specified). Otherwise it is set to
n_estimators.

Added in version 0.20.


n_trees_per_iteration_intThe number of trees that are built at each iteration. For binary classifiers,
this is always 1.

Added in version 1.4.0.


feature_importances_ndarray of shape (n_features,)The impurity-based feature importances.

oob_improvement_ndarray of shape (n_estimators,)The improvement in loss on the out-of-bag samples
relative to the previous iteration.
oob_improvement_[0] is the improvement in
loss of the first stage over the init estimator.
Only available if subsample < 1.0.

oob_scores_ndarray of shape (n_estimators,)The full history of the loss values on the out-of-bag
samples. Only available if subsample < 1.0.

Added in version 1.3.


oob_score_floatThe last value of the loss on the out-of-bag samples. It is
the same as oob_scores_[-1]. Only available if subsample < 1.0.

Added in version 1.3.


train_score_ndarray of shape (n_estimators,)The i-th score train_score_[i] is the loss of the
model at iteration i on the in-bag sample.
If subsample == 1 this is the loss on the training data.

init_estimatorThe estimator that provides the initial predictions. Set via the init
argument.

estimators_ndarray of DecisionTreeRegressor of             shape (n_estimators, n_trees_per_iteration_)The collection of fitted sub-estimators. n_trees_per_iteration_ is 1 for
binary classification, otherwise n_classes.

classes_ndarray of shape (n_classes,)The classes labels.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


n_classes_intThe number of classes.

max_features_intThe inferred value of max_features.





See also

HistGradientBoostingClassifierHistogram-based Gradient Boosting Classification Tree.

sklearn.tree.DecisionTreeClassifierA decision tree classifier.

RandomForestClassifierA meta-estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.

AdaBoostClassifierA meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.



Notes
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
max_features=n_features, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
random_state has to be fixed.
References
J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.

Friedman, Stochastic Gradient Boosting, 1999

T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.
Examples
The following example shows how to fit a gradient boosting classifier with
100 decision stumps as weak learners.
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier


>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]


>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.913...




apply(X)[source]#
Apply trees in the ensemble to X, return leaf indices.

Added in version 0.17.


Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, its dtype will be converted to
dtype=np.float32. If a sparse matrix is provided, it will
be converted to a sparse csr_matrix.



Returns:

X_leavesarray-like of shape (n_samples, n_estimators, n_classes)For each datapoint x in X and for each tree in the ensemble,
return the index of the leaf x ends up in each estimator.
In the case of binary classification n_classes is 1.







decision_function(X)[source]#
Compute the decision function of X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

scorendarray of shape (n_samples, n_classes) or (n_samples,)The decision function of the input samples, which corresponds to
the raw values predicted from the trees of the ensemble . The
order of the classes corresponds to that in the attribute
classes_. Regression and binary classification produce an
array of shape (n_samples,).







property feature_importances_#
The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.
Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
sklearn.inspection.permutation_importance as an alternative.

Returns:

feature_importances_ndarray of shape (n_features,)The values of this array sum to 1, unless all trees are single node
trees consisting of only the root node, in which case it will be an
array of zeros.







fit(X, y, sample_weight=None, monitor=None)[source]#
Fit the gradient boosting model.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.

yarray-like of shape (n_samples,)Target values (strings or integers in classification, real numbers
in regression)
For classification, labels must correspond to classes.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. In the case of
classification, splits are also ignored if they would result in any
single class carrying a negative weight in either child node.

monitorcallable, default=NoneThe monitor is called after each iteration with the current
iteration, a reference to the estimator and the local variables of
_fit_stages as keyword arguments callable(i, self,
locals()). If the callable returns True the fitting procedure
is stopped. The monitor can be used for various things such as
computing held-out estimates, early stopping, model introspect, and
snapshotting.



Returns:

selfobjectFitted estimator.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict class for X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

yndarray of shape (n_samples,)The predicted values.







predict_log_proba(X)[source]#
Predict class log-probabilities for X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

pndarray of shape (n_samples, n_classes)The class log-probabilities of the input samples. The order of the
classes corresponds to that in the attribute classes_.



Raises:

AttributeErrorIf the loss does not support probabilities.







predict_proba(X)[source]#
Predict class probabilities for X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

pndarray of shape (n_samples, n_classes)The class probabilities of the input samples. The order of the
classes corresponds to that in the attribute classes_.



Raises:

AttributeErrorIf the loss does not support probabilities.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, monitor: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → GradientBoostingClassifier[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

monitorstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for monitor parameter in fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → GradientBoostingClassifier[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







staged_decision_function(X)[source]#
Compute decision function of X for each iteration.
This method allows monitoring (i.e. determine error on testing set)
after each stage.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Yields:

scoregenerator of ndarray of shape (n_samples, k)The decision function of the input samples, which corresponds to
the raw values predicted from the trees of the ensemble . The
classes corresponds to that in the attribute classes_.
Regression and binary classification are special cases with
k == 1, otherwise k==n_classes.







staged_predict(X)[source]#
Predict class at each stage for X.
This method allows monitoring (i.e. determine error on testing set)
after each stage.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Yields:

ygenerator of ndarray of shape (n_samples,)The predicted value of the input samples.







staged_predict_proba(X)[source]#
Predict class probabilities at each stage for X.
This method allows monitoring (i.e. determine error on testing set)
after each stage.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Yields:

ygenerator of ndarray of shape (n_samples,)The predicted value of the input samples.







Gallery examples#

Feature transformations with ensembles of trees
Feature transformations with ensembles of trees

Gradient Boosting Out-of-Bag estimates
Gradient Boosting Out-of-Bag estimates

Gradient Boosting regularization
Gradient Boosting regularization

Feature discretization
Feature discretization










previous
ExtraTreesRegressor




next
GradientBoostingRegressor










 On this page
  


GradientBoostingClassifier
apply
decision_function
feature_importances_
fit
get_metadata_routing
get_params
predict
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_score_request
staged_decision_function
staged_predict
staged_predict_proba


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gradient Boosted Trees Classifier with Early Stopping,Gradient Boosting Classifier (scikit-learn) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
ElasticNet classifier that re-weights the features using Naive Bayes log-count ratios,no url,ElasticNet classifier that re-weights the features using Naive Bayes log-count ratios,NB_LENETCD,Binary Classification,no documentation retrieved,NUM,Elastic-Net Classifier with Naive Bayes Feature Weighting,ElasticNet classifier that re-weights the features using Naive Bayes log-count ratios
"Classifier implementing the k-nearest neighbors vote. Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Based on scikit-learn KNeighborsClassifier",http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html,Classifier implementing the k-nearest neighbors vote,KNNC,Binary Classification,"













KNeighborsClassifier — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.neighbors
KNeighborsClassifier









KNeighborsClassifier#


class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)[source]#
Classifier implementing the k-nearest neighbors vote.
Read more in the User Guide.

Parameters:

n_neighborsint, default=5Number of neighbors to use by default for kneighbors queries.

weights{‘uniform’, ‘distance’}, callable or None, default=’uniform’Weight function used in prediction.  Possible values:

‘uniform’ : uniform weights.  All points in each neighborhood
are weighted equally.
‘distance’ : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.
[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.

Refer to the example entitled
Nearest Neighbors Classification
showing the impact of the weights parameter on the decision
boundary.

algorithm{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’Algorithm used to compute the nearest neighbors:

‘ball_tree’ will use BallTree
‘kd_tree’ will use KDTree
‘brute’ will use a brute-force search.
‘auto’ will attempt to decide the most appropriate algorithm
based on the values passed to fit method.

Note: fitting on sparse input will override the setting of
this parameter, using brute force.

leaf_sizeint, default=30Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.

pfloat, default=2Power parameter for the Minkowski metric. When p = 1, this is equivalent
to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.
For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected
to be positive.

metricstr or callable, default=’minkowski’Metric to use for distance computation. Default is “minkowski”, which
results in the standard Euclidean distance when p = 2. See the
documentation of scipy.spatial.distance and
the metrics listed in
distance_metrics for valid metric
values.
If metric is “precomputed”, X is assumed to be a distance matrix and
must be square during fit. X may be a sparse graph, in which
case only “nonzero” elements may be considered neighbors.
If metric is a callable function, it takes two arrays representing 1D
vectors as inputs and must return one value indicating the distance
between those vectors. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.

metric_paramsdict, default=NoneAdditional keyword arguments for the metric function.

n_jobsint, default=NoneThe number of parallel jobs to run for neighbors search.
None means 1 unless in a joblib.parallel_backend context.
-1 means using all processors. See Glossary
for more details.
Doesn’t affect fit method.



Attributes:

classes_array of shape (n_classes,)Class labels known to the classifier

effective_metric_str or callbleThe distance metric used. It will be same as the metric parameter
or a synonym of it, e.g. ‘euclidean’ if the metric parameter set to
‘minkowski’ and p parameter set to 2.

effective_metric_params_dictAdditional keyword arguments for the metric function. For most metrics
will be same with metric_params parameter, but may also contain the
p parameter value if the effective_metric_ attribute is set to
‘minkowski’.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


n_samples_fit_intNumber of samples in the fitted data.

outputs_2d_boolFalse when y’s shape is (n_samples, ) or (n_samples, 1) during fit
otherwise True.





See also

RadiusNeighborsClassifierClassifier based on neighbors within a fixed radius.

KNeighborsRegressorRegression based on k-nearest neighbors.

RadiusNeighborsRegressorRegression based on neighbors within a fixed radius.

NearestNeighborsUnsupervised learner for implementing neighbor searches.



Notes
See Nearest Neighbors in the online documentation
for a discussion of the choice of algorithm and leaf_size.

Warning
Regarding the Nearest Neighbors algorithms, if it is found that two
neighbors, neighbor k+1 and k, have identical distances
but different labels, the results will depend on the ordering of the
training data.

https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm
Examples
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsClassifier
>>> neigh = KNeighborsClassifier(n_neighbors=3)
>>> neigh.fit(X, y)
KNeighborsClassifier(...)
>>> print(neigh.predict([[1.1]]))
[0]
>>> print(neigh.predict_proba([[0.9]]))
[[0.666... 0.333...]]




fit(X, y)[source]#
Fit the k-nearest neighbors classifier from the training dataset.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric=’precomputed’Training data.

y{array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)Target values.



Returns:

selfKNeighborsClassifierThe fitted k-nearest neighbors classifier.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







kneighbors(X=None, n_neighbors=None, return_distance=True)[source]#
Find the K-neighbors of a point.
Returns indices of and distances to the neighbors of each point.

Parameters:

X{array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == ‘precomputed’, default=NoneThe query point or points.
If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.

n_neighborsint, default=NoneNumber of neighbors required for each sample. The default is the
value passed to the constructor.

return_distancebool, default=TrueWhether or not to return the distances.



Returns:

neigh_distndarray of shape (n_queries, n_neighbors)Array representing the lengths to points, only present if
return_distance=True.

neigh_indndarray of shape (n_queries, n_neighbors)Indices of the nearest points in the population matrix.




Examples
In the following example, we construct a NearestNeighbors
class from an array representing our data set and ask who’s
the closest point to [1,1,1]
>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=1)
>>> neigh.fit(samples)
NearestNeighbors(n_neighbors=1)
>>> print(neigh.kneighbors([[1., 1., 1.]]))
(array([[0.5]]), array([[2]]))


As you can see, it returns [[0.5]], and [[2]], which means that the
element is at distance 0.5 and is the third element of samples
(indexes start at 0). You can also query for multiple points:
>>> X = [[0., 1., 0.], [1., 0., 1.]]
>>> neigh.kneighbors(X, return_distance=False)
array([[1],
       [2]]...)





kneighbors_graph(X=None, n_neighbors=None, mode='connectivity')[source]#
Compute the (weighted) graph of k-Neighbors for points in X.

Parameters:

X{array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == ‘precomputed’, default=NoneThe query point or points.
If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
For metric='precomputed' the shape should be
(n_queries, n_indexed). Otherwise the shape should be
(n_queries, n_features).

n_neighborsint, default=NoneNumber of neighbors for each sample. The default is the value
passed to the constructor.

mode{‘connectivity’, ‘distance’}, default=’connectivity’Type of returned matrix: ‘connectivity’ will return the
connectivity matrix with ones and zeros, in ‘distance’ the
edges are distances between points, type of distance
depends on the selected metric parameter in
NearestNeighbors class.



Returns:

Asparse-matrix of shape (n_queries, n_samples_fit)n_samples_fit is the number of samples in the fitted data.
A[i, j] gives the weight of the edge connecting i to j.
The matrix is of CSR format.





See also

NearestNeighbors.radius_neighbors_graphCompute the (weighted) graph of Neighbors for points in X.



Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=2)
>>> neigh.fit(X)
NearestNeighbors(n_neighbors=2)
>>> A = neigh.kneighbors_graph(X)
>>> A.toarray()
array([[1., 0., 1.],
       [0., 1., 1.],
       [1., 0., 1.]])





predict(X)[source]#
Predict the class labels for the provided data.

Parameters:

X{array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == ‘precomputed’Test samples.



Returns:

yndarray of shape (n_queries,) or (n_queries, n_outputs)Class labels for each data sample.







predict_proba(X)[source]#
Return probability estimates for the test data X.

Parameters:

X{array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == ‘precomputed’Test samples.



Returns:

pndarray of shape (n_queries, n_classes), or a list of n_outputs                 of such arrays if n_outputs > 1.The class probabilities of the input samples. Classes are ordered
by lexicographic order.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → KNeighborsClassifier[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







Gallery examples#

Release Highlights for scikit-learn 0.24
Release Highlights for scikit-learn 0.24

Classifier comparison
Classifier comparison

Plot the decision boundaries of a VotingClassifier
Plot the decision boundaries of a VotingClassifier

Caching nearest neighbors
Caching nearest neighbors

Comparing Nearest Neighbors with and without Neighborhood Components Analysis
Comparing Nearest Neighbors with and without Neighborhood Components Analysis

Dimensionality Reduction with Neighborhood Components Analysis
Dimensionality Reduction with Neighborhood Components Analysis

Nearest Neighbors Classification
Nearest Neighbors Classification

Importance of Feature Scaling
Importance of Feature Scaling

Digits Classification Exercise
Digits Classification Exercise

Classification of text documents using sparse features
Classification of text documents using sparse features










previous
KDTree




next
KNeighborsRegressor










 On this page
  


KNeighborsClassifier
fit
get_metadata_routing
get_params
kneighbors
kneighbors_graph
predict
predict_proba
score
set_params
set_score_request


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Auto-tuned K-Nearest Neighbors Classifier (Euclidean Distance),"Classifier implementing the k-nearest neighbors vote. Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Based on scikit-learn KNeighborsClassifier"
"Approximate Kernel Support Vector Classifier using ElasticNet. Support vector machines are a class of “maximum margin” classifiers. They seek to maximize the separation they find between classes, and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations.",http://scikit-learn.org/stable/modules/kernel_approximation.html,Approximate Kernel Support Vector Classifier using ElasticNet,ASVMEC,Binary Classification,"













6.7. Kernel Approximation — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

1. Supervised learning
1.1. Linear Models
1.2. Linear and Quadratic Discriminant Analysis
1.3. Kernel ridge regression
1.4. Support Vector Machines
1.5. Stochastic Gradient Descent
1.6. Nearest Neighbors
1.7. Gaussian Processes
1.8. Cross decomposition
1.9. Naive Bayes
1.10. Decision Trees
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking
1.12. Multiclass and multioutput algorithms
1.13. Feature selection
1.14. Semi-supervised learning
1.15. Isotonic regression
1.16. Probability calibration
1.17. Neural network models (supervised)


2. Unsupervised learning
2.1. Gaussian mixture models
2.2. Manifold learning
2.3. Clustering
2.4. Biclustering
2.5. Decomposing signals in components (matrix factorization problems)
2.6. Covariance estimation
2.7. Novelty and Outlier Detection
2.8. Density Estimation
2.9. Neural network models (unsupervised)


3. Model selection and evaluation
3.1. Cross-validation: evaluating estimator performance
3.2. Tuning the hyper-parameters of an estimator
3.3. Tuning the decision threshold for class prediction
3.4. Metrics and scoring: quantifying the quality of predictions
3.5. Validation curves: plotting scores to evaluate models


4. Inspection
4.1. Partial Dependence and Individual Conditional Expectation plots
4.2. Permutation feature importance


5. Visualizations
6. Dataset transformations
6.1. Pipelines and composite estimators
6.2. Feature extraction
6.3. Preprocessing data
6.4. Imputation of missing values
6.5. Unsupervised dimensionality reduction
6.6. Random Projection
6.7. Kernel Approximation
6.8. Pairwise metrics, Affinities and Kernels
6.9. Transforming the prediction target (y)


7. Dataset loading utilities
7.1. Toy datasets
7.2. Real world datasets
7.3. Generated datasets
7.4. Loading other datasets


8. Computing with scikit-learn
8.1. Strategies to scale computationally: bigger data
8.2. Computational Performance
8.3. Parallelism, resource management, and configuration


9. Model persistence
10. Common pitfalls and recommended practices
11. Dispatching
11.1. Array API support (experimental)


12. Choosing the right estimator
13. External Resources, Videos and Talks






















User Guide
6. Dataset transformations










6.7. Kernel Approximation#
This submodule contains functions that approximate the feature mappings that
correspond to certain kernels, as they are used for example in support vector
machines (see Support Vector Machines).
The following feature functions perform non-linear transformations of the
input, which can serve as a basis for linear classification or other
algorithms.
The advantage of using approximate explicit feature maps compared to the
kernel trick,
which makes use of feature maps implicitly, is that explicit mappings
can be better suited for online learning and can significantly reduce the cost
of learning with very large datasets.
Standard kernelized SVMs do not scale well to large datasets, but using an
approximate kernel map it is possible to use much more efficient linear SVMs.
In particular, the combination of kernel map approximations with
SGDClassifier can make non-linear learning on large datasets possible.
Since there has not been much empirical work using approximate embeddings, it
is advisable to compare results against exact kernel methods when possible.

See also
Polynomial regression: extending linear models with basis functions for an exact polynomial transformation.


6.7.1. Nystroem Method for Kernel Approximation#
The Nystroem method, as implemented in Nystroem is a general method for
reduced rank approximations of kernels. It achieves this by subsampling without
replacement rows/columns of the data on which the kernel is evaluated. While the
computational complexity of the exact method is
\(\mathcal{O}(n^3_{\text{samples}})\), the complexity of the approximation
is \(\mathcal{O}(n^2_{\text{components}} \cdot n_{\text{samples}})\), where
one can set \(n_{\text{components}} \ll n_{\text{samples}}\) without a
significative decrease in performance [WS2001].
We can construct the eigendecomposition of the kernel matrix \(K\), based
on the features of the data, and then split it into sampled and unsampled data
points.

\[\begin{split}K = U \Lambda U^T
= \begin{bmatrix} U_1 \\ U_2\end{bmatrix} \Lambda \begin{bmatrix} U_1 \\ U_2 \end{bmatrix}^T
= \begin{bmatrix} U_1 \Lambda U_1^T & U_1 \Lambda U_2^T \\ U_2 \Lambda U_1^T & U_2 \Lambda U_2^T \end{bmatrix}
\equiv \begin{bmatrix} K_{11} & K_{12} \\ K_{21} & K_{22} \end{bmatrix}\end{split}\]
where:

\(U\) is orthonormal
\(\Lambda\) is diagonal matrix of eigenvalues
\(U_1\) is orthonormal matrix of samples that were chosen
\(U_2\) is orthonormal matrix of samples that were not chosen

Given that \(U_1 \Lambda U_1^T\) can be obtained by orthonormalization of
the matrix \(K_{11}\), and \(U_2 \Lambda U_1^T\) can be evaluated (as
well as its transpose), the only remaining term to elucidate is
\(U_2 \Lambda U_2^T\). To do this we can express it in terms of the already
evaluated matrices:

\[\begin{split}\begin{align} U_2 \Lambda U_2^T &= \left(K_{21} U_1 \Lambda^{-1}\right) \Lambda \left(K_{21} U_1 \Lambda^{-1}\right)^T
\\&= K_{21} U_1 (\Lambda^{-1} \Lambda) \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} U_1 \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} K_{11}^{-1} K_{21}^T
\\&= \left( K_{21} K_{11}^{-\frac12} \right) \left( K_{21} K_{11}^{-\frac12} \right)^T
.\end{align}\end{split}\]
During fit, the class Nystroem evaluates the basis \(U_1\), and
computes the normalization constant, \(K_{11}^{-\frac12}\). Later, during
transform, the kernel matrix is determined between the basis (given by the
components_ attribute) and the new data points, X. This matrix is then
multiplied by the normalization_ matrix for the final result.
By default Nystroem uses the rbf kernel, but it can use any kernel
function or a precomputed kernel matrix. The number of samples used - which is
also the dimensionality of the features computed - is given by the parameter
n_components.
Examples

See the example entitled
Time-related feature engineering,
that shows an efficient machine learning pipeline that uses a
Nystroem kernel.



6.7.2. Radial Basis Function Kernel#
The RBFSampler constructs an approximate mapping for the radial basis
function kernel, also known as Random Kitchen Sinks [RR2007]. This
transformation can be used to explicitly model a kernel map, prior to applying
a linear algorithm, for example a linear SVM:
>>> from sklearn.kernel_approximation import RBFSampler
>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
>>> y = [0, 0, 1, 1]
>>> rbf_feature = RBFSampler(gamma=1, random_state=1)
>>> X_features = rbf_feature.fit_transform(X)
>>> clf = SGDClassifier(max_iter=5)
>>> clf.fit(X_features, y)
SGDClassifier(max_iter=5)
>>> clf.score(X_features, y)
1.0


The mapping relies on a Monte Carlo approximation to the
kernel values. The fit function performs the Monte Carlo sampling, whereas
the transform method performs the mapping of the data.  Because of the
inherent randomness of the process, results may vary between different calls to
the fit function.
The fit function takes two arguments:
n_components, which is the target dimensionality of the feature transform,
and gamma, the parameter of the RBF-kernel.  A higher n_components will
result in a better approximation of the kernel and will yield results more
similar to those produced by a kernel SVM. Note that “fitting” the feature
function does not actually depend on the data given to the fit function.
Only the dimensionality of the data is used.
Details on the method can be found in [RR2007].
For a given value of n_components RBFSampler is often less accurate
as Nystroem. RBFSampler is cheaper to compute, though, making
use of larger feature spaces more efficient.




Comparing an exact RBF kernel (left) with the approximation (right)#


Examples

Explicit feature map approximation for RBF kernels



6.7.3. Additive Chi Squared Kernel#
The additive chi squared kernel is a kernel on histograms, often used in computer vision.
The additive chi squared kernel as used here is given by

\[k(x, y) = \sum_i \frac{2x_iy_i}{x_i+y_i}\]
This is not exactly the same as sklearn.metrics.pairwise.additive_chi2_kernel.
The authors of [VZ2010] prefer the version above as it is always positive
definite.
Since the kernel is additive, it is possible to treat all components
\(x_i\) separately for embedding. This makes it possible to sample
the Fourier transform in regular intervals, instead of approximating
using Monte Carlo sampling.
The class AdditiveChi2Sampler implements this component wise
deterministic sampling. Each component is sampled \(n\) times, yielding
\(2n+1\) dimensions per input dimension (the multiple of two stems
from the real and complex part of the Fourier transform).
In the literature, \(n\) is usually chosen to be 1 or 2, transforming
the dataset to size n_samples * 5 * n_features (in the case of \(n=2\)).
The approximate feature map provided by AdditiveChi2Sampler can be combined
with the approximate feature map provided by RBFSampler to yield an approximate
feature map for the exponentiated chi squared kernel.
See the [VZ2010] for details and [VVZ2010] for combination with the RBFSampler.


6.7.4. Skewed Chi Squared Kernel#
The skewed chi squared kernel is given by:

\[k(x,y) = \prod_i \frac{2\sqrt{x_i+c}\sqrt{y_i+c}}{x_i + y_i + 2c}\]
It has properties that are similar to the exponentiated chi squared kernel
often used in computer vision, but allows for a simple Monte Carlo
approximation of the feature map.
The usage of the SkewedChi2Sampler is the same as the usage described
above for the RBFSampler. The only difference is in the free
parameter, that is called \(c\).
For a motivation for this mapping and the mathematical details see [LS2010].


6.7.5. Polynomial Kernel Approximation via Tensor Sketch#
The polynomial kernel is a popular type of kernel
function given by:

\[k(x, y) = (\gamma x^\top y +c_0)^d\]
where:

x, y are the input vectors
d is the kernel degree

Intuitively, the feature space of the polynomial kernel of degree d
consists of all possible degree-d products among input features, which enables
learning algorithms using this kernel to account for interactions between features.
The TensorSketch [PP2013] method, as implemented in PolynomialCountSketch, is a
scalable, input data independent method for polynomial kernel approximation.
It is based on the concept of Count sketch [WIKICS] [CCF2002] , a dimensionality
reduction technique similar to feature hashing, which instead uses several
independent hash functions. TensorSketch obtains a Count Sketch of the outer product
of two vectors (or a vector with itself), which can be used as an approximation of the
polynomial kernel feature space. In particular, instead of explicitly computing
the outer product, TensorSketch computes the Count Sketch of the vectors and then
uses polynomial multiplication via the Fast Fourier Transform to compute the
Count Sketch of their outer product.
Conveniently, the training phase of TensorSketch simply consists of initializing
some random variables. It is thus independent of the input data, i.e. it only
depends on the number of input features, but not the data values.
In addition, this method can transform samples in
\(\mathcal{O}(n_{\text{samples}}(n_{\text{features}} + n_{\text{components}} \log(n_{\text{components}})))\)
time, where \(n_{\text{components}}\) is the desired output dimension,
determined by n_components.
Examples

Scalable learning with polynomial kernel approximation



6.7.6. Mathematical Details#
Kernel methods like support vector machines or kernelized
PCA rely on a property of reproducing kernel Hilbert spaces.
For any positive definite kernel function \(k\) (a so called Mercer kernel),
it is guaranteed that there exists a mapping \(\phi\)
into a Hilbert space \(\mathcal{H}\), such that

\[k(x,y) = \langle \phi(x), \phi(y) \rangle\]
Where \(\langle \cdot, \cdot \rangle\) denotes the inner product in the
Hilbert space.
If an algorithm, such as a linear support vector machine or PCA,
relies only on the scalar product of data points \(x_i\), one may use
the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm
to the mapped data points \(\phi(x_i)\).
The advantage of using \(k\) is that the mapping \(\phi\) never has
to be calculated explicitly, allowing for arbitrary large
features (even infinite).
One drawback of kernel methods is, that it might be necessary
to store many kernel values \(k(x_i, x_j)\) during optimization.
If a kernelized classifier is applied to new data \(y_j\),
\(k(x_i, y_j)\) needs to be computed to make predictions,
possibly for many different \(x_i\) in the training set.
The classes in this submodule allow to approximate the embedding
\(\phi\), thereby working explicitly with the representations
\(\phi(x_i)\), which obviates the need to apply the kernel
or store training examples.
References


[WS2001]
“Using the Nyström method to speed up kernel machines”
Williams, C.K.I.; Seeger, M. - 2001.


[RR2007]
(1,2)
“Random features for large-scale kernel machines”
Rahimi, A. and Recht, B. - Advances in neural information processing 2007,


[LS2010]
“Random Fourier approximations for skewed multiplicative histogram kernels”
Li, F., Ionescu, C., and Sminchisescu, C.
- Pattern Recognition,  DAGM 2010, Lecture Notes in Computer Science.


[VZ2010]
(1,2)
“Efficient additive kernels via explicit feature maps”
Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010


[VVZ2010]
“Generalized RBF feature maps for Efficient Detection”
Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010


[PP2013]
“Fast and scalable polynomial kernels via explicit feature maps”
Pham, N., & Pagh, R. - 2013


[CCF2002]
“Finding frequent items in data streams”
Charikar, M., Chen, K., & Farach-Colton - 2002


[WIKICS]
“Wikipedia: Count sketch”












previous
6.6. Random Projection




next
6.8. Pairwise metrics, Affinities and Kernels










 On this page
  


6.7.1. Nystroem Method for Kernel Approximation
6.7.2. Radial Basis Function Kernel
6.7.3. Additive Chi Squared Kernel
6.7.4. Skewed Chi Squared Kernel
6.7.5. Polynomial Kernel Approximation via Tensor Sketch
6.7.6. Mathematical Details





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Nystroem Kernel SVM Classifier using Elastic-Net,"Approximate Kernel Support Vector Classifier using ElasticNet. Support vector machines are a class of “maximum margin” classifiers. They seek to maximize the separation they find between classes, and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations."
"Keras-based Image Fine-Tune Classification.
    
    The module uses Convolutional Neural Networks (CNNs) to employ transfer learning (""fine-tuning"") for
    image variable types. It initializes the CNNs using pretrained weights and proceeds to train and
    fine-tune those weights based on the dataset.
    
    Transfer learning methodologies rely on different elements, the two most
    essential of which are task size and similarity to the pretrained image dataset. Becaise Deep CNN
    (DCNN) features are more generic in early layers and more dataset-specific in later layers, the
    module is capable of only fine-tuning the higher level layers of the network.
    This is driven by the understanding that the prior features of a DCNN contain more generic image
    features that are useful to many tasks, but later layers of the Deep CNN become progressively more
    specific to the exact elements of the classes relevant in the given task.
    
    Parameters
    ----------
    Pretrained model (model_name): select (default=squeezenet)
        Model architecture trained on a pretrained dataset.
    
        ``values: ['squeezenet', 'resnet50', 'xception', 'efficientnet-b0', 'efficientnet-b4']``
    Batch size (batch_size): int (default=32)
        Mini-batch size for training.
    
        ``values: [0, 32]``
    Number of epoch (epoch): int (default=1000)
        Number of epochs for training.
    
        ``values: [0, 1000]``
    Early Stop parameter (earlystop_patience): int (default=5)
        Early stopping window. Number of epochs that stops the model from training when the validation
        score consecutively fails to improve
    
        ``values: [0, 1000]``
    reduce_lr_on_plateau (reduce_lr_on_plateau): selectgrid (default=False)
        When True, reduces the learning rate when a metric has stopped improving.
    reduce_lr_patience (reduce_lr_patience): intgrid (default=3)
        Number of epochs to wait before reducing the learning rate.
    reduce_lr_factor (reduce_lr_factor): float (default=0.2)
        Factor by which the learning rate will be reduced. Specifically `new_lr = lr * factor`.
    
        ``values: [0, 1]``
    weights_initialization (weights_initialization): select (default=pretrained)
        Whether to use a randomly initialized model or pretrained weights as a starting point before
        fine-tuning the model base.
    
        ``values: ['random', 'pretrained']``
    optimizer (optimizer): select (default=rmsprop)
        Name of the optimizer.
    
        ``values: ['rmsprop', 'adadelta', 'adagrad', 'adam', 'momentum', 'sgd', 'adam_tf']``
    use_discriminative_learning_rate (use_discriminative_learning_rate): selectgrid (default=False)
        When True (default is False), uses a different learning rate for each trainable layer. By
        default, DataRobot uses cosine learning rate decay structure (decayed to 0.0 learning rate) to
        decay learning rate by layer. If not all layers are trainable, learning rate decay proceeds by
        layer number. If all the layers are trainable, learning rate decays by convolutional
        blocks/groups specific to the model architecture chosen.
    
        ``values: ['True', 'False']``
    learning rate (learning_rate_init): float (default='auto')
        Initial learning rate. When value is set to auto, we set the initial learning rate is based on
        the chosen optimizer.
    
        ``values: {'float': [0, 1], 'select': ['auto']}``
    Trainable scope (trainable_scope): multi (default='all')
        Number of layers to enable training the weights of the base CNN model in fine-tune modelers.
        Either:
    
        integers: enable the last convolutional layers of the chosen network to be trainable
    
        all: All learnable layers are trainable
    
        chain_thaw: First, fine-tunes any new layers (often only a Softmax layer)
        to the target task until convergence on a validation set. Then, fine-tunes each
        layer individually starting from the first layer in the network. Last, the entire model is
        trained with all layers. Each time the model converges as measured on the validation set, the
        weights are reloaded to the best setting, thereby preventing overfitting to any layer.
    
        ``values: {'int': [0, 100], 'select': ['all', 'chain_thaw']}``
    featurizer_pool (featurizer_pool): select (default=avg)
        Type of summarizer to use to squash the multi-dimensional CNN features
        applied on initial, intermediate, and top convolutional layers of the network.
    
        ``values: ['avg', 'gem', 'max']``
    image_aug_list_id (image_aug_list_id): select (default=None)
        ID of the augmentation list used to control the transformations applied to the images
    
        ``values: [None]``
    loss (loss): select (default=crossentropy)
        The type of loss used for tuning the model:
    
        focal_loss: penalizes hard-to-classify examples more heavily relative to easy-to-classify
        examples.
    
        crossentropy: is the basic log loss and is used by default.
    
        blend_loss: uses a weighted average between crossentropy and focal loss.
    
        ``values: ['crossentropy', 'focal_loss', 'blend_loss']``
    
    References
    ----------
    .. [1] Jeremy Howard and Sebastian Ruder.
       ""Universal language  model  fine-tuning  for  text  classification"".
       arXiv preprint arXiv:1801.06146.
       `[link]
       <https://arxiv.org/pdf/1801.06146v5.pdf>`__
    .. [2] Pan, S. J., and Yang, Q.
       ""A Survey on Transfer Learning""
       Knowledge and Data Engineering, IEEE Transactionson 22 (10): 1345-1359.
       `[link]
       <https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf>`__
    
    See Also
    --------
    Source:
        `Convolutional Neural Network wikipedia
        <https://en.wikipedia.org/wiki/Convolutional_neural_network>`_",no url,Keras-based Image Fine-Tune Classification.,IMGFTC,Binary Classification,no documentation retrieved,NUM,Fine-Tuned Image Classifier (All Layers),Image Classfication using pre-trained deep neural network models.
"Elasticnet Classifier. ElasticNet is a linear model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. Based on lightning CDClassifier",http://contrib.scikit-learn.org/lightning/,Elasticnet Classifier,LENETCDWC,Binary Classification,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Elastic-Net Classifier (L1 / Binomial Deviance),"Elasticnet Classifier. ElasticNet is a linear model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. Based on lightning CDClassifier"
Tunes character n-grams and generates out-of-sample predictions.,no url,Tunes character n-grams and generates out-of-sample predictions,CNGEC2,Binary Classification,no documentation retrieved,NUM,Auto-Tuned Char N-Gram Text Modeler using token counts,Tunes character n-grams and generates out-of-sample predictions.
"LightGBM Trees Classifier with Grid Search and Early Stopping support
    
    LightGBM is a gradient boosting framework. It uses a tree-based algorithm and is designed to be
    distributed and efficient, providing the following advantages:
    
    #. Faster training speed and higher efficiency
    #. Lower memory usage
    #. Better accuracy
    #. Support for parallel learning
    #. Handling of large-scale data
    
    **Gradient Boosting Machines:**
    
    Gradient Boosting Machines (or Generalized Boosted Models, depending on who you
    ask to explain the acronym 'GBM') are an advanced algorithm for fitting
    extremely accurate predictive models. GBMs have won a number of recent predictive
    modeling competitions and are considered by many data scientists to be the
    most versatile and useful predictive modeling algorithm. GBMs require very
    little preprocessing, elegantly handle missing data, strike a good balance between
    bias and variance, and are typically able to find complicated interaction terms, making them a
    useful ""Swiss army knife"" of predictive models.
    
    GBMs are a generalization of Freund and Schapire's adaboost algorithm (1995) that handles
    arbitrary loss functions. They are very similar in concept to random forests, in that
    they fit individual decision trees to random re-samples of input data, where each
    tree sees a bootstrap sample of the rows of the dataset and N arbitrarily chosen
    columns, where N is a configurable parameter of the model. GBMs differ from random
    forests in a single major aspect: rather than fitting the trees independently, the
    GBM fits each successive tree to the residual errors from all the previous trees
    combined. This is advantageous, as the model focuses each iteration on the examples
    that are most difficult to predict (and therefore most useful to get correct).
    
    Due to their iterative nature, GBMs are almost guaranteed to overfit the training data,
    given enough iterations. Therefore, the 2 critical parameters of the algorithm are the
    learning rate (or how fast the model fits the data) and the number of trees the model
    is allowed to fit. It is critical to tune one of these 2 parameters, and
    when done correctly, GBMs are capable of finding the exact point in the training data
    where overfitting begins, and halt one iteration prior to that point. In this manner GBMs
    are usually capable of squeezing every last bit of information out of the training
    set and producing a model with the highest possible accuracy without overfitting.
    
    **Early Stopping Support:**
    
    Early stopping is a method for determining the number of trees to use
    for a boosted trees model. The training data is split into a training set and a test set, and at
    each iteration the model is scored on the test set. If test set performance decreases for 200
    iterations (tunable in Advanced Tuning), the training procedure stops and the model returns
    the fit from the best tree seen so far. The approach saves time by not continuing past the point
    where it is clear that the model is overfitting and further trees will not result in more accuracy.
    
    
    Note that the early stopping test set uses a 90/10 train/test split *within* the training data
    for a given model. For example, a 64% model on the Leaderboard will internally use 57.6% of the
    data for training, and 6.4% of the data for early stopping. A 100% model on the Leaderboard will
    internally use 90% of the data for training and 10% of the data for early stopping.
    
    Since the early stopping test set was used for early stopping, it cannot be used for training.
    
    This limitation also applies to grid search: within the grid search train/test split, the model will
    use a 90/10 train/test split for early stopping.
    
    **Grid Search Support:**
    
    Grid search is supported in this task. During training, grid search is run to estimate the optimal
    model parameter values that yield the best performance (evaluated by the configured loss function
    ). The grid search runs on a 70/30 train/test split within the training data; the estimated score
    uses 30% of the training data split. After the grid search completes and the best
    tuning parameters are found, the final model is retrained on 100% of training data. Validation
    scores of the final model are different from the validation scores of the grid search.
    
    Grid search is run on the task parameter with one of the following types:
    'intgrid', 'floatgrid', 'listgrid(int)', 'listgrid(float)', 'selectgrid', or 'multi'. Refer to the
    **Parameters** section for details of task parameter definitions.
    
    For each grid search parameter, the search space is defined by the parameter values. Refer to the
    **Parameters** section for details of task parameter definitions.
    
    Parameters
    ----------
    learning_rate (lr): floatgrid (default='0.1')
        Shrink the contribution of each tree by `learning_rate`.
        There is a trade-off between learning_rate (lr) and n_estimators(n).
        In dart, it also affects normalization
        ``values: [1e-7, 1e2]``
    n_estimators (n): intgrid (default='10')
        Number of boosting stages to perform.
        Gradient boosting is fairly robust to overfitting so a large number usually results in better
        performance.
        ``values: [1, 1e6]``
    num_leaves (nl): intgrid (default='31')
        Number of leaves in one tree.
        ``values: [2, 1e4]``
    max_depth (md): intgrid (default='none')
        Maximum depth of the individual regression estimators.
        The maximum depth limits the number of nodes in the tree. Tune this parameter
        for best performance; the best value depends on the interaction
        of the input variables. Deeper the tree the more variable interactions
        the model can capture. Tree still grow by leaf-wise.
        <0 means no limit
        ``values: ['none', [1, 1e4]]``
    max_bin (mb): intgrid (default='255')
        Max number of bin that feature values will bucket in.
        Small bin may reduce training accuracy but may increase general power (deal with overfit).
        LightGBM will auto compress memory according max_bin. For example, LightGBM will use
        uint8_t for feature value if max_bin=255.
        ``values: [3, 1e4]``
    subsample_for_bin (ssfb): int (default=50000)
        Number of samples for constructing bins.
        ``values: [1, 1e6]``
    min_split_gain (msg): floatgrid (default='0')
        Minimum loss reduction required to make a further partition on a leaf node of the tree.
        ``values: [0, 100]``
    min_child_weight (mcw): intgrid (default='5')
        Minimum sum of instance weight(hessian) needed in a child(leaf).
        ``values: [0, 1e2]``
    min_child_samples (mcs): int (default='10')
        Minimum number of data need in a child(leaf).
        ``values: [0, 1e3]``
    subsample (ss): floatgrid (default='1.0')
        Subsample ratio of the training instance.
        ``values: [0.01, 1]``
    subsample_freq (ssf): intgrid (default='1')
        Frequency of subsample 'none' means it is not enabled.
        ``values: ['none', [1, 1e3]]``
    colsample_bytree (cbt): floatgrid (default='1.0')
        Subsample ratio of columns when constructing each tree.
        By default, the value of ``colsample_bytree`` for LightGBM classes is 1.0. However, based on the
        training data, DataRobot may choose a different initial value for this parameter.
        ``values: [0, 1]``
    reg_alpha (ra): floatgrid (default='0')
        L1 regularization term on weights.
        ``values: [0, 1e6]``
    reg_lambda (rl): floatgrid (default='0')
        L2 regularization term on weights.
        ``values: [0, 1e6]``
    sigmoid (s): floatgrid (default='1.0')
        Parameter for sigmoid function. Used in binary classification and LambdaRank.
        ``values: [1e-06, 1e03]``
    is_unbalance (iu): select (default=False)
        Set to true if training data are unbalanced. Used in binary classification.
        ``values: [True, False]``
    early_stopping_rounds (esr): int (default='10')
        Will stop training if one metric of one validation data doesn't improve in last
        early_stopping_round rounds.
        ``values: [0, 1e3]``
    
    References
    ----------
    .. [1] Chen, T, and He, T.
       Higgs Boson Discovery with Boosted Trees."" Cowan et al.,
       editor, JMLR: Workshop and Conference Proceedings. No. 42. 2015.
       `[link]
       <http://proceedings.mlr.press/v42/chen14.pdf>`__
    .. [2] Freund, Yoav, and Robert E. Schapire.
       ""A decision-theoretic generalization of on-line learning and an application to boosting.""
       Journal of computer and system sciences
       55.1 (1997): 119-139.
       `[link]
       <https://doi.org/10.1006/jcss.1997.1504>`__
    .. [3] Friedman, Jerome H.
       ""Greedy function approximation: a gradient boosting machine.""
       Annals of statistics (2001): 1189-1232.
       `[link]
       <https://statweb.stanford.edu/~jhf/ftp/trebst.pdf>`__
    .. [4] Breiman, Leo. Arcing the edge.
       Technical Report 486, Statistics Department,
       University of California at Berkeley, 1997.
       `[link]
       <https://www.stat.berkeley.edu/~breiman/arcing-the-edge.pdf>`__
    
    See Also
    --------
    Source:
        `LightGBM on GitHub
        <https://github.com/Microsoft/LightGBM>`_
    Source:
        `Gradient boosting wikipedia
        <https://en.wikipedia.org/wiki/Gradient_boosting>`_",no url,LightGBM Trees Classifier with Grid Search and Early Stopping support,ESLGBMTC,Binary Classification,no documentation retrieved,NUM,Light Gradient Boosted Trees Classifier with Early Stopping,Light GBM Classifier with Early Stopping with GBDT
Naive Bayes Classifier for numeric inputs (scikit-learn). Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes theorem with the “naive” assumption of independence between every pair of features.,http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB,Naive Bayes Classifier for numeric inputs (scikit-learn),GNBC,Binary Classification,"













GaussianNB — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.naive_bayes
GaussianNB









GaussianNB#


class sklearn.naive_bayes.GaussianNB(*, priors=None, var_smoothing=1e-09)[source]#
Gaussian Naive Bayes (GaussianNB).
Can perform online updates to model parameters via partial_fit.
For details on algorithm used to update feature means and variance online,
see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:

http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf

Read more in the User Guide.

Parameters:

priorsarray-like of shape (n_classes,), default=NonePrior probabilities of the classes. If specified, the priors are not
adjusted according to the data.

var_smoothingfloat, default=1e-9Portion of the largest variance of all features that is added to
variances for calculation stability.

Added in version 0.20.




Attributes:

class_count_ndarray of shape (n_classes,)number of training samples observed in each class.

class_prior_ndarray of shape (n_classes,)probability of each class.

classes_ndarray of shape (n_classes,)class labels known to the classifier.

epsilon_floatabsolute additive value to variances.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


var_ndarray of shape (n_classes, n_features)Variance of each feature per class.

Added in version 1.0.


theta_ndarray of shape (n_classes, n_features)mean of each feature per class.





See also

BernoulliNBNaive Bayes classifier for multivariate Bernoulli models.

CategoricalNBNaive Bayes classifier for categorical features.

ComplementNBComplement Naive Bayes classifier.

MultinomialNBNaive Bayes classifier for multinomial models.



Examples
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> Y = np.array([1, 1, 1, 2, 2, 2])
>>> from sklearn.naive_bayes import GaussianNB
>>> clf = GaussianNB()
>>> clf.fit(X, Y)
GaussianNB()
>>> print(clf.predict([[-0.8, -1]]))
[1]
>>> clf_pf = GaussianNB()
>>> clf_pf.partial_fit(X, Y, np.unique(Y))
GaussianNB()
>>> print(clf_pf.predict([[-0.8, -1]]))
[1]




fit(X, y, sample_weight=None)[source]#
Fit Gaussian Naive Bayes according to X, y.

Parameters:

Xarray-like of shape (n_samples, n_features)Training vectors, where n_samples is the number of samples
and n_features is the number of features.

yarray-like of shape (n_samples,)Target values.

sample_weightarray-like of shape (n_samples,), default=NoneWeights applied to individual samples (1. for unweighted).

Added in version 0.17: Gaussian Naive Bayes supports fitting with sample_weight.




Returns:

selfobjectReturns the instance itself.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







partial_fit(X, y, classes=None, sample_weight=None)[source]#
Incremental fit on a batch of samples.
This method is expected to be called several times consecutively
on different chunks of a dataset so as to implement out-of-core
or online learning.
This is especially useful when the whole dataset is too big to fit in
memory at once.
This method has some performance and numerical stability overhead,
hence it is better to call partial_fit on chunks of data that are
as large as possible (as long as fitting in the memory budget) to
hide the overhead.

Parameters:

Xarray-like of shape (n_samples, n_features)Training vectors, where n_samples is the number of samples and
n_features is the number of features.

yarray-like of shape (n_samples,)Target values.

classesarray-like of shape (n_classes,), default=NoneList of all the classes that can possibly appear in the y vector.
Must be provided at the first call to partial_fit, can be omitted
in subsequent calls.

sample_weightarray-like of shape (n_samples,), default=NoneWeights applied to individual samples (1. for unweighted).

Added in version 0.17.




Returns:

selfobjectReturns the instance itself.







predict(X)[source]#
Perform classification on an array of test vectors X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Cndarray of shape (n_samples,)Predicted target values for X.







predict_joint_log_proba(X)[source]#
Return joint log probability estimates for the test vector X.
For each row x of X and class y, the joint log probability is given by
log P(x, y) = log P(y) + log P(x|y),
where log P(y) is the class prior probability and log P(x|y) is
the class-conditional probability.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Cndarray of shape (n_samples, n_classes)Returns the joint log-probability of the samples for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.







predict_log_proba(X)[source]#
Return log-probability estimates for the test vector X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Carray-like of shape (n_samples, n_classes)Returns the log-probability of the samples for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.







predict_proba(X)[source]#
Return probability estimates for the test vector X.

Parameters:

Xarray-like of shape (n_samples, n_features)The input samples.



Returns:

Carray-like of shape (n_samples, n_classes)Returns the probability of the samples for each class in
the model. The columns correspond to the classes in sorted
order, as they appear in the attribute classes_.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → GaussianNB[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_partial_fit_request(*, classes: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → GaussianNB[source]#
Request metadata passed to the partial_fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to partial_fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to partial_fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

classesstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for classes parameter in partial_fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in partial_fit.



Returns:

selfobjectThe updated object.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → GaussianNB[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







Gallery examples#

Comparison of Calibration of Classifiers
Comparison of Calibration of Classifiers

Probability Calibration curves
Probability Calibration curves

Probability calibration of classifiers
Probability calibration of classifiers

Classifier comparison
Classifier comparison

Plot class probabilities calculated by the VotingClassifier
Plot class probabilities calculated by the VotingClassifier

Plotting Learning Curves and Checking Models’ Scalability
Plotting Learning Curves and Checking Models' Scalability










previous
ComplementNB




next
MultinomialNB










 On this page
  


GaussianNB
fit
get_metadata_routing
get_params
partial_fit
predict
predict_joint_log_proba
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_partial_fit_request
set_score_request


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gaussian Naive Bayes classifier (scikit-learn),Naive Bayes Classifier for numeric inputs (scikit-learn). Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes theorem with the “naive” assumption of independence between every pair of features.
Logistic Regression with no penalty.  Logistic regression is a generalized linear model that uses a logistic link function. Based on scikit-learn.,http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression,Logistic Regression with no penalty,LR,Binary Classification,"













LogisticRegression — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.linear_model
LogisticRegression









LogisticRegression#


class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)[source]#
Logistic Regression (aka logit, MaxEnt) classifier.
In the multiclass case, the training algorithm uses the one-vs-rest (OvR)
scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the
cross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’.
(Currently the ‘multinomial’ option is supported only by the ‘lbfgs’,
‘sag’, ‘saga’ and ‘newton-cg’ solvers.)
This class implements regularized logistic regression using the
‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers. Note
that regularization is applied by default. It can handle both dense
and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit
floats for optimal performance; any other input format will be converted
(and copied).
The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization
with primal formulation, or no regularization. The ‘liblinear’ solver
supports both L1 and L2 regularization, with a dual formulation only for
the L2 penalty. The Elastic-Net regularization is only supported by the
‘saga’ solver.
Read more in the User Guide.

Parameters:

penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’Specify the norm of the penalty:

None: no penalty is added;
'l2': add a L2 penalty term and it is the default choice;
'l1': add a L1 penalty term;
'elasticnet': both L1 and L2 penalty terms are added.


Warning
Some penalties may not work with some solvers. See the parameter
solver below, to know the compatibility between the penalty and
solver.


Added in version 0.19: l1 penalty with SAGA solver (allowing ‘multinomial’ + L1)


dualbool, default=FalseDual (constrained) or primal (regularized, see also
this equation) formulation. Dual formulation
is only implemented for l2 penalty with liblinear solver. Prefer dual=False when
n_samples > n_features.

tolfloat, default=1e-4Tolerance for stopping criteria.

Cfloat, default=1.0Inverse of regularization strength; must be a positive float.
Like in support vector machines, smaller values specify stronger
regularization.

fit_interceptbool, default=TrueSpecifies if a constant (a.k.a. bias or intercept) should be
added to the decision function.

intercept_scalingfloat, default=1Useful only when the solver ‘liblinear’ is used
and self.fit_intercept is set to True. In this case, x becomes
[x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equal to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic_feature_weight.
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.

class_weightdict or ‘balanced’, default=NoneWeights associated with classes in the form {class_label: weight}.
If not given, all classes are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as n_samples / (n_classes * np.bincount(y)).
Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.

Added in version 0.17: class_weight=’balanced’


random_stateint, RandomState instance, default=NoneUsed when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the
data. See Glossary for details.

solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’},             default=’lbfgs’Algorithm to use in the optimization problem. Default is ‘lbfgs’.
To choose a solver, you might want to consider the following aspects:

For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’
and ‘saga’ are faster for large ones;
For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and
‘lbfgs’ handle multinomial loss;
‘liblinear’ and ‘newton-cholesky’ can only handle binary classification
by default. To apply a one-versus-rest scheme for the multiclass setting
one can wrapt it with the OneVsRestClassifier.
‘newton-cholesky’ is a good choice for n_samples >> n_features,
especially with one-hot encoded categorical features with rare
categories. Be aware that the memory usage of this solver has a quadratic
dependency on n_features because it explicitly computes the Hessian
matrix.


Warning
The choice of the algorithm depends on the penalty chosen and on
(multinomial) multiclass support:


solver
penalty
multinomial multiclass



‘lbfgs’
‘l2’, None
yes

‘liblinear’
‘l1’, ‘l2’
no

‘newton-cg’
‘l2’, None
yes

‘newton-cholesky’
‘l2’, None
no

‘sag’
‘l2’, None
yes

‘saga’
‘elasticnet’, ‘l1’, ‘l2’, None
yes






Note
‘sag’ and ‘saga’ fast convergence is only guaranteed on features
with approximately the same scale. You can preprocess the data with
a scaler from sklearn.preprocessing.


See also
Refer to the User Guide for more information regarding
LogisticRegression and more specifically the
Table
summarizing solver/penalty supports.


Added in version 0.17: Stochastic Average Gradient descent solver.


Added in version 0.19: SAGA solver.


Changed in version 0.22: The default solver changed from ‘liblinear’ to ‘lbfgs’ in 0.22.


Added in version 1.2: newton-cholesky solver.


max_iterint, default=100Maximum number of iterations taken for the solvers to converge.

multi_class{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’If the option chosen is ‘ovr’, then a binary problem is fit for each
label. For ‘multinomial’ the loss minimised is the multinomial loss fit
across the entire probability distribution, even when the data is
binary. ‘multinomial’ is unavailable when solver=’liblinear’.
‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,
and otherwise selects ‘multinomial’.

Added in version 0.18: Stochastic Average Gradient descent solver for ‘multinomial’ case.


Changed in version 0.22: Default changed from ‘ovr’ to ‘auto’ in 0.22.


Deprecated since version 1.5: multi_class was deprecated in version 1.5 and will be removed in 1.7.
From then on, the recommended ‘multinomial’ will always be used for
n_classes >= 3.
Solvers that do not support ‘multinomial’ will raise an error.
Use sklearn.multiclass.OneVsRestClassifier(LogisticRegression()) if you
still want to use OvR.


verboseint, default=0For the liblinear and lbfgs solvers set verbose to any positive
number for verbosity.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
Useless for liblinear solver. See the Glossary.

Added in version 0.17: warm_start to support lbfgs, newton-cg, sag, saga solvers.


n_jobsint, default=NoneNumber of CPU cores used when parallelizing over classes if
multi_class=’ovr’”. This parameter is ignored when the solver is
set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or
not. None means 1 unless in a joblib.parallel_backend
context. -1 means using all processors.
See Glossary for more details.

l1_ratiofloat, default=NoneThe Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only
used if penalty='elasticnet'. Setting l1_ratio=0 is equivalent
to using penalty='l2', while setting l1_ratio=1 is equivalent
to using penalty='l1'. For 0 < l1_ratio <1, the penalty is a
combination of L1 and L2.



Attributes:

classes_ndarray of shape (n_classes, )A list of class labels known to the classifier.

coef_ndarray of shape (1, n_features) or (n_classes, n_features)Coefficient of the features in the decision function.
coef_ is of shape (1, n_features) when the given problem is binary.
In particular, when multi_class='multinomial', coef_ corresponds
to outcome 1 (True) and -coef_ corresponds to outcome 0 (False).

intercept_ndarray of shape (1,) or (n_classes,)Intercept (a.k.a. bias) added to the decision function.
If fit_intercept is set to False, the intercept is set to zero.
intercept_ is of shape (1,) when the given problem is binary.
In particular, when multi_class='multinomial', intercept_
corresponds to outcome 1 (True) and -intercept_ corresponds to
outcome 0 (False).

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


n_iter_ndarray of shape (n_classes,) or (1, )Actual number of iterations for all classes. If binary or multinomial,
it returns only 1 element. For liblinear solver, only the maximum
number of iteration across all classes is given.

Changed in version 0.20: In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
max_iter. n_iter_ will now report at most max_iter.






See also

SGDClassifierIncrementally trained logistic regression (when given the parameter loss=""log_loss"").

LogisticRegressionCVLogistic regression with built-in cross validation.



Notes
The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.
Predict output may not match that of standalone liblinear in certain
cases. See differences from liblinear
in the narrative documentation.
References

L-BFGS-B – Software for Large-scale Bound-constrained OptimizationCiyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.
http://users.iems.northwestern.edu/~nocedal/lbfgsb.html

LIBLINEAR – A Library for Large Linear Classificationhttps://www.csie.ntu.edu.tw/~cjlin/liblinear/

SAG – Mark Schmidt, Nicolas Le Roux, and Francis BachMinimizing Finite Sums with the Stochastic Average Gradient
https://hal.inria.fr/hal-00860051/document

SAGA – Defazio, A., Bach F. & Lacoste-Julien S. (2014).“SAGA: A Fast Incremental Gradient Method With Support
for Non-Strongly Convex Composite Objectives”

Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descentmethods for logistic regression and maximum entropy models.
Machine Learning 85(1-2):41-75.
https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf


Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import LogisticRegression
>>> X, y = load_iris(return_X_y=True)
>>> clf = LogisticRegression(random_state=0).fit(X, y)
>>> clf.predict(X[:2, :])
array([0, 0])
>>> clf.predict_proba(X[:2, :])
array([[9.8...e-01, 1.8...e-02, 1.4...e-08],
       [9.7...e-01, 2.8...e-02, ...e-08]])
>>> clf.score(X, y)
0.97...




decision_function(X)[source]#
Predict confidence scores for samples.
The confidence score for a sample is proportional to the signed
distance of that sample to the hyperplane.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the confidence scores.



Returns:

scoresndarray of shape (n_samples,) or (n_samples, n_classes)Confidence scores per (n_samples, n_classes) combination. In the
binary case, confidence score for self.classes_[1] where >0 means
this class would be predicted.







densify()[source]#
Convert coefficient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the
default format of coef_ and is required for fitting, so calling
this method is only required on models that have previously been
sparsified; otherwise, it is a no-op.

Returns:

selfFitted estimator.







fit(X, y, sample_weight=None)[source]#
Fit the model according to the given training data.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training vector, where n_samples is the number of samples and
n_features is the number of features.

yarray-like of shape (n_samples,)Target vector relative to X.

sample_weightarray-like of shape (n_samples,) default=NoneArray of weights that are assigned to individual samples.
If not provided, then each sample is given unit weight.

Added in version 0.17: sample_weight support to LogisticRegression.




Returns:

selfFitted estimator.




Notes
The SAGA solver supports both float64 and float32 bit arrays.



get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict class labels for samples in X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the predictions.



Returns:

y_predndarray of shape (n_samples,)Vector containing the class labels for each sample.







predict_log_proba(X)[source]#
Predict logarithm of probability estimates.
The returned estimates for all classes are ordered by the
label of classes.

Parameters:

Xarray-like of shape (n_samples, n_features)Vector to be scored, where n_samples is the number of samples and
n_features is the number of features.



Returns:

Tarray-like of shape (n_samples, n_classes)Returns the log-probability of the sample for each class in the
model, where classes are ordered as they are in self.classes_.







predict_proba(X)[source]#
Probability estimates.
The returned estimates for all classes are ordered by the
label of classes.
For a multi_class problem, if multi_class is set to be “multinomial”
the softmax function is used to find the predicted probability of
each class.
Else use a one-vs-rest approach, i.e. calculate the probability
of each class assuming it to be positive using the logistic function
and normalize these values across all the classes.

Parameters:

Xarray-like of shape (n_samples, n_features)Vector to be scored, where n_samples is the number of samples and
n_features is the number of features.



Returns:

Tarray-like of shape (n_samples, n_classes)Returns the probability of the sample for each class in the model,
where classes are ordered as they are in self.classes_.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → LogisticRegression[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → LogisticRegression[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







sparsify()[source]#
Convert coefficient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for
L1-regularized models can be much more memory- and storage-efficient
than the usual numpy.ndarray representation.
The intercept_ member is not converted.

Returns:

selfFitted estimator.




Notes
For non-sparse models, i.e. when there are not many zeros in coef_,
this may actually increase memory usage, so use this method with
care. A rule of thumb is that the number of zero elements, which can
be computed with (coef_ == 0).sum(), must be more than 50% for this
to provide significant benefits.
After calling this method, further fitting with the partial_fit
method (if any) will not work until you call densify.



Gallery examples#

Release Highlights for scikit-learn 1.5
Release Highlights for scikit-learn 1.5

Release Highlights for scikit-learn 1.3
Release Highlights for scikit-learn 1.3

Release Highlights for scikit-learn 1.1
Release Highlights for scikit-learn 1.1

Release Highlights for scikit-learn 1.0
Release Highlights for scikit-learn 1.0

Release Highlights for scikit-learn 0.24
Release Highlights for scikit-learn 0.24

Release Highlights for scikit-learn 0.23
Release Highlights for scikit-learn 0.23

Release Highlights for scikit-learn 0.22
Release Highlights for scikit-learn 0.22

Probability Calibration curves
Probability Calibration curves

Plot classification probability
Plot classification probability

Feature transformations with ensembles of trees
Feature transformations with ensembles of trees

Plot class probabilities calculated by the VotingClassifier
Plot class probabilities calculated by the VotingClassifier

Model-based and sequential feature selection
Model-based and sequential feature selection

Recursive feature elimination
Recursive feature elimination

Recursive feature elimination with cross-validation
Recursive feature elimination with cross-validation

Comparing various online solvers
Comparing various online solvers

L1 Penalty and Sparsity in Logistic Regression
L1 Penalty and Sparsity in Logistic Regression

Logistic Regression 3-class Classifier
Logistic Regression 3-class Classifier

Logistic function
Logistic function

MNIST classification using multinomial logistic + L1
MNIST classification using multinomial logistic + L1

Multiclass sparse logistic regression on 20newgroups
Multiclass sparse logistic regression on 20newgroups

Plot multinomial and One-vs-Rest Logistic Regression
Plot multinomial and One-vs-Rest Logistic Regression

Regularization path of L1- Logistic Regression
Regularization path of L1- Logistic Regression

Displaying Pipelines
Displaying Pipelines

Displaying estimators and complex pipelines
Displaying estimators and complex pipelines

Introducing the set_output API
Introducing the set_output API

Visualizations with Display Objects
Visualizations with Display Objects

Class Likelihood Ratios to measure classification performance
Class Likelihood Ratios to measure classification performance

Multiclass Receiver Operating Characteristic (ROC)
Multiclass Receiver Operating Characteristic (ROC)

Post-hoc tuning the cut-off point of decision function
Post-hoc tuning the cut-off point of decision function

Post-tuning the decision threshold for cost-sensitive learning
Post-tuning the decision threshold for cost-sensitive learning

Multilabel classification using a classifier chain
Multilabel classification using a classifier chain

Restricted Boltzmann Machine features for digit classification
Restricted Boltzmann Machine features for digit classification

Column Transformer with Mixed Types
Column Transformer with Mixed Types

Pipelining: chaining a PCA and a logistic regression
Pipelining: chaining a PCA and a logistic regression

Feature discretization
Feature discretization

Digits Classification Exercise
Digits Classification Exercise

Classification of text documents using sparse features
Classification of text documents using sparse features










previous
sklearn.linear_model




next
LogisticRegressionCV










 On this page
  


LogisticRegression
decision_function
densify
fit
get_metadata_routing
get_params
predict
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_score_request
sparsify


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Logistic Regression,Logistic Regression with no penalty.  Logistic regression is a generalized linear model that uses a logistic link function. Based on scikit-learn.
"Approximate Kernel Support Vector Classifier using sklearn LogisticRegression. Support vector machines are a class of “maximum margin” classifiers. They seek to maximize the separation they find between classes, and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations. ",http://scikit-learn.org/stable/modules/kernel_approximation.html,Approximate Kernel Support Vector Classifier using sklearn LogisticRegression,ASVMSKC,Binary Classification,"













6.7. Kernel Approximation — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

1. Supervised learning
1.1. Linear Models
1.2. Linear and Quadratic Discriminant Analysis
1.3. Kernel ridge regression
1.4. Support Vector Machines
1.5. Stochastic Gradient Descent
1.6. Nearest Neighbors
1.7. Gaussian Processes
1.8. Cross decomposition
1.9. Naive Bayes
1.10. Decision Trees
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking
1.12. Multiclass and multioutput algorithms
1.13. Feature selection
1.14. Semi-supervised learning
1.15. Isotonic regression
1.16. Probability calibration
1.17. Neural network models (supervised)


2. Unsupervised learning
2.1. Gaussian mixture models
2.2. Manifold learning
2.3. Clustering
2.4. Biclustering
2.5. Decomposing signals in components (matrix factorization problems)
2.6. Covariance estimation
2.7. Novelty and Outlier Detection
2.8. Density Estimation
2.9. Neural network models (unsupervised)


3. Model selection and evaluation
3.1. Cross-validation: evaluating estimator performance
3.2. Tuning the hyper-parameters of an estimator
3.3. Tuning the decision threshold for class prediction
3.4. Metrics and scoring: quantifying the quality of predictions
3.5. Validation curves: plotting scores to evaluate models


4. Inspection
4.1. Partial Dependence and Individual Conditional Expectation plots
4.2. Permutation feature importance


5. Visualizations
6. Dataset transformations
6.1. Pipelines and composite estimators
6.2. Feature extraction
6.3. Preprocessing data
6.4. Imputation of missing values
6.5. Unsupervised dimensionality reduction
6.6. Random Projection
6.7. Kernel Approximation
6.8. Pairwise metrics, Affinities and Kernels
6.9. Transforming the prediction target (y)


7. Dataset loading utilities
7.1. Toy datasets
7.2. Real world datasets
7.3. Generated datasets
7.4. Loading other datasets


8. Computing with scikit-learn
8.1. Strategies to scale computationally: bigger data
8.2. Computational Performance
8.3. Parallelism, resource management, and configuration


9. Model persistence
10. Common pitfalls and recommended practices
11. Dispatching
11.1. Array API support (experimental)


12. Choosing the right estimator
13. External Resources, Videos and Talks






















User Guide
6. Dataset transformations










6.7. Kernel Approximation#
This submodule contains functions that approximate the feature mappings that
correspond to certain kernels, as they are used for example in support vector
machines (see Support Vector Machines).
The following feature functions perform non-linear transformations of the
input, which can serve as a basis for linear classification or other
algorithms.
The advantage of using approximate explicit feature maps compared to the
kernel trick,
which makes use of feature maps implicitly, is that explicit mappings
can be better suited for online learning and can significantly reduce the cost
of learning with very large datasets.
Standard kernelized SVMs do not scale well to large datasets, but using an
approximate kernel map it is possible to use much more efficient linear SVMs.
In particular, the combination of kernel map approximations with
SGDClassifier can make non-linear learning on large datasets possible.
Since there has not been much empirical work using approximate embeddings, it
is advisable to compare results against exact kernel methods when possible.

See also
Polynomial regression: extending linear models with basis functions for an exact polynomial transformation.


6.7.1. Nystroem Method for Kernel Approximation#
The Nystroem method, as implemented in Nystroem is a general method for
reduced rank approximations of kernels. It achieves this by subsampling without
replacement rows/columns of the data on which the kernel is evaluated. While the
computational complexity of the exact method is
\(\mathcal{O}(n^3_{\text{samples}})\), the complexity of the approximation
is \(\mathcal{O}(n^2_{\text{components}} \cdot n_{\text{samples}})\), where
one can set \(n_{\text{components}} \ll n_{\text{samples}}\) without a
significative decrease in performance [WS2001].
We can construct the eigendecomposition of the kernel matrix \(K\), based
on the features of the data, and then split it into sampled and unsampled data
points.

\[\begin{split}K = U \Lambda U^T
= \begin{bmatrix} U_1 \\ U_2\end{bmatrix} \Lambda \begin{bmatrix} U_1 \\ U_2 \end{bmatrix}^T
= \begin{bmatrix} U_1 \Lambda U_1^T & U_1 \Lambda U_2^T \\ U_2 \Lambda U_1^T & U_2 \Lambda U_2^T \end{bmatrix}
\equiv \begin{bmatrix} K_{11} & K_{12} \\ K_{21} & K_{22} \end{bmatrix}\end{split}\]
where:

\(U\) is orthonormal
\(\Lambda\) is diagonal matrix of eigenvalues
\(U_1\) is orthonormal matrix of samples that were chosen
\(U_2\) is orthonormal matrix of samples that were not chosen

Given that \(U_1 \Lambda U_1^T\) can be obtained by orthonormalization of
the matrix \(K_{11}\), and \(U_2 \Lambda U_1^T\) can be evaluated (as
well as its transpose), the only remaining term to elucidate is
\(U_2 \Lambda U_2^T\). To do this we can express it in terms of the already
evaluated matrices:

\[\begin{split}\begin{align} U_2 \Lambda U_2^T &= \left(K_{21} U_1 \Lambda^{-1}\right) \Lambda \left(K_{21} U_1 \Lambda^{-1}\right)^T
\\&= K_{21} U_1 (\Lambda^{-1} \Lambda) \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} U_1 \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} K_{11}^{-1} K_{21}^T
\\&= \left( K_{21} K_{11}^{-\frac12} \right) \left( K_{21} K_{11}^{-\frac12} \right)^T
.\end{align}\end{split}\]
During fit, the class Nystroem evaluates the basis \(U_1\), and
computes the normalization constant, \(K_{11}^{-\frac12}\). Later, during
transform, the kernel matrix is determined between the basis (given by the
components_ attribute) and the new data points, X. This matrix is then
multiplied by the normalization_ matrix for the final result.
By default Nystroem uses the rbf kernel, but it can use any kernel
function or a precomputed kernel matrix. The number of samples used - which is
also the dimensionality of the features computed - is given by the parameter
n_components.
Examples

See the example entitled
Time-related feature engineering,
that shows an efficient machine learning pipeline that uses a
Nystroem kernel.



6.7.2. Radial Basis Function Kernel#
The RBFSampler constructs an approximate mapping for the radial basis
function kernel, also known as Random Kitchen Sinks [RR2007]. This
transformation can be used to explicitly model a kernel map, prior to applying
a linear algorithm, for example a linear SVM:
>>> from sklearn.kernel_approximation import RBFSampler
>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
>>> y = [0, 0, 1, 1]
>>> rbf_feature = RBFSampler(gamma=1, random_state=1)
>>> X_features = rbf_feature.fit_transform(X)
>>> clf = SGDClassifier(max_iter=5)
>>> clf.fit(X_features, y)
SGDClassifier(max_iter=5)
>>> clf.score(X_features, y)
1.0


The mapping relies on a Monte Carlo approximation to the
kernel values. The fit function performs the Monte Carlo sampling, whereas
the transform method performs the mapping of the data.  Because of the
inherent randomness of the process, results may vary between different calls to
the fit function.
The fit function takes two arguments:
n_components, which is the target dimensionality of the feature transform,
and gamma, the parameter of the RBF-kernel.  A higher n_components will
result in a better approximation of the kernel and will yield results more
similar to those produced by a kernel SVM. Note that “fitting” the feature
function does not actually depend on the data given to the fit function.
Only the dimensionality of the data is used.
Details on the method can be found in [RR2007].
For a given value of n_components RBFSampler is often less accurate
as Nystroem. RBFSampler is cheaper to compute, though, making
use of larger feature spaces more efficient.




Comparing an exact RBF kernel (left) with the approximation (right)#


Examples

Explicit feature map approximation for RBF kernels



6.7.3. Additive Chi Squared Kernel#
The additive chi squared kernel is a kernel on histograms, often used in computer vision.
The additive chi squared kernel as used here is given by

\[k(x, y) = \sum_i \frac{2x_iy_i}{x_i+y_i}\]
This is not exactly the same as sklearn.metrics.pairwise.additive_chi2_kernel.
The authors of [VZ2010] prefer the version above as it is always positive
definite.
Since the kernel is additive, it is possible to treat all components
\(x_i\) separately for embedding. This makes it possible to sample
the Fourier transform in regular intervals, instead of approximating
using Monte Carlo sampling.
The class AdditiveChi2Sampler implements this component wise
deterministic sampling. Each component is sampled \(n\) times, yielding
\(2n+1\) dimensions per input dimension (the multiple of two stems
from the real and complex part of the Fourier transform).
In the literature, \(n\) is usually chosen to be 1 or 2, transforming
the dataset to size n_samples * 5 * n_features (in the case of \(n=2\)).
The approximate feature map provided by AdditiveChi2Sampler can be combined
with the approximate feature map provided by RBFSampler to yield an approximate
feature map for the exponentiated chi squared kernel.
See the [VZ2010] for details and [VVZ2010] for combination with the RBFSampler.


6.7.4. Skewed Chi Squared Kernel#
The skewed chi squared kernel is given by:

\[k(x,y) = \prod_i \frac{2\sqrt{x_i+c}\sqrt{y_i+c}}{x_i + y_i + 2c}\]
It has properties that are similar to the exponentiated chi squared kernel
often used in computer vision, but allows for a simple Monte Carlo
approximation of the feature map.
The usage of the SkewedChi2Sampler is the same as the usage described
above for the RBFSampler. The only difference is in the free
parameter, that is called \(c\).
For a motivation for this mapping and the mathematical details see [LS2010].


6.7.5. Polynomial Kernel Approximation via Tensor Sketch#
The polynomial kernel is a popular type of kernel
function given by:

\[k(x, y) = (\gamma x^\top y +c_0)^d\]
where:

x, y are the input vectors
d is the kernel degree

Intuitively, the feature space of the polynomial kernel of degree d
consists of all possible degree-d products among input features, which enables
learning algorithms using this kernel to account for interactions between features.
The TensorSketch [PP2013] method, as implemented in PolynomialCountSketch, is a
scalable, input data independent method for polynomial kernel approximation.
It is based on the concept of Count sketch [WIKICS] [CCF2002] , a dimensionality
reduction technique similar to feature hashing, which instead uses several
independent hash functions. TensorSketch obtains a Count Sketch of the outer product
of two vectors (or a vector with itself), which can be used as an approximation of the
polynomial kernel feature space. In particular, instead of explicitly computing
the outer product, TensorSketch computes the Count Sketch of the vectors and then
uses polynomial multiplication via the Fast Fourier Transform to compute the
Count Sketch of their outer product.
Conveniently, the training phase of TensorSketch simply consists of initializing
some random variables. It is thus independent of the input data, i.e. it only
depends on the number of input features, but not the data values.
In addition, this method can transform samples in
\(\mathcal{O}(n_{\text{samples}}(n_{\text{features}} + n_{\text{components}} \log(n_{\text{components}})))\)
time, where \(n_{\text{components}}\) is the desired output dimension,
determined by n_components.
Examples

Scalable learning with polynomial kernel approximation



6.7.6. Mathematical Details#
Kernel methods like support vector machines or kernelized
PCA rely on a property of reproducing kernel Hilbert spaces.
For any positive definite kernel function \(k\) (a so called Mercer kernel),
it is guaranteed that there exists a mapping \(\phi\)
into a Hilbert space \(\mathcal{H}\), such that

\[k(x,y) = \langle \phi(x), \phi(y) \rangle\]
Where \(\langle \cdot, \cdot \rangle\) denotes the inner product in the
Hilbert space.
If an algorithm, such as a linear support vector machine or PCA,
relies only on the scalar product of data points \(x_i\), one may use
the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm
to the mapped data points \(\phi(x_i)\).
The advantage of using \(k\) is that the mapping \(\phi\) never has
to be calculated explicitly, allowing for arbitrary large
features (even infinite).
One drawback of kernel methods is, that it might be necessary
to store many kernel values \(k(x_i, x_j)\) during optimization.
If a kernelized classifier is applied to new data \(y_j\),
\(k(x_i, y_j)\) needs to be computed to make predictions,
possibly for many different \(x_i\) in the training set.
The classes in this submodule allow to approximate the embedding
\(\phi\), thereby working explicitly with the representations
\(\phi(x_i)\), which obviates the need to apply the kernel
or store training examples.
References


[WS2001]
“Using the Nyström method to speed up kernel machines”
Williams, C.K.I.; Seeger, M. - 2001.


[RR2007]
(1,2)
“Random features for large-scale kernel machines”
Rahimi, A. and Recht, B. - Advances in neural information processing 2007,


[LS2010]
“Random Fourier approximations for skewed multiplicative histogram kernels”
Li, F., Ionescu, C., and Sminchisescu, C.
- Pattern Recognition,  DAGM 2010, Lecture Notes in Computer Science.


[VZ2010]
(1,2)
“Efficient additive kernels via explicit feature maps”
Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010


[VVZ2010]
“Generalized RBF feature maps for Efficient Detection”
Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010


[PP2013]
“Fast and scalable polynomial kernels via explicit feature maps”
Pham, N., & Pagh, R. - 2013


[CCF2002]
“Finding frequent items in data streams”
Charikar, M., Chen, K., & Farach-Colton - 2002


[WIKICS]
“Wikipedia: Count sketch”












previous
6.6. Random Projection




next
6.8. Pairwise metrics, Affinities and Kernels










 On this page
  


6.7.1. Nystroem Method for Kernel Approximation
6.7.2. Radial Basis Function Kernel
6.7.3. Additive Chi Squared Kernel
6.7.4. Skewed Chi Squared Kernel
6.7.5. Polynomial Kernel Approximation via Tensor Sketch
6.7.6. Mathematical Details





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Nystroem Kernel SVM Classifier using Logistic Regression,"Approximate Kernel Support Vector Classifier using sklearn LogisticRegression. Support vector machines are a class of “maximum margin” classifiers. They seek to maximize the separation they find between classes, and can optionally include a penalty function that allows them to mis-classify some observations for the sake of wider margins between the classes for the rest of the observations. "
Random Forests based on scikit-learn. Random forests are an ensemble method where hundreds (or thousands) of individual decision trees are fit to boostrap re-samples of the original dataset.  ExtraTrees are a variant of RandomForests with even more randomness.,,Random Forests based on scikit-learn,RFC,Binary Classification,,NUM,ExtraTrees Classifier (Gini),Random Forests based on scikit-learn. Random forests are an ensemble method where hundreds (or thousands) of individual decision trees are fit to boostrap re-samples of the original dataset.  ExtraTrees are a variant of RandomForests with even more randomness.
Gradient Boosting Classifier (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Classifier (xgboost) with Early-Stopping,ESXGBC2,Binary Classification,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Classifier with Early Stopping,Gradient Boosting Classifier (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Light GBM Classifier with Early Stopping with GBDT and Boosting on Residuals,https://github.com/Microsoft/LightGBM/blob/master/docs/README.md,Light GBM Classifier with Early Stopping with GBDT and Boosting on Residuals,RES_ESLGBMTC,Boosted Classification,no documentation retrieved,NUM,Light Gradient Boosting Classification on ElasticNet Predictions,Light GBM Classifier with Early Stopping with GBDT and Boosting on Residuals
Boosting on ElasticNet Predictions. A Extreme Gradient Boosting Machine is trained with as offset the raw predictions of the main model,no url,Boosting on ElasticNet Predictions,RES_XGBC2,Boosted Classification,no documentation retrieved,NUM,eXtreme Gradient Boosting on ElasticNet Predictions,Boosting on ElasticNet Predictions. A Extreme Gradient Boosting Machine is trained with as offset the raw predictions of the main model
Text fit on Residuals. A ElasticNet Classifier is trained on text features with as offset the raw predictions of the main model,no url,Text fit on Residuals,XL_LENETCDWC,Boosted Classification,no documentation retrieved,NUM,Text fit on Residuals (L1 / Binomial Deviance),Text fit on Residuals. A ElasticNet Classifier is trained on text features with as offset the raw predictions of the main model
Light GBM Classifier with GBDT with Boosting on Residuals,https://github.com/Microsoft/LightGBM/blob/master/docs/README.md,Light GBM Classifier with GBDT with Boosting on Residuals,RES_PLGBMTC,Boosted Classification,no documentation retrieved,NUM,Light Gradient Boosting on ElasticNet Predictions,Light GBM Classifier with GBDT with Boosting on Residuals
Text fit on Residuals. A ElasticNet Classifier is trained on text features with as offset the raw predictions of the main model,no url,Text fit on Residuals,XL_LENETCD,Boosted Classification,no documentation retrieved,NUM,Text fit on Residuals (L1 / Binomial Deviance),Text fit on Residuals. A ElasticNet Classifier is trained on text features with as offset the raw predictions of the main model
Gradient Boosting Classifier (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Classifier (xgboost) with Early-Stopping,XL_XGBC2,Boosted Classification,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Classifier (Boosted Predictions),Gradient Boosting Classifier (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
"Keras-based Image Fine-Tune Regressor
    
    The module uses Convolutional Neural Networks (CNNs) to employ transfer learning (""fine-tuning"") for
    image variable types. It initializes the CNNs using pretrained weights and proceeds to train and
    fine-tune those weights based on the dataset.
    
    Transfer learning methodologies rely on different elements, the two most
    essential of which are task size and similarity to the pretrained image dataset. Becaise Deep CNN
    (DCNN) features are more generic in early layers and more dataset-specific in later layers, the
    module is capable of only fine-tuning the higher level layers of the network.
    This is driven by the understanding that the prior features of a DCNN contain more generic image
    features that are useful to many tasks, but later layers of the Deep CNN become progressively more
    specific to the exact elements of the classes relevant in the given task.
    
    Parameters
    ----------
    Pretrained model (model_name): select (default=squeezenet)
        Model architecture trained on a pretrained dataset.
    
        ``values: ['squeezenet', 'resnet50', 'xception', 'efficientnet-b0', 'efficientnet-b4']``
    Batch size (batch_size): int (default=32)
        Mini-batch size for training.
    
        ``values: [0, 32]``
    Number of epoch (epoch): int (default=1000)
        Number of epochs for training.
    
        ``values: [0, 1000]``
    Early Stop parameter (earlystop_patience): int (default=5)
        Early stopping window. Number of epochs that stops the model from training when the validation
        score consecutively fails to improve
    
        ``values: [0, 1000]``
    reduce_lr_on_plateau (reduce_lr_on_plateau): selectgrid (default=False)
        When True, reduces the learning rate when a metric has stopped improving.
    reduce_lr_patience (reduce_lr_patience): intgrid (default=3)
        Number of epochs to wait before reducing the learning rate.
    reduce_lr_factor (reduce_lr_factor): float (default=0.2)
        Factor by which the learning rate will be reduced. Specifically `new_lr = lr * factor`.
    
        ``values: [0, 1]``
    weights_initialization (weights_initialization): select (default=pretrained)
        Whether to use a randomly initialized model or pretrained weights as a starting point before
        fine-tuning the model base.
    
        ``values: ['random', 'pretrained']``
    optimizer (optimizer): select (default=rmsprop)
        Name of the optimizer.
    
        ``values: ['rmsprop', 'adadelta', 'adagrad', 'adam', 'momentum', 'sgd', 'adam_tf']``
    use_discriminative_learning_rate (use_discriminative_learning_rate): selectgrid (default=False)
        When True (default is False), uses a different learning rate for each trainable layer. By
        default, DataRobot uses cosine learning rate decay structure (decayed to 0.0 learning rate) to
        decay learning rate by layer. If not all layers are trainable, learning rate decay proceeds by
        layer number. If all the layers are trainable, learning rate decays by convolutional
        blocks/groups specific to the model architecture chosen.
    
        ``values: ['True', 'False']``
    learning rate (learning_rate_init): float (default='auto')
        Initial learning rate. When value is set to auto, we set the initial learning rate is based on
        the chosen optimizer.
    
        ``values: {'float': [0, 1], 'select': ['auto']}``
    Trainable scope (trainable_scope): multi (default='all')
        Number of layers to enable training the weights of the base CNN model in fine-tune modelers.
        Either:
    
        integers: enable the last convolutional layers of the chosen network to be trainable
    
        all: All learnable layers are trainable
    
        chain_thaw: First, fine-tunes any new layers (often only a Softmax layer)
        to the target task until convergence on a validation set. Then, fine-tunes each
        layer individually starting from the first layer in the network. Last, the entire model is
        trained with all layers. Each time the model converges as measured on the validation set, the
        weights are reloaded to the best setting, thereby preventing overfitting to any layer.
    
        ``values: {'int': [0, 100], 'select': ['all', 'chain_thaw']}``
    featurizer_pool (featurizer_pool): select (default=avg)
        Type of summarizer to use to squash the multi-dimensional CNN features
        applied on initial, intermediate, and top convolutional layers of the network.
    
        ``values: ['avg', 'gem', 'max']``
    image_aug_list_id (image_aug_list_id): select (default=None)
        ID of the augmentation list used to control the transformations applied to the images
    
        ``values: [None]``
    loss (loss): select (default=mean_squared_error)
        The type of loss used for tuning the model.
    
        ``values: ['gaussian', 'poisson', 'tweedie', 'mean_absolute_error', 'mean_squared_error',
        'root_mean_squared_error']``
    
    References
    ----------
    .. [1] Jeremy Howard and Sebastian Ruder.
       ""Universal language  model  fine-tuning  for  text  classification"".
       arXiv preprint arXiv:1801.06146.
       `[link]
       <https://arxiv.org/pdf/1801.06146v5.pdf>`__
    .. [2] Pan, S. J., and Yang, Q.
       ""A Survey on Transfer Learning""
       Knowledge and Data Engineering, IEEE Transactionson 22 (10): 1345-1359.
       `[link]
       <https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf>`__
    
    See Also
    --------
    Source:
        `Convolutional Neural Network wikipedia
        <https://en.wikipedia.org/wiki/Convolutional_neural_network>`_",no url,Keras-based Image Fine-Tune Regressor,IMGFTR,Multi-class Classification,no documentation retrieved,NUM,Fine-Tuned Image Regressor (All Layers),Image Regression using pre-trained deep neural network models.
Stochastic Gradient Descent Classifier. Based on scikit-learn. Stochastic Gradient Descent is a extremely scalable method for fitting linear regression models to big data.,http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier,Stochastic Gradient Descent Classifier,SGDC,Multi-class Classification,"













SGDClassifier — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.linear_model
SGDClassifier









SGDClassifier#


class sklearn.linear_model.SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)[source]#
Linear classifiers (SVM, logistic regression, etc.) with SGD training.
This estimator implements regularized linear models with stochastic
gradient descent (SGD) learning: the gradient of the loss is estimated
each sample at a time and the model is updated along the way with a
decreasing strength schedule (aka learning rate). SGD allows minibatch
(online/out-of-core) learning via the partial_fit method.
For best results using the default learning rate schedule, the data should
have zero mean and unit variance.
This implementation works with data represented as dense or sparse arrays
of floating point values for the features. The model it fits can be
controlled with the loss parameter; by default, it fits a linear support
vector machine (SVM).
The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.
Read more in the User Guide.

Parameters:

loss{‘hinge’, ‘log_loss’, ‘modified_huber’, ‘squared_hinge’,        ‘perceptron’, ‘squared_error’, ‘huber’, ‘epsilon_insensitive’,        ‘squared_epsilon_insensitive’}, default=’hinge’The loss function to be used.

‘hinge’ gives a linear SVM.
‘log_loss’ gives logistic regression, a probabilistic classifier.
‘modified_huber’ is another smooth loss that brings tolerance to
outliers as well as probability estimates.
‘squared_hinge’ is like hinge but is quadratically penalized.
‘perceptron’ is the linear loss used by the perceptron algorithm.
The other losses, ‘squared_error’, ‘huber’, ‘epsilon_insensitive’ and
‘squared_epsilon_insensitive’ are designed for regression but can be useful
in classification as well; see
SGDRegressor for a description.

More details about the losses formulas can be found in the
User Guide.

penalty{‘l2’, ‘l1’, ‘elasticnet’, None}, default=’l2’The penalty (aka regularization term) to be used. Defaults to ‘l2’
which is the standard regularizer for linear SVM models. ‘l1’ and
‘elasticnet’ might bring sparsity to the model (feature selection)
not achievable with ‘l2’. No penalty is added when set to None.

alphafloat, default=0.0001Constant that multiplies the regularization term. The higher the
value, the stronger the regularization. Also used to compute the
learning rate when learning_rate is set to ‘optimal’.
Values must be in the range [0.0, inf).

l1_ratiofloat, default=0.15The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Only used if penalty is ‘elasticnet’.
Values must be in the range [0.0, 1.0].

fit_interceptbool, default=TrueWhether the intercept should be estimated or not. If False, the
data is assumed to be already centered.

max_iterint, default=1000The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the fit method, and not the
partial_fit method.
Values must be in the range [1, inf).

Added in version 0.19.


tolfloat or None, default=1e-3The stopping criterion. If it is not None, training will stop
when (loss > best_loss - tol) for n_iter_no_change consecutive
epochs.
Convergence is checked against the training loss or the
validation loss depending on the early_stopping parameter.
Values must be in the range [0.0, inf).

Added in version 0.19.


shufflebool, default=TrueWhether or not the training data should be shuffled after each epoch.

verboseint, default=0The verbosity level.
Values must be in the range [0, inf).

epsilonfloat, default=0.1Epsilon in the epsilon-insensitive loss functions; only if loss is
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.
For ‘huber’, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.
Values must be in the range [0.0, inf).

n_jobsint, default=NoneThe number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation.
None means 1 unless in a joblib.parallel_backend context.
-1 means using all processors. See Glossary
for more details.

random_stateint, RandomState instance, default=NoneUsed for shuffling the data, when shuffle is set to True.
Pass an int for reproducible output across multiple function calls.
See Glossary.
Integer values must be in the range [0, 2**32 - 1].

learning_ratestr, default=’optimal’The learning rate schedule:

‘constant’: eta = eta0
‘optimal’: eta = 1.0 / (alpha * (t + t0))
where t0 is chosen by a heuristic proposed by Leon Bottou.
‘invscaling’: eta = eta0 / pow(t, power_t)
‘adaptive’: eta = eta0, as long as the training keeps decreasing.
Each time n_iter_no_change consecutive epochs fail to decrease the
training loss by tol or fail to increase validation score by tol if
early_stopping is True, the current learning rate is divided by 5.


Added in version 0.20: Added ‘adaptive’ option





eta0float, default=0.0The initial learning rate for the ‘constant’, ‘invscaling’ or
‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by
the default schedule ‘optimal’.
Values must be in the range [0.0, inf).

power_tfloat, default=0.5The exponent for inverse scaling learning rate.
Values must be in the range (-inf, inf).

early_stoppingbool, default=FalseWhether to use early stopping to terminate training when validation
score is not improving. If set to True, it will automatically set aside
a stratified fraction of training data as validation and terminate
training when validation score returned by the score method is not
improving by at least tol for n_iter_no_change consecutive epochs.

Added in version 0.20: Added ‘early_stopping’ option


validation_fractionfloat, default=0.1The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.
Values must be in the range (0.0, 1.0).

Added in version 0.20: Added ‘validation_fraction’ option


n_iter_no_changeint, default=5Number of iterations with no improvement to wait before stopping
fitting.
Convergence is checked against the training loss or the
validation loss depending on the early_stopping parameter.
Integer values must be in the range [1, max_iter).

Added in version 0.20: Added ‘n_iter_no_change’ option


class_weightdict, {class_label: weight} or “balanced”, default=NonePreset for the class_weight fit parameter.
Weights associated with classes. If not given, all classes
are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as n_samples / (n_classes * np.bincount(y)).

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See the Glossary.
Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.
If a dynamic learning rate is used, the learning rate is adapted
depending on the number of samples already seen. Calling fit resets
this counter, while partial_fit will result in increasing the
existing counter.

averagebool or int, default=FalseWhen set to True, computes the averaged SGD weights across all
updates and stores the result in the coef_ attribute. If set to
an int greater than 1, averaging will begin once the total number of
samples seen reaches average. So average=10 will begin
averaging after seeing 10 samples.
Integer values must be in the range [1, n_samples].



Attributes:

coef_ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)Weights assigned to the features.

intercept_ndarray of shape (1,) if n_classes == 2 else (n_classes,)Constants in decision function.

n_iter_intThe actual number of iterations before reaching the stopping criterion.
For multiclass fits, it is the maximum over every binary fit.

loss_function_concrete LossFunction
Deprecated since version 1.4: Attribute loss_function_ was deprecated in version 1.4 and will be
removed in 1.6.


classes_array of shape (n_classes,)
t_intNumber of weight updates performed during training.
Same as (n_iter_ * n_samples + 1).

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

sklearn.svm.LinearSVCLinear support vector classification.

LogisticRegressionLogistic regression.

PerceptronInherits from SGDClassifier. Perceptron() is equivalent to SGDClassifier(loss=""perceptron"", eta0=1, learning_rate=""constant"", penalty=None).



Examples
>>> import numpy as np
>>> from sklearn.linear_model import SGDClassifier
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.pipeline import make_pipeline
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> Y = np.array([1, 1, 2, 2])
>>> # Always scale the input. The most convenient way is to use a pipeline.
>>> clf = make_pipeline(StandardScaler(),
...                     SGDClassifier(max_iter=1000, tol=1e-3))
>>> clf.fit(X, Y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('sgdclassifier', SGDClassifier())])
>>> print(clf.predict([[-0.8, -1]]))
[1]




decision_function(X)[source]#
Predict confidence scores for samples.
The confidence score for a sample is proportional to the signed
distance of that sample to the hyperplane.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the confidence scores.



Returns:

scoresndarray of shape (n_samples,) or (n_samples, n_classes)Confidence scores per (n_samples, n_classes) combination. In the
binary case, confidence score for self.classes_[1] where >0 means
this class would be predicted.







densify()[source]#
Convert coefficient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the
default format of coef_ and is required for fitting, so calling
this method is only required on models that have previously been
sparsified; otherwise, it is a no-op.

Returns:

selfFitted estimator.







fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)[source]#
Fit linear model with Stochastic Gradient Descent.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Training data.

yndarray of shape (n_samples,)Target values.

coef_initndarray of shape (n_classes, n_features), default=NoneThe initial coefficients to warm-start the optimization.

intercept_initndarray of shape (n_classes,), default=NoneThe initial intercept to warm-start the optimization.

sample_weightarray-like, shape (n_samples,), default=NoneWeights applied to individual samples.
If not provided, uniform weights are assumed. These weights will
be multiplied with class_weight (passed through the
constructor) if class_weight is specified.



Returns:

selfobjectReturns an instance of self.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







partial_fit(X, y, classes=None, sample_weight=None)[source]#
Perform one epoch of stochastic gradient descent on given samples.
Internally, this method uses max_iter = 1. Therefore, it is not
guaranteed that a minimum of the cost function is reached after calling
it once. Matters such as objective convergence, early stopping, and
learning rate adjustments should be handled by the user.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Subset of the training data.

yndarray of shape (n_samples,)Subset of the target values.

classesndarray of shape (n_classes,), default=NoneClasses across all calls to partial_fit.
Can be obtained by via np.unique(y_all), where y_all is the
target vector of the entire dataset.
This argument is required for the first call to partial_fit
and can be omitted in the subsequent calls.
Note that y doesn’t need to contain all labels in classes.

sample_weightarray-like, shape (n_samples,), default=NoneWeights applied to individual samples.
If not provided, uniform weights are assumed.



Returns:

selfobjectReturns an instance of self.







predict(X)[source]#
Predict class labels for samples in X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the predictions.



Returns:

y_predndarray of shape (n_samples,)Vector containing the class labels for each sample.







predict_log_proba(X)[source]#
Log of probability estimates.
This method is only available for log loss and modified Huber loss.
When loss=”modified_huber”, probability estimates may be hard zeros
and ones, so taking the logarithm is not possible.
See predict_proba for details.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Input data for prediction.



Returns:

Tarray-like, shape (n_samples, n_classes)Returns the log-probability of the sample for each class in the
model, where classes are ordered as they are in
self.classes_.







predict_proba(X)[source]#
Probability estimates.
This method is only available for log loss and modified Huber loss.
Multiclass probability estimates are derived from binary (one-vs.-rest)
estimates by simple normalization, as recommended by Zadrozny and
Elkan.
Binary probability estimates for loss=”modified_huber” are given by
(clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions
it is necessary to perform proper probability calibration by wrapping
the classifier with
CalibratedClassifierCV instead.

Parameters:

X{array-like, sparse matrix}, shape (n_samples, n_features)Input data for prediction.



Returns:

ndarray of shape (n_samples, n_classes)Returns the probability of the sample for each class in the model,
where classes are ordered as they are in self.classes_.




References
Zadrozny and Elkan, “Transforming classifier scores into multiclass
probability estimates”, SIGKDD’02,
https://dl.acm.org/doi/pdf/10.1145/775047.775151
The justification for the formula in the loss=”modified_huber”
case is in the appendix B in:
http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf



score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, coef_init: bool | None | str = '$UNCHANGED$', intercept_init: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → SGDClassifier[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

coef_initstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for coef_init parameter in fit.

intercept_initstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for intercept_init parameter in fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_partial_fit_request(*, classes: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → SGDClassifier[source]#
Request metadata passed to the partial_fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to partial_fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to partial_fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

classesstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for classes parameter in partial_fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in partial_fit.



Returns:

selfobjectThe updated object.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → SGDClassifier[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







sparsify()[source]#
Convert coefficient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for
L1-regularized models can be much more memory- and storage-efficient
than the usual numpy.ndarray representation.
The intercept_ member is not converted.

Returns:

selfFitted estimator.




Notes
For non-sparse models, i.e. when there are not many zeros in coef_,
this may actually increase memory usage, so use this method with
care. A rule of thumb is that the number of zero elements, which can
be computed with (coef_ == 0).sum(), must be more than 50% for this
to provide significant benefits.
After calling this method, further fitting with the partial_fit
method (if any) will not work until you call densify.



Gallery examples#

Model Complexity Influence
Model Complexity Influence

Out-of-core classification of text documents
Out-of-core classification of text documents

Comparing various online solvers
Comparing various online solvers

Early stopping of Stochastic Gradient Descent
Early stopping of Stochastic Gradient Descent

Plot multi-class SGD on the iris dataset
Plot multi-class SGD on the iris dataset

SGD: Maximum margin separating hyperplane
SGD: Maximum margin separating hyperplane

SGD: Penalties
SGD: Penalties

SGD: Weighted samples
SGD: Weighted samples

SGD: convex loss functions
SGD: convex loss functions

Explicit feature map approximation for RBF kernels
Explicit feature map approximation for RBF kernels

Comparing randomized search and grid search for hyperparameter estimation
Comparing randomized search and grid search for hyperparameter estimation

Semi-supervised Classification on a Text Dataset
Semi-supervised Classification on a Text Dataset

Classification of text documents using sparse features
Classification of text documents using sparse features










previous
RidgeClassifierCV




next
SGDOneClassSVM










 On this page
  


SGDClassifier
decision_function
densify
fit
get_metadata_routing
get_params
partial_fit
predict
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_partial_fit_request
set_score_request
sparsify


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Stochastic Gradient Descent Classifier,Stochastic Gradient Descent Classifier. Based on scikit-learn. Stochastic Gradient Descent is a extremely scalable method for fitting linear regression models to big data.
Logistic Regression with no penalty.  Logistic regression is a generalized linear model that uses a logistic link function. Based on lightning CDClassifier,http://contrib.scikit-learn.org/lightning/,Logistic Regression with no penalty,LRCD,Multi-class Classification,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Logistic Regression,Logistic Regression with no penalty.  Logistic regression is a generalized linear model that uses a logistic link function. Based on lightning CDClassifier
Logistic Regression with L1 or L2 penalty. The L2 (or ridge) penalty tends to shrink regression coefficients while the L1 (or lasso) penalty selects variables. Based on scikit-learn,http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression,Logistic Regression with L1 or L2 penalty,LR1,Multi-class Classification,"













LogisticRegression — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.linear_model
LogisticRegression









LogisticRegression#


class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)[source]#
Logistic Regression (aka logit, MaxEnt) classifier.
In the multiclass case, the training algorithm uses the one-vs-rest (OvR)
scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the
cross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’.
(Currently the ‘multinomial’ option is supported only by the ‘lbfgs’,
‘sag’, ‘saga’ and ‘newton-cg’ solvers.)
This class implements regularized logistic regression using the
‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers. Note
that regularization is applied by default. It can handle both dense
and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit
floats for optimal performance; any other input format will be converted
(and copied).
The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization
with primal formulation, or no regularization. The ‘liblinear’ solver
supports both L1 and L2 regularization, with a dual formulation only for
the L2 penalty. The Elastic-Net regularization is only supported by the
‘saga’ solver.
Read more in the User Guide.

Parameters:

penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’Specify the norm of the penalty:

None: no penalty is added;
'l2': add a L2 penalty term and it is the default choice;
'l1': add a L1 penalty term;
'elasticnet': both L1 and L2 penalty terms are added.


Warning
Some penalties may not work with some solvers. See the parameter
solver below, to know the compatibility between the penalty and
solver.


Added in version 0.19: l1 penalty with SAGA solver (allowing ‘multinomial’ + L1)


dualbool, default=FalseDual (constrained) or primal (regularized, see also
this equation) formulation. Dual formulation
is only implemented for l2 penalty with liblinear solver. Prefer dual=False when
n_samples > n_features.

tolfloat, default=1e-4Tolerance for stopping criteria.

Cfloat, default=1.0Inverse of regularization strength; must be a positive float.
Like in support vector machines, smaller values specify stronger
regularization.

fit_interceptbool, default=TrueSpecifies if a constant (a.k.a. bias or intercept) should be
added to the decision function.

intercept_scalingfloat, default=1Useful only when the solver ‘liblinear’ is used
and self.fit_intercept is set to True. In this case, x becomes
[x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equal to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic_feature_weight.
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.

class_weightdict or ‘balanced’, default=NoneWeights associated with classes in the form {class_label: weight}.
If not given, all classes are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as n_samples / (n_classes * np.bincount(y)).
Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.

Added in version 0.17: class_weight=’balanced’


random_stateint, RandomState instance, default=NoneUsed when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the
data. See Glossary for details.

solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’},             default=’lbfgs’Algorithm to use in the optimization problem. Default is ‘lbfgs’.
To choose a solver, you might want to consider the following aspects:

For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’
and ‘saga’ are faster for large ones;
For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and
‘lbfgs’ handle multinomial loss;
‘liblinear’ and ‘newton-cholesky’ can only handle binary classification
by default. To apply a one-versus-rest scheme for the multiclass setting
one can wrapt it with the OneVsRestClassifier.
‘newton-cholesky’ is a good choice for n_samples >> n_features,
especially with one-hot encoded categorical features with rare
categories. Be aware that the memory usage of this solver has a quadratic
dependency on n_features because it explicitly computes the Hessian
matrix.


Warning
The choice of the algorithm depends on the penalty chosen and on
(multinomial) multiclass support:


solver
penalty
multinomial multiclass



‘lbfgs’
‘l2’, None
yes

‘liblinear’
‘l1’, ‘l2’
no

‘newton-cg’
‘l2’, None
yes

‘newton-cholesky’
‘l2’, None
no

‘sag’
‘l2’, None
yes

‘saga’
‘elasticnet’, ‘l1’, ‘l2’, None
yes






Note
‘sag’ and ‘saga’ fast convergence is only guaranteed on features
with approximately the same scale. You can preprocess the data with
a scaler from sklearn.preprocessing.


See also
Refer to the User Guide for more information regarding
LogisticRegression and more specifically the
Table
summarizing solver/penalty supports.


Added in version 0.17: Stochastic Average Gradient descent solver.


Added in version 0.19: SAGA solver.


Changed in version 0.22: The default solver changed from ‘liblinear’ to ‘lbfgs’ in 0.22.


Added in version 1.2: newton-cholesky solver.


max_iterint, default=100Maximum number of iterations taken for the solvers to converge.

multi_class{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’If the option chosen is ‘ovr’, then a binary problem is fit for each
label. For ‘multinomial’ the loss minimised is the multinomial loss fit
across the entire probability distribution, even when the data is
binary. ‘multinomial’ is unavailable when solver=’liblinear’.
‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,
and otherwise selects ‘multinomial’.

Added in version 0.18: Stochastic Average Gradient descent solver for ‘multinomial’ case.


Changed in version 0.22: Default changed from ‘ovr’ to ‘auto’ in 0.22.


Deprecated since version 1.5: multi_class was deprecated in version 1.5 and will be removed in 1.7.
From then on, the recommended ‘multinomial’ will always be used for
n_classes >= 3.
Solvers that do not support ‘multinomial’ will raise an error.
Use sklearn.multiclass.OneVsRestClassifier(LogisticRegression()) if you
still want to use OvR.


verboseint, default=0For the liblinear and lbfgs solvers set verbose to any positive
number for verbosity.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
Useless for liblinear solver. See the Glossary.

Added in version 0.17: warm_start to support lbfgs, newton-cg, sag, saga solvers.


n_jobsint, default=NoneNumber of CPU cores used when parallelizing over classes if
multi_class=’ovr’”. This parameter is ignored when the solver is
set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or
not. None means 1 unless in a joblib.parallel_backend
context. -1 means using all processors.
See Glossary for more details.

l1_ratiofloat, default=NoneThe Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only
used if penalty='elasticnet'. Setting l1_ratio=0 is equivalent
to using penalty='l2', while setting l1_ratio=1 is equivalent
to using penalty='l1'. For 0 < l1_ratio <1, the penalty is a
combination of L1 and L2.



Attributes:

classes_ndarray of shape (n_classes, )A list of class labels known to the classifier.

coef_ndarray of shape (1, n_features) or (n_classes, n_features)Coefficient of the features in the decision function.
coef_ is of shape (1, n_features) when the given problem is binary.
In particular, when multi_class='multinomial', coef_ corresponds
to outcome 1 (True) and -coef_ corresponds to outcome 0 (False).

intercept_ndarray of shape (1,) or (n_classes,)Intercept (a.k.a. bias) added to the decision function.
If fit_intercept is set to False, the intercept is set to zero.
intercept_ is of shape (1,) when the given problem is binary.
In particular, when multi_class='multinomial', intercept_
corresponds to outcome 1 (True) and -intercept_ corresponds to
outcome 0 (False).

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


n_iter_ndarray of shape (n_classes,) or (1, )Actual number of iterations for all classes. If binary or multinomial,
it returns only 1 element. For liblinear solver, only the maximum
number of iteration across all classes is given.

Changed in version 0.20: In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
max_iter. n_iter_ will now report at most max_iter.






See also

SGDClassifierIncrementally trained logistic regression (when given the parameter loss=""log_loss"").

LogisticRegressionCVLogistic regression with built-in cross validation.



Notes
The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.
Predict output may not match that of standalone liblinear in certain
cases. See differences from liblinear
in the narrative documentation.
References

L-BFGS-B – Software for Large-scale Bound-constrained OptimizationCiyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.
http://users.iems.northwestern.edu/~nocedal/lbfgsb.html

LIBLINEAR – A Library for Large Linear Classificationhttps://www.csie.ntu.edu.tw/~cjlin/liblinear/

SAG – Mark Schmidt, Nicolas Le Roux, and Francis BachMinimizing Finite Sums with the Stochastic Average Gradient
https://hal.inria.fr/hal-00860051/document

SAGA – Defazio, A., Bach F. & Lacoste-Julien S. (2014).“SAGA: A Fast Incremental Gradient Method With Support
for Non-Strongly Convex Composite Objectives”

Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descentmethods for logistic regression and maximum entropy models.
Machine Learning 85(1-2):41-75.
https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf


Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import LogisticRegression
>>> X, y = load_iris(return_X_y=True)
>>> clf = LogisticRegression(random_state=0).fit(X, y)
>>> clf.predict(X[:2, :])
array([0, 0])
>>> clf.predict_proba(X[:2, :])
array([[9.8...e-01, 1.8...e-02, 1.4...e-08],
       [9.7...e-01, 2.8...e-02, ...e-08]])
>>> clf.score(X, y)
0.97...




decision_function(X)[source]#
Predict confidence scores for samples.
The confidence score for a sample is proportional to the signed
distance of that sample to the hyperplane.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the confidence scores.



Returns:

scoresndarray of shape (n_samples,) or (n_samples, n_classes)Confidence scores per (n_samples, n_classes) combination. In the
binary case, confidence score for self.classes_[1] where >0 means
this class would be predicted.







densify()[source]#
Convert coefficient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the
default format of coef_ and is required for fitting, so calling
this method is only required on models that have previously been
sparsified; otherwise, it is a no-op.

Returns:

selfFitted estimator.







fit(X, y, sample_weight=None)[source]#
Fit the model according to the given training data.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training vector, where n_samples is the number of samples and
n_features is the number of features.

yarray-like of shape (n_samples,)Target vector relative to X.

sample_weightarray-like of shape (n_samples,) default=NoneArray of weights that are assigned to individual samples.
If not provided, then each sample is given unit weight.

Added in version 0.17: sample_weight support to LogisticRegression.




Returns:

selfFitted estimator.




Notes
The SAGA solver supports both float64 and float32 bit arrays.



get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict class labels for samples in X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the predictions.



Returns:

y_predndarray of shape (n_samples,)Vector containing the class labels for each sample.







predict_log_proba(X)[source]#
Predict logarithm of probability estimates.
The returned estimates for all classes are ordered by the
label of classes.

Parameters:

Xarray-like of shape (n_samples, n_features)Vector to be scored, where n_samples is the number of samples and
n_features is the number of features.



Returns:

Tarray-like of shape (n_samples, n_classes)Returns the log-probability of the sample for each class in the
model, where classes are ordered as they are in self.classes_.







predict_proba(X)[source]#
Probability estimates.
The returned estimates for all classes are ordered by the
label of classes.
For a multi_class problem, if multi_class is set to be “multinomial”
the softmax function is used to find the predicted probability of
each class.
Else use a one-vs-rest approach, i.e. calculate the probability
of each class assuming it to be positive using the logistic function
and normalize these values across all the classes.

Parameters:

Xarray-like of shape (n_samples, n_features)Vector to be scored, where n_samples is the number of samples and
n_features is the number of features.



Returns:

Tarray-like of shape (n_samples, n_classes)Returns the probability of the sample for each class in the model,
where classes are ordered as they are in self.classes_.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → LogisticRegression[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → LogisticRegression[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







sparsify()[source]#
Convert coefficient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for
L1-regularized models can be much more memory- and storage-efficient
than the usual numpy.ndarray representation.
The intercept_ member is not converted.

Returns:

selfFitted estimator.




Notes
For non-sparse models, i.e. when there are not many zeros in coef_,
this may actually increase memory usage, so use this method with
care. A rule of thumb is that the number of zero elements, which can
be computed with (coef_ == 0).sum(), must be more than 50% for this
to provide significant benefits.
After calling this method, further fitting with the partial_fit
method (if any) will not work until you call densify.



Gallery examples#

Release Highlights for scikit-learn 1.5
Release Highlights for scikit-learn 1.5

Release Highlights for scikit-learn 1.3
Release Highlights for scikit-learn 1.3

Release Highlights for scikit-learn 1.1
Release Highlights for scikit-learn 1.1

Release Highlights for scikit-learn 1.0
Release Highlights for scikit-learn 1.0

Release Highlights for scikit-learn 0.24
Release Highlights for scikit-learn 0.24

Release Highlights for scikit-learn 0.23
Release Highlights for scikit-learn 0.23

Release Highlights for scikit-learn 0.22
Release Highlights for scikit-learn 0.22

Probability Calibration curves
Probability Calibration curves

Plot classification probability
Plot classification probability

Feature transformations with ensembles of trees
Feature transformations with ensembles of trees

Plot class probabilities calculated by the VotingClassifier
Plot class probabilities calculated by the VotingClassifier

Model-based and sequential feature selection
Model-based and sequential feature selection

Recursive feature elimination
Recursive feature elimination

Recursive feature elimination with cross-validation
Recursive feature elimination with cross-validation

Comparing various online solvers
Comparing various online solvers

L1 Penalty and Sparsity in Logistic Regression
L1 Penalty and Sparsity in Logistic Regression

Logistic Regression 3-class Classifier
Logistic Regression 3-class Classifier

Logistic function
Logistic function

MNIST classification using multinomial logistic + L1
MNIST classification using multinomial logistic + L1

Multiclass sparse logistic regression on 20newgroups
Multiclass sparse logistic regression on 20newgroups

Plot multinomial and One-vs-Rest Logistic Regression
Plot multinomial and One-vs-Rest Logistic Regression

Regularization path of L1- Logistic Regression
Regularization path of L1- Logistic Regression

Displaying Pipelines
Displaying Pipelines

Displaying estimators and complex pipelines
Displaying estimators and complex pipelines

Introducing the set_output API
Introducing the set_output API

Visualizations with Display Objects
Visualizations with Display Objects

Class Likelihood Ratios to measure classification performance
Class Likelihood Ratios to measure classification performance

Multiclass Receiver Operating Characteristic (ROC)
Multiclass Receiver Operating Characteristic (ROC)

Post-hoc tuning the cut-off point of decision function
Post-hoc tuning the cut-off point of decision function

Post-tuning the decision threshold for cost-sensitive learning
Post-tuning the decision threshold for cost-sensitive learning

Multilabel classification using a classifier chain
Multilabel classification using a classifier chain

Restricted Boltzmann Machine features for digit classification
Restricted Boltzmann Machine features for digit classification

Column Transformer with Mixed Types
Column Transformer with Mixed Types

Pipelining: chaining a PCA and a logistic regression
Pipelining: chaining a PCA and a logistic regression

Feature discretization
Feature discretization

Digits Classification Exercise
Digits Classification Exercise

Classification of text documents using sparse features
Classification of text documents using sparse features










previous
sklearn.linear_model




next
LogisticRegressionCV










 On this page
  


LogisticRegression
decision_function
densify
fit
get_metadata_routing
get_params
predict
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_score_request
sparsify


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Regularized Logistic Regression (L2),Logistic Regression with L1 or L2 penalty. The L2 (or ridge) penalty tends to shrink regression coefficients while the L1 (or lasso) penalty selects variables. Based on scikit-learn
"LightGBM Dropout Additive Regression Trees Classifier
    
    LightGBM is a gradient boosting framework. It uses a tree-based algorithm and is designed to be
    distributed and efficient, providing the following advantages:
    
    #. Faster training speed and higher efficiency
    #. Lower memory usage
    #. Better accuracy
    #. Support for parallel learning
    #. Handling of large-scale data
    
    DART (Dropout Additive Regression Trees, proposed by Rasmi et al.) is a novel way of employing
    dropouts in Gradient Boosted Trees. It results in the Dropout Additive Regression Trees algorithm.
    By employing dropout techniques commonly used by deep neural nets, Rasmi et al. showed improve
    Gradient Boosted Trees results (in some situations).
    
    **Gradient Boosting Machines:**
    
    Gradient Boosting Machines (or Generalized Boosted Models, depending on who you
    ask to explain the acronym 'GBM') are an advanced algorithm for fitting
    extremely accurate predictive models. GBMs have won a number of recent predictive
    modeling competitions and are considered by many data scientists to be the
    most versatile and useful predictive modeling algorithm. GBMs require very
    little preprocessing, elegantly handle missing data, strike a good balance between
    bias and variance, and are typically able to find complicated interaction terms, making them a
    useful ""Swiss army knife"" of predictive models.
    
    GBMs are a generalization of Freund and Schapire's adaboost algorithm (1995) that handles
    arbitrary loss functions. They are very similar in concept to random forests, in that
    they fit individual decision trees to random re-samples of input data, where each
    tree sees a bootstrap sample of the rows of the dataset and N arbitrarily chosen
    columns, where N is a configurable parameter of the model. GBMs differ from random
    forests in a single major aspect: rather than fitting the trees independently, the
    GBM fits each successive tree to the residual errors from all the previous trees
    combined. This is advantageous, as the model focuses each iteration on the examples
    that are most difficult to predict (and therefore most useful to get correct).
    
    Due to their iterative nature, GBMs are almost guaranteed to overfit the training data,
    given enough iterations. Therefore, the 2 critical parameters of the algorithm are the
    learning rate (or how fast the model fits the data) and the number of trees the model
    is allowed to fit. It is critical to tune one of these 2 parameters, and
    when done correctly, GBMs are capable of finding the exact point in the training data
    where overfitting begins, and halt one iteration prior to that point. In this manner GBMs
    are usually capable of squeezing every last bit of information out of the training
    set and producing a model with the highest possible accuracy without overfitting.
    
    Parameters
    ----------
    learning_rate (lr): floatgrid (default='0.1')
        Shrink the contribution of each tree by `learning_rate`.
        There is a trade-off between learning_rate (lr) and n_estimators(n).
        In dart, it also affects normalization
        ``values: [1e-7, 1e2]``
    n_estimators (n): intgrid (default='10')
        Number of boosting stages to perform.
        Gradient boosting is fairly robust to overfitting so a large number usually results in better
        performance.
        ``values: [1, 1e6]``
    num_leaves (nl): intgrid (default='31')
        Number of leaves in one tree.
        ``values: [2, 1e4]``
    max_depth (md): intgrid (default='none')
        Maximum depth of the individual regression estimators.
        The maximum depth limits the number of nodes in the tree. Tune this parameter
        for best performance; the best value depends on the interaction
        of the input variables. Deeper the tree the more variable interactions
        the model can capture. Tree still grow by leaf-wise.
        <0 means no limit
        ``values: ['none', [1, 1e4]]``
    max_bin (mb): intgrid (default='255')
        Max number of bin that feature values will bucket in.
        Small bin may reduce training accuracy but may increase general power (deal with overfit).
        LightGBM will auto compress memory according max_bin. For example, LightGBM will use
        uint8_t for feature value if max_bin=255.
        ``values: [3, 1e4]``
    subsample_for_bin (ssfb): int (default=50000)
        Number of samples for constructing bins.
        ``values: [1, 1e6]``
    min_split_gain (msg): floatgrid (default='0')
        Minimum loss reduction required to make a further partition on a leaf node of the tree.
        ``values: [0, 100]``
    min_child_weight (mcw): intgrid (default='5')
        Minimum sum of instance weight(hessian) needed in a child(leaf).
        ``values: [0, 1e2]``
    min_child_samples (mcs): int (default='10')
        Minimum number of data need in a child(leaf).
        ``values: [0, 1e3]``
    subsample (ss): floatgrid (default='1.0')
        Subsample ratio of the training instance.
        ``values: [0.01, 1]``
    subsample_freq (ssf): intgrid (default='1')
        Frequency of subsample 'none' means it is not enabled.
        ``values: ['none', [1, 1e3]]``
    colsample_bytree (cbt): floatgrid (default='1.0')
        Subsample ratio of columns when constructing each tree.
        By default, the value of ``colsample_bytree`` for LightGBM classes is 1.0. However, based on the
        training data, DataRobot may choose a different initial value for this parameter.
        ``values: [0, 1]``
    reg_alpha (ra): floatgrid (default='0')
        L1 regularization term on weights.
        ``values: [0, 1e6]``
    reg_lambda (rl): floatgrid (default='0')
        L2 regularization term on weights.
        ``values: [0, 1e6]``
    sigmoid (s): floatgrid (default='1.0')
        Parameter for sigmoid function. Used in binary classification and LambdaRank.
        ``values: [1e-06, 1e03]``
    is_unbalance (iu): select (default=False)
        Set to true if training data are unbalanced. Used in binary classification.
        ``values: [True, False]``
    drop_rate (dr): floatgrid (default='0.1')
        Dropout rate.
        ``values: [0, 1]``
    skip_drop (sd): floatgrid (default='0.5')
        Probability of skipping drop.
        ``values: [0, 1]``
    max_drop (md): intgrid (default='50')
        Max number of dropped trees on one iteration. <=0 means no limit.
        ``values: ['auto', [1, 1e3]]``
    uniform_drop (ud): select (default=False)
        True if want to use uniform drop.
        ``values: [True, False]``
    xgboost_dart_mode (xdm): select (default=False)
        True if want to use xgboost dart mode.
        ``values: [True, False]``
    drop_seed (ds): intgrid (default='4')
        Used to random seed to choose dropping models.
        ``values: [0, 1e2]``
    
    References
    ----------
    .. [1] Chen, T, and He, T.
       Higgs Boson Discovery with Boosted Trees."" Cowan et al.,
       editor, JMLR: Workshop and Conference Proceedings. No. 42. 2015.
       `[link]
       <http://proceedings.mlr.press/v42/chen14.pdf>`__
    .. [2] Freund, Yoav, and Robert E. Schapire.
       ""A decision-theoretic generalization of on-line learning and an application to boosting.""
       Journal of computer and system sciences
       55.1 (1997): 119-139.
       `[link]
       <https://doi.org/10.1006/jcss.1997.1504>`__
    .. [3] Friedman, Jerome H.
       ""Greedy function approximation: a gradient boosting machine.""
       Annals of statistics (2001): 1189-1232.
       `[link]
       <https://statweb.stanford.edu/~jhf/ftp/trebst.pdf>`__
    .. [4] Breiman, Leo. Arcing the edge.
       Technical Report 486, Statistics Department,
       University of California at Berkeley, 1997.
       `[link]
       <https://www.stat.berkeley.edu/~breiman/arcing-the-edge.pdf>`__
    .. [5] Rashmi Korlakai Vinayak, Ran Gilad-Bachrach.
       ""DART: Dropouts meet Multiple Additive Regression Trees.""
       `[link]
       <http://proceedings.mlr.press/v38/korlakaivinayak15.pdf>`__
    
    See Also
    --------
    Source:
        `LightGBM on GitHub
        <https://github.com/Microsoft/LightGBM>`_
    Source:
        `Gradient boosting wikipedia
        <https://en.wikipedia.org/wiki/Gradient_boosting>`_",no url,LightGBM Dropout Additive Regression Trees Classifier,PLGBMDC,Multi-class Classification,no documentation retrieved,NUM,Dropout Additive Regression Trees Classifier,Dropout Additive Regression Trees Classifier
"Keras-based Variable Length Multi Image Fine-Tune Classification.
    
    The module uses Convolutional Neural Networks (CNNs) to employ transfer learning (""fine-tuning"") for
    image variable types. It initializes the CNNs using pretrained weights and proceeds to train and
    fine-tune those weights based on the dataset.
    
    Transfer learning methodologies rely on different elements, the two most
    essential of which are task size and similarity to the pretrained image dataset. Becaise Deep CNN
    (DCNN) features are more generic in early layers and more dataset-specific in later layers, the
    module is capable of only fine-tuning the higher level layers of the network.
    This is driven by the understanding that the prior features of a DCNN contain more generic image
    features that are useful to many tasks, but later layers of the Deep CNN become progressively more
    specific to the exact elements of the classes relevant in the given task.
    
    Multiple-image data introduces a new set of challenges because the images can either be
    ordered or unordered, empty or not empty. CNNs, and Deep Learning in general, can proceed
    independent of the exact ordering on the data. Additionally, common practices of
    padding data before feeding it to neural networks has long been proven to work in various
    applications. DataRobot apply the same methodology when there is no image by adding in black images.
    When images are unordered, these act as ""no information"" data and are used as placeholders to
    randomize the image order while training. In a multiple-image input model, the base model networks
    share the same weights even in fine-tuning, to further ensure independence from ordering.
    
    Parameters
    ----------
    Pretrained model (model_name): select (default=squeezenet)
        Model architecture trained on a pretrained dataset.
    
        ``values: ['squeezenet', 'resnet50', 'xception', 'efficientnet-b0', 'efficientnet-b4']``
    Batch size (batch_size): int (default=32)
        Mini-batch size for training.
    
        ``values: [0, 32]``
    Number of epoch (epoch): int (default=1000)
        Number of epochs for training.
    
        ``values: [0, 1000]``
    Early Stop parameter (earlystop_patience): int (default=5)
        Early stopping window. Number of epochs that stops the model from training when the validation
        score consecutively fails to improve
    
        ``values: [0, 1000]``
    reduce_lr_on_plateau (reduce_lr_on_plateau): selectgrid (default=False)
        When True, reduces the learning rate when a metric has stopped improving.
    reduce_lr_patience (reduce_lr_patience): intgrid (default=3)
        Number of epochs to wait before reducing the learning rate.
    reduce_lr_factor (reduce_lr_factor): float (default=0.2)
        Factor by which the learning rate will be reduced. Specifically `new_lr = lr * factor`.
    
        ``values: [0, 1]``
    weights_initialization (weights_initialization): select (default=pretrained)
        Whether to use a randomly initialized model or pretrained weights as a starting point before
        fine-tuning the model base.
    
        ``values: ['random', 'pretrained']``
    optimizer (optimizer): select (default=rmsprop)
        Name of the optimizer.
    
        ``values: ['rmsprop', 'adadelta', 'adagrad', 'adam', 'momentum', 'sgd', 'adam_tf']``
    use_discriminative_learning_rate (use_discriminative_learning_rate): selectgrid (default=False)
        When True (default is False), uses a different learning rate for each trainable layer. By
        default, DataRobot uses cosine learning rate decay structure (decayed to 0.0 learning rate) to
        decay learning rate by layer. If not all layers are trainable, learning rate decay proceeds by
        layer number. If all the layers are trainable, learning rate decays by convolutional
        blocks/groups specific to the model architecture chosen.
    
        ``values: ['True', 'False']``
    learning rate (learning_rate_init): float (default='auto')
        Initial learning rate. When value is set to auto, we set the initial learning rate is based on
        the chosen optimizer.
    
        ``values: {'float': [0, 1], 'select': ['auto']}``
    Trainable scope (trainable_scope): multi (default='all')
        Number of layers to enable training the weights of the base CNN model in fine-tune modelers.
        Either:
    
        integers: enable the last convolutional layers of the chosen network to be trainable
    
        all: All learnable layers are trainable
    
        chain_thaw: First, fine-tunes any new layers (often only a Softmax layer)
        to the target task until convergence on a validation set. Then, fine-tunes each
        layer individually starting from the first layer in the network. Last, the entire model is
        trained with all layers. Each time the model converges as measured on the validation set, the
        weights are reloaded to the best setting, thereby preventing overfitting to any layer.
    
        ``values: {'int': [0, 100], 'select': ['all', 'chain_thaw']}``
    featurizer_pool (featurizer_pool): select (default=avg)
        Type of summarizer to use to squash the multi-dimensional CNN features
        applied on initial, intermediate, and top convolutional layers of the network.
    
        ``values: ['avg', 'gem', 'max']``
    image_aug_list_id (image_aug_list_id): select (default=None)
        ID of the augmentation list used to control the transformations applied to the images
    
        ``values: [None]``
    loss (loss): select (default=crossentropy)
        The type of loss used for tuning the model:
    
        focal_loss: penalizes hard-to-classify examples more heavily relative to easy-to-classify
        examples.
    
        crossentropy: is the basic log loss and is used by default.
    
        blend_loss: uses a weighted average between crossentropy and focal loss.
    
        ``values: ['crossentropy', 'focal_loss', 'blend_loss']``
    Multi Image Training Type (variable_num_of_images_train_mode): select (default='MULTI_INPUT_CNN')
        Determines model and training type for multi-image data, either:
    
        ``values: ['MULTI_INPUT_CNN', 'SINGLE_INPUT_CNN']``
    
        Multi: A multi input CNN model with shared base architecture (shared weights) which
        trains on all of the available image columns and handles variable length image rows
    
        Single: A single input CNN model which trains on all available training images
        and based on labels, averages the predictions at testing time. It handles variable length image
        rows.
    
    References
    ----------
    .. [1] Jeremy Howard and Sebastian Ruder.
       ""Universal language  model  fine-tuning  for  text  classification"".
       arXiv preprint arXiv:1801.06146.
       `[link]
       <https://arxiv.org/pdf/1801.06146v5.pdf>`__
    .. [2] Pan, S. J., and Yang, Q.
       ""A Survey on Transfer Learning""
       Knowledge and Data Engineering, IEEE Transactionson 22 (10): 1345-1359.
       `[link]
       <https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf>`__
    
    See Also
    --------
    Source:
        `Convolutional Neural Network wikipedia
        <https://en.wikipedia.org/wiki/Convolutional_neural_network>`_",no url,Keras-based Variable Length Multi Image Fine-Tune Classification.,MULTIIMGFTC,Multi-class Classification,no documentation retrieved,NUM,Fine-Tuned Multi-Image Classifier (All Layers),Multi Image Classification using pre-trained deep neural network models
Random Forests based on scikit-learn. These are built shallowly for TreeShap performance,no url,Random Forests based on scikit-learn,SHAPRFC,Multi-class Classification,no documentation retrieved,NUM,ExtraTrees Classifier (Gini),Random Forests based on scikit-learn. These are built shallowly for TreeShap performance
"LightGBM Random Forest Classifier
    
    LightGBM is a gradient boosting framework. It uses a tree-based algorithm and is designed to be
    distributed and efficient, providing the following advantages:
    
    #. Faster training speed and higher efficiency
    #. Lower memory usage
    #. Better accuracy
    #. Support for parallel learning
    #. Handling of large-scale data
    
    LightGBM implements both the gradient boosted trees and random forest algorithms. Random forests are
    an ensemble method where hundreds (or thousands) of individual decision trees are fit to bootstrap
    re-samples of the original dataset, with each tree being allowed to use a random selection of N
    variables, where N is the major configurable parameter of this algorithm.
    
    **Gradient Boosting Machines:**
    
    Gradient Boosting Machines (or Generalized Boosted Models, depending on who you
    ask to explain the acronym 'GBM') are an advanced algorithm for fitting
    extremely accurate predictive models. GBMs have won a number of recent predictive
    modeling competitions and are considered by many data scientists to be the
    most versatile and useful predictive modeling algorithm. GBMs require very
    little preprocessing, elegantly handle missing data, strike a good balance between
    bias and variance, and are typically able to find complicated interaction terms, making them a
    useful ""Swiss army knife"" of predictive models.
    
    GBMs are a generalization of Freund and Schapire's adaboost algorithm (1995) that handles
    arbitrary loss functions. They are very similar in concept to random forests, in that
    they fit individual decision trees to random re-samples of input data, where each
    tree sees a bootstrap sample of the rows of the dataset and N arbitrarily chosen
    columns, where N is a configurable parameter of the model. GBMs differ from random
    forests in a single major aspect: rather than fitting the trees independently, the
    GBM fits each successive tree to the residual errors from all the previous trees
    combined. This is advantageous, as the model focuses each iteration on the examples
    that are most difficult to predict (and therefore most useful to get correct).
    
    Due to their iterative nature, GBMs are almost guaranteed to overfit the training data,
    given enough iterations. Therefore, the 2 critical parameters of the algorithm are the
    learning rate (or how fast the model fits the data) and the number of trees the model
    is allowed to fit. It is critical to tune one of these 2 parameters, and
    when done correctly, GBMs are capable of finding the exact point in the training data
    where overfitting begins, and halt one iteration prior to that point. In this manner GBMs
    are usually capable of squeezing every last bit of information out of the training
    set and producing a model with the highest possible accuracy without overfitting.
    
    Parameters
    ----------
    learning_rate (lr): floatgrid (default='0.1')
        Not used in random forest mode.
        ``values: [1e-7, 1e2]``
    n_estimators (n): intgrid (default='10')
        Number of boosting stages to perform.
        Gradient boosting is fairly robust to overfitting so a large number usually results in better
        performance.
        ``values: [1, 1e6]``
    num_leaves (nl): intgrid (default='31')
        Number of leaves in one tree.
        ``values: [2, 1e4]``
    max_depth (md): intgrid (default='none')
        Maximum depth of the individual regression estimators.
        The maximum depth limits the number of nodes in the tree. Tune this parameter
        for best performance; the best value depends on the interaction
        of the input variables. Deeper the tree the more variable interactions
        the model can capture. Tree still grow by leaf-wise.
        <0 means no limit
        ``values: ['none', [1, 1e4]]``
    max_bin (mb): intgrid (default='255')
        Max number of bin that feature values will bucket in.
        Small bin may reduce training accuracy but may increase general power (deal with overfit).
        LightGBM will auto compress memory according max_bin. For example, LightGBM will use
        uint8_t for feature value if max_bin=255.
        ``values: [3, 1e4]``
    subsample_for_bin (ssfb): int (default=50000)
        Number of samples for constructing bins.
        ``values: [1, 1e6]``
    min_split_gain (msg): floatgrid (default='0')
        Minimum loss reduction required to make a further partition on a leaf node of the tree.
        ``values: [0, 100]``
    min_child_weight (mcw): intgrid (default='5')
        Minimum sum of instance weight(hessian) needed in a child(leaf).
        ``values: [0, 1e2]``
    min_child_samples (mcs): int (default='10')
        Minimum number of data need in a child(leaf).
        ``values: [0, 1e3]``
    subsample (ss): floatgrid (default='1.0')
        Subsample ratio of the training instance.
        ``values: [0.01, 1]``
    subsample_freq (ssf): intgrid (default='1')
        Frequency of subsample 'none' means it is not enabled.
        ``values: ['none', [1, 1e3]]``
    colsample_bytree (cbt): floatgrid (default='1.0')
        Subsample ratio of columns when constructing each tree.
        By default, the value of ``colsample_bytree`` for LightGBM classes is 1.0. However, based on the
        training data, DataRobot may choose a different initial value for this parameter.
        ``values: [0, 1]``
    reg_alpha (ra): floatgrid (default='0')
        L1 regularization term on weights.
        ``values: [0, 1e6]``
    reg_lambda (rl): floatgrid (default='0')
        L2 regularization term on weights.
        ``values: [0, 1e6]``
    sigmoid (s): floatgrid (default='1.0')
        Parameter for sigmoid function. Used in binary classification and LambdaRank.
        ``values: [1e-06, 1e03]``
    is_unbalance (iu): select (default=False)
        Set to true if training data are unbalanced. Used in binary classification.
        ``values: [True, False]``
    
    References
    ----------
    .. [1] Breiman, Leo.
       ""Random forests.""
       Machine learning 45.1 (2001): 5-32.
       `[link]
       <http://machinelearning202.pbworks.com/w/file/fetch/60606349/breiman_randomforests.pdf>`__
    .. [2] Liaw, Andy, and Matthew Wiener.
       ""Classification and regression by randomForest.""
       R news 2.3 (2002): 18-22.
       `[link]
       <https://cogns.northwestern.edu/cbmg/LiawAndWiener2002.pdf>`__
    .. [3] Ho, Tin Kam.
       ""Random decision forests."" Document Analysis and Recognition, 1995.,
       Proceedings of the Third International Conference on. Vol. 1. IEEE, 1995.
       `[link]
       <https://doi.org/10.1109/ICDAR.1995.598994>`__
    .. [4] Geurts, Pierre, Damien Ernst, and Louis Wehenkel.
       ""Extremely randomized trees."" Machine learning 63.1 (2006): 3-42.
       `[link]
       <https://doi.org/10.1007/s10994-006-6226-1>`__
    
    See Also
    --------
    Source:
        `sklearn.ensemble.RandomForestClassifier
        <http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>`_
    
    Source:
        `sklearn ensemble methods: Random Forests
        <http://scikit-learn.org/stable/modules/ensemble.html>`_
    
    Source:
        `Random Forests wikipedia
        <https://en.wikipedia.org/wiki/Random_forest>`_
    
    Source:
        `sklearn.ensemble.ExtraTreesClassifier
        <http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html>`_",no url,LightGBM Random Forest Classifier,PLGBMRFC,Multi-class Classification,no documentation retrieved,NUM,LightGBM Random Forest Classifier,Light GBM Random Forest Classifier
Keras Convolutional Neural Network Multi-Class Classifier for Text,no url,Keras Convolutional Neural Network Multi-Class Classifier for Text,KERAS_CNN_TEXT_MULTIC,Multi-class Classification,no documentation retrieved,NUM,Keras Text Convolutional Neural Network Multi-class Classifier,Keras Convolutional Neural Network Multi-Class Classifier for Text
Light GBM Classifier with Early Stopping with GBDT using Unsupervised Learning features.,https://github.com/Microsoft/LightGBM/blob/master/docs/README.md,Light GBM Classifier with Early Stopping with GBDT using Unsupervised Learning features,UESLGBMTC,Multi-class Classification,no documentation retrieved,NUM,Light Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features,Light GBM Classifier with Early Stopping with GBDT using Unsupervised Learning features.
Gradient Boosting Classifier (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Classifier (xgboost),PXGBC2,Multi-class Classification,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Classifier,Gradient Boosting Classifier (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Gradient Boosting Classifier (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Classifier (xgboost),XGBC2,Multi-class Classification,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Classifier,Gradient Boosting Classifier (xgboost). Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
"Keras-based Variable Length Multi Image Fine-Tune Regression.
    
    The module uses Convolutional Neural Networks (CNNs) to employ transfer learning (""fine-tuning"") for
    image variable types. It initializes the CNNs using pretrained weights and proceeds to train and
    fine-tune those weights based on the dataset.
    
    Transfer learning methodologies rely on different elements, the two most
    essential of which are task size and similarity to the pretrained image dataset. Becaise Deep CNN
    (DCNN) features are more generic in early layers and more dataset-specific in later layers, the
    module is capable of only fine-tuning the higher level layers of the network.
    This is driven by the understanding that the prior features of a DCNN contain more generic image
    features that are useful to many tasks, but later layers of the Deep CNN become progressively more
    specific to the exact elements of the classes relevant in the given task.
    
    Multiple-image data introduces a new set of challenges because the images can either be
    ordered or unordered, empty or not empty. CNNs, and Deep Learning in general, can proceed
    independent of the exact ordering on the data. Additionally, common practices of
    padding data before feeding it to neural networks has long been proven to work in various
    applications. DataRobot apply the same methodology when there is no image by adding in black images.
    When images are unordered, these act as ""no information"" data and are used as placeholders to
    randomize the image order while training. In a multiple-image input model, the base model networks
    share the same weights even in fine-tuning, to further ensure independence from ordering.
    
    Parameters
    ----------
    Pretrained model (model_name): select (default=squeezenet)
        Model architecture trained on a pretrained dataset.
    
        ``values: ['squeezenet', 'resnet50', 'xception', 'efficientnet-b0', 'efficientnet-b4']``
    Batch size (batch_size): int (default=32)
        Mini-batch size for training.
    
        ``values: [0, 32]``
    Number of epoch (epoch): int (default=1000)
        Number of epochs for training.
    
        ``values: [0, 1000]``
    Early Stop parameter (earlystop_patience): int (default=5)
        Early stopping window. Number of epochs that stops the model from training when the validation
        score consecutively fails to improve
    
        ``values: [0, 1000]``
    reduce_lr_on_plateau (reduce_lr_on_plateau): selectgrid (default=False)
        When True, reduces the learning rate when a metric has stopped improving.
    reduce_lr_patience (reduce_lr_patience): intgrid (default=3)
        Number of epochs to wait before reducing the learning rate.
    reduce_lr_factor (reduce_lr_factor): float (default=0.2)
        Factor by which the learning rate will be reduced. Specifically `new_lr = lr * factor`.
    
        ``values: [0, 1]``
    weights_initialization (weights_initialization): select (default=pretrained)
        Whether to use a randomly initialized model or pretrained weights as a starting point before
        fine-tuning the model base.
    
        ``values: ['random', 'pretrained']``
    optimizer (optimizer): select (default=rmsprop)
        Name of the optimizer.
    
        ``values: ['rmsprop', 'adadelta', 'adagrad', 'adam', 'momentum', 'sgd', 'adam_tf']``
    use_discriminative_learning_rate (use_discriminative_learning_rate): selectgrid (default=False)
        When True (default is False), uses a different learning rate for each trainable layer. By
        default, DataRobot uses cosine learning rate decay structure (decayed to 0.0 learning rate) to
        decay learning rate by layer. If not all layers are trainable, learning rate decay proceeds by
        layer number. If all the layers are trainable, learning rate decays by convolutional
        blocks/groups specific to the model architecture chosen.
    
        ``values: ['True', 'False']``
    learning rate (learning_rate_init): float (default='auto')
        Initial learning rate. When value is set to auto, we set the initial learning rate is based on
        the chosen optimizer.
    
        ``values: {'float': [0, 1], 'select': ['auto']}``
    Trainable scope (trainable_scope): multi (default='all')
        Number of layers to enable training the weights of the base CNN model in fine-tune modelers.
        Either:
    
        integers: enable the last convolutional layers of the chosen network to be trainable
    
        all: All learnable layers are trainable
    
        chain_thaw: First, fine-tunes any new layers (often only a Softmax layer)
        to the target task until convergence on a validation set. Then, fine-tunes each
        layer individually starting from the first layer in the network. Last, the entire model is
        trained with all layers. Each time the model converges as measured on the validation set, the
        weights are reloaded to the best setting, thereby preventing overfitting to any layer.
    
        ``values: {'int': [0, 100], 'select': ['all', 'chain_thaw']}``
    featurizer_pool (featurizer_pool): select (default=avg)
        Type of summarizer to use to squash the multi-dimensional CNN features
        applied on initial, intermediate, and top convolutional layers of the network.
    
        ``values: ['avg', 'gem', 'max']``
    image_aug_list_id (image_aug_list_id): select (default=None)
        ID of the augmentation list used to control the transformations applied to the images
    
        ``values: [None]``
    loss (loss): select (default=mean_squared_error)
        The type of loss used for tuning the model.
    
        ``values: ['gaussian', 'poisson', 'tweedie', 'mean_absolute_error', 'mean_squared_error',
        'root_mean_squared_error']``
    Multi Image Training Type (variable_num_of_images_train_mode): select (default='MULTI_INPUT_CNN')
        Determines model and training type for multi-image data, either:
    
        ``values: ['MULTI_INPUT_CNN', 'SINGLE_INPUT_CNN']``
    
        Multi: A multi input CNN model with shared base architecture (shared weights) which
        trains on all of the available image columns and handles variable length image rows
    
        Single: A single input CNN model which trains on all available training images
        and based on labels, averages the predictions at testing time. It handles variable length image
        rows.
    
    References
    ----------
    .. [1] Jeremy Howard and Sebastian Ruder.
       ""Universal language  model  fine-tuning  for  text  classification"".
       arXiv preprint arXiv:1801.06146.
       `[link]
       <https://arxiv.org/pdf/1801.06146v5.pdf>`__
    .. [2] Pan, S. J., and Yang, Q.
       ""A Survey on Transfer Learning""
       Knowledge and Data Engineering, IEEE Transactionson 22 (10): 1345-1359.
       `[link]
       <https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf>`__
    
    See Also
    --------
    Source:
        `Convolutional Neural Network wikipedia
        <https://en.wikipedia.org/wiki/Convolutional_neural_network>`_",no url,Keras-based Variable Length Multi Image Fine-Tune Regression.,MULTIIMGFTR,Multi-class Classification,no documentation retrieved,NUM,Fine-Tuned Multi-Image Regressor (All Layers),Multi Image Regression using pre-trained deep neural network models
"Elasticnet Classifier. ElasticNet is a linear model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. Based on lightning CDClassifier",http://contrib.scikit-learn.org/lightning/,Elasticnet Classifier,LENETCD,Multi-class Classification,"




lightning — lightning 0.6.3.dev0 documentation

































          lightning




Introduction
References
Examples

Site 

Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


Introduction
Primal coordinate descent
Dual coordinate ascent
FISTA
Stochastic gradient method (SGD)
AdaGrad
Stochastic averaged gradient (SAG and SAGA)
Stochastic variance-reduced gradient (SVRG)
PRank
Examples
SGD: Convex Loss Functions
Signal recovery by 1D total variation
Robust regression
Trace norm
SAGA: Weighted samples
Classification of text documents
Sensitivity to hyper-parameters in SVRG
Sparse non-linear classification
L2 solver comparison


Classification
lightning.classification.AdaGradClassifier
lightning.classification.CDClassifier
lightning.classification.FistaClassifier
lightning.classification.KernelSVC
lightning.classification.LinearSVC
lightning.classification.SDCAClassifier
lightning.classification.SAGClassifier
lightning.classification.SAGAClassifier
lightning.classification.SGDClassifier
lightning.classification.SVRGClassifier


Regression
lightning.regression.AdaGradRegressor
lightning.regression.CDRegressor
lightning.regression.FistaRegressor
lightning.regression.LinearSVR
lightning.regression.SAGRegressor
lightning.regression.SAGARegressor
lightning.regression.SDCARegressor
lightning.regression.SGDRegressor
lightning.regression.SVRGRegressor


Ranking
lightning.ranking.PRank
lightning.ranking.KernelPRank


























lightning¶
lightning is a library for large-scale linear classification, regression and
ranking in Python.
Highlights:

follows the scikit-learn API conventions
supports natively both dense and sparse data representations
computationally demanding parts implemented in Cython

Solvers supported:

primal coordinate descent
dual coordinate descent (SDCA, Prox-SDCA)
SGD, AdaGrad, SAG, SAGA, SVRG
FISTA


Example¶
Example that shows how to learn a multiclass classifier with group lasso
penalty on the News20 dataset (c.f., Blondel et al. 2013):
from sklearn.datasets import fetch_20newsgroups_vectorized
from lightning.classification import CDClassifier

# Load News20 dataset from scikit-learn.
bunch = fetch_20newsgroups_vectorized(subset=""all"")
X = bunch.data
y = bunch.target

# Set classifier options.
clf = CDClassifier(penalty=""l1/l2"",
                   loss=""squared_hinge"",
                   multiclass=True,
                   max_iter=20,
                   alpha=1e-4,
                   C=1.0 / X.shape[0],
                   tol=1e-3)

# Train the model.
clf.fit(X, y)

# Accuracy
print(clf.score(X, y))

# Percentage of selected features
print(clf.n_nonzero(percentage=True))




Dependencies¶
lightning requires Python >= 3.7, setuptools, Joblib, Numpy >= 1.12, SciPy >= 0.19 and
scikit-learn >= 0.19. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need pytest.


Installation¶
Precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:
pip install sklearn-contrib-lightning


or conda:
conda install -c conda-forge sklearn-contrib-lightning


The development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type:
git clone https://github.com/scikit-learn-contrib/lightning.git
cd lightning
python setup.py install




Documentation¶
http://contrib.scikit-learn.org/lightning/


On GitHub¶
https://github.com/scikit-learn-contrib/lightning


Citing¶
If you use this software, please cite it. Here is a BibTex snippet that you can use:
@misc{lightning_2016,
  author       = {Blondel, Mathieu and
                  Pedregosa, Fabian},
  title        = {{Lightning: large-scale linear classification,
                 regression and ranking in Python}},
  year         = 2016,
  doi          = {10.5281/zenodo.200504},
  url          = {https://doi.org/10.5281/zenodo.200504}
}


Other citing formats are available in its Zenodo entry.


Authors¶

Mathieu Blondel
Manoj Kumar
Arnaud Rachez
Fabian Pedregosa
Nikita Titov











Back to top


        © Copyright 2022, Mathieu Blondel.
      Created using Sphinx 4.4.0.




",NUM,Elastic-Net Classifier (L1 / Binomial Deviance),"Elasticnet Classifier. ElasticNet is a linear model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. Based on lightning CDClassifier"
Gradient Boosting Classifier (scikit-learn) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier,Gradient Boosting Classifier (scikit-learn) with Early-Stopping,ESGBC,Multi-class Classification,"













GradientBoostingClassifier — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.ensemble
GradientBoos...









GradientBoostingClassifier#


class sklearn.ensemble.GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)[source]#
Gradient Boosting for classification.
This algorithm builds an additive model in a forward stage-wise fashion; it
allows for the optimization of arbitrary differentiable loss functions. In
each stage n_classes_ regression trees are fit on the negative gradient
of the loss function, e.g. binary or multiclass log loss. Binary
classification is a special case where only a single regression tree is
induced.
HistGradientBoostingClassifier is a much faster variant
of this algorithm for intermediate and large datasets (n_samples >= 10_000) and
supports monotonic constraints.
Read more in the User Guide.

Parameters:

loss{‘log_loss’, ‘exponential’}, default=’log_loss’The loss function to be optimized. ‘log_loss’ refers to binomial and
multinomial deviance, the same as used in logistic regression.
It is a good choice for classification with probabilistic outputs.
For loss ‘exponential’, gradient boosting recovers the AdaBoost algorithm.

learning_ratefloat, default=0.1Learning rate shrinks the contribution of each tree by learning_rate.
There is a trade-off between learning_rate and n_estimators.
Values must be in the range [0.0, inf).

n_estimatorsint, default=100The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.
Values must be in the range [1, inf).

subsamplefloat, default=1.0The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. subsample interacts with the parameter n_estimators.
Choosing subsample < 1.0 leads to a reduction of variance
and an increase in bias.
Values must be in the range (0.0, 1.0].

criterion{‘friedman_mse’, ‘squared_error’}, default=’friedman_mse’The function to measure the quality of a split. Supported criteria are
‘friedman_mse’ for the mean squared error with improvement score by
Friedman, ‘squared_error’ for mean squared error. The default value of
‘friedman_mse’ is generally the best as it can provide a better
approximation in some cases.

Added in version 0.18.


min_samples_splitint or float, default=2The minimum number of samples required to split an internal node:

If int, values must be in the range [2, inf).
If float, values must be in the range (0.0, 1.0] and min_samples_split
will be ceil(min_samples_split * n_samples).


Changed in version 0.18: Added float values for fractions.


min_samples_leafint or float, default=1The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least min_samples_leaf training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.

If int, values must be in the range [1, inf).
If float, values must be in the range (0.0, 1.0) and min_samples_leaf
will be ceil(min_samples_leaf * n_samples).


Changed in version 0.18: Added float values for fractions.


min_weight_fraction_leaffloat, default=0.0The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.
Values must be in the range [0.0, 0.5].

max_depthint or None, default=3Maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
If int, values must be in the range [1, inf).

min_impurity_decreasefloat, default=0.0A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.
Values must be in the range [0.0, inf).
The weighted impurity decrease equation is the following:
N_t / N * (impurity - N_t_R / N_t * right_impurity
                    - N_t_L / N_t * left_impurity)


where N is the total number of samples, N_t is the number of
samples at the current node, N_t_L is the number of samples in the
left child, and N_t_R is the number of samples in the right child.
N, N_t, N_t_R and N_t_L all refer to the weighted sum,
if sample_weight is passed.

Added in version 0.19.


initestimator or ‘zero’, default=NoneAn estimator object that is used to compute the initial predictions.
init has to provide fit and predict_proba. If
‘zero’, the initial raw predictions are set to zero. By default, a
DummyEstimator predicting the classes priors is used.

random_stateint, RandomState instance or None, default=NoneControls the random seed given to each Tree estimator at each
boosting iteration.
In addition, it controls the random permutation of the features at
each split (see Notes for more details).
It also controls the random splitting of the training data to obtain a
validation set if n_iter_no_change is not None.
Pass an int for reproducible output across multiple function calls.
See Glossary.

max_features{‘sqrt’, ‘log2’}, int or float, default=NoneThe number of features to consider when looking for the best split:

If int, values must be in the range [1, inf).
If float, values must be in the range (0.0, 1.0] and the features
considered at each split will be max(1, int(max_features * n_features_in_)).
If ‘sqrt’, then max_features=sqrt(n_features).
If ‘log2’, then max_features=log2(n_features).
If None, then max_features=n_features.

Choosing max_features < n_features leads to a reduction of variance
and an increase in bias.
Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than max_features features.

verboseint, default=0Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.
Values must be in the range [0, inf).

max_leaf_nodesint, default=NoneGrow trees with max_leaf_nodes in best-first fashion.
Best nodes are defined as relative reduction in impurity.
Values must be in the range [2, inf).
If None, then unlimited number of leaf nodes.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See the Glossary.

validation_fractionfloat, default=0.1The proportion of training data to set aside as validation set for
early stopping. Values must be in the range (0.0, 1.0).
Only used if n_iter_no_change is set to an integer.

Added in version 0.20.


n_iter_no_changeint, default=Nonen_iter_no_change is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside validation_fraction size of the training
data as validation and terminate training when validation score is not
improving in all of the previous n_iter_no_change numbers of
iterations. The split is stratified.
Values must be in the range [1, inf).
See
Early stopping in Gradient Boosting.

Added in version 0.20.


tolfloat, default=1e-4Tolerance for the early stopping. When the loss is not improving
by at least tol for n_iter_no_change iterations (if set to a
number), the training stops.
Values must be in the range [0.0, inf).

Added in version 0.20.


ccp_alphanon-negative float, default=0.0Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
ccp_alpha will be chosen. By default, no pruning is performed.
Values must be in the range [0.0, inf).
See Minimal Cost-Complexity Pruning for details.

Added in version 0.22.




Attributes:

n_estimators_intThe number of estimators as selected by early stopping (if
n_iter_no_change is specified). Otherwise it is set to
n_estimators.

Added in version 0.20.


n_trees_per_iteration_intThe number of trees that are built at each iteration. For binary classifiers,
this is always 1.

Added in version 1.4.0.


feature_importances_ndarray of shape (n_features,)The impurity-based feature importances.

oob_improvement_ndarray of shape (n_estimators,)The improvement in loss on the out-of-bag samples
relative to the previous iteration.
oob_improvement_[0] is the improvement in
loss of the first stage over the init estimator.
Only available if subsample < 1.0.

oob_scores_ndarray of shape (n_estimators,)The full history of the loss values on the out-of-bag
samples. Only available if subsample < 1.0.

Added in version 1.3.


oob_score_floatThe last value of the loss on the out-of-bag samples. It is
the same as oob_scores_[-1]. Only available if subsample < 1.0.

Added in version 1.3.


train_score_ndarray of shape (n_estimators,)The i-th score train_score_[i] is the loss of the
model at iteration i on the in-bag sample.
If subsample == 1 this is the loss on the training data.

init_estimatorThe estimator that provides the initial predictions. Set via the init
argument.

estimators_ndarray of DecisionTreeRegressor of             shape (n_estimators, n_trees_per_iteration_)The collection of fitted sub-estimators. n_trees_per_iteration_ is 1 for
binary classification, otherwise n_classes.

classes_ndarray of shape (n_classes,)The classes labels.

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


n_classes_intThe number of classes.

max_features_intThe inferred value of max_features.





See also

HistGradientBoostingClassifierHistogram-based Gradient Boosting Classification Tree.

sklearn.tree.DecisionTreeClassifierA decision tree classifier.

RandomForestClassifierA meta-estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.

AdaBoostClassifierA meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.



Notes
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
max_features=n_features, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
random_state has to be fixed.
References
J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.

Friedman, Stochastic Gradient Boosting, 1999

T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.
Examples
The following example shows how to fit a gradient boosting classifier with
100 decision stumps as weak learners.
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier


>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]


>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.913...




apply(X)[source]#
Apply trees in the ensemble to X, return leaf indices.

Added in version 0.17.


Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, its dtype will be converted to
dtype=np.float32. If a sparse matrix is provided, it will
be converted to a sparse csr_matrix.



Returns:

X_leavesarray-like of shape (n_samples, n_estimators, n_classes)For each datapoint x in X and for each tree in the ensemble,
return the index of the leaf x ends up in each estimator.
In the case of binary classification n_classes is 1.







decision_function(X)[source]#
Compute the decision function of X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

scorendarray of shape (n_samples, n_classes) or (n_samples,)The decision function of the input samples, which corresponds to
the raw values predicted from the trees of the ensemble . The
order of the classes corresponds to that in the attribute
classes_. Regression and binary classification produce an
array of shape (n_samples,).







property feature_importances_#
The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.
Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
sklearn.inspection.permutation_importance as an alternative.

Returns:

feature_importances_ndarray of shape (n_features,)The values of this array sum to 1, unless all trees are single node
trees consisting of only the root node, in which case it will be an
array of zeros.







fit(X, y, sample_weight=None, monitor=None)[source]#
Fit the gradient boosting model.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.

yarray-like of shape (n_samples,)Target values (strings or integers in classification, real numbers
in regression)
For classification, labels must correspond to classes.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. In the case of
classification, splits are also ignored if they would result in any
single class carrying a negative weight in either child node.

monitorcallable, default=NoneThe monitor is called after each iteration with the current
iteration, a reference to the estimator and the local variables of
_fit_stages as keyword arguments callable(i, self,
locals()). If the callable returns True the fitting procedure
is stopped. The monitor can be used for various things such as
computing held-out estimates, early stopping, model introspect, and
snapshotting.



Returns:

selfobjectFitted estimator.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict class for X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

yndarray of shape (n_samples,)The predicted values.







predict_log_proba(X)[source]#
Predict class log-probabilities for X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

pndarray of shape (n_samples, n_classes)The class log-probabilities of the input samples. The order of the
classes corresponds to that in the attribute classes_.



Raises:

AttributeErrorIf the loss does not support probabilities.







predict_proba(X)[source]#
Predict class probabilities for X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

pndarray of shape (n_samples, n_classes)The class probabilities of the input samples. The order of the
classes corresponds to that in the attribute classes_.



Raises:

AttributeErrorIf the loss does not support probabilities.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, monitor: bool | None | str = '$UNCHANGED$', sample_weight: bool | None | str = '$UNCHANGED$') → GradientBoostingClassifier[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

monitorstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for monitor parameter in fit.

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → GradientBoostingClassifier[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







staged_decision_function(X)[source]#
Compute decision function of X for each iteration.
This method allows monitoring (i.e. determine error on testing set)
after each stage.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Yields:

scoregenerator of ndarray of shape (n_samples, k)The decision function of the input samples, which corresponds to
the raw values predicted from the trees of the ensemble . The
classes corresponds to that in the attribute classes_.
Regression and binary classification are special cases with
k == 1, otherwise k==n_classes.







staged_predict(X)[source]#
Predict class at each stage for X.
This method allows monitoring (i.e. determine error on testing set)
after each stage.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Yields:

ygenerator of ndarray of shape (n_samples,)The predicted value of the input samples.







staged_predict_proba(X)[source]#
Predict class probabilities at each stage for X.
This method allows monitoring (i.e. determine error on testing set)
after each stage.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Yields:

ygenerator of ndarray of shape (n_samples,)The predicted value of the input samples.







Gallery examples#

Feature transformations with ensembles of trees
Feature transformations with ensembles of trees

Gradient Boosting Out-of-Bag estimates
Gradient Boosting Out-of-Bag estimates

Gradient Boosting regularization
Gradient Boosting regularization

Feature discretization
Feature discretization










previous
ExtraTreesRegressor




next
GradientBoostingRegressor










 On this page
  


GradientBoostingClassifier
apply
decision_function
feature_importances_
fit
get_metadata_routing
get_params
predict
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_score_request
staged_decision_function
staged_predict
staged_predict_proba


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Gradient Boosted Trees Classifier with Early Stopping,Gradient Boosting Classifier (scikit-learn) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
"Keras-based Image Fine-Tune Classification.
    
    The module uses Convolutional Neural Networks (CNNs) to employ transfer learning (""fine-tuning"") for
    image variable types. It initializes the CNNs using pretrained weights and proceeds to train and
    fine-tune those weights based on the dataset.
    
    Transfer learning methodologies rely on different elements, the two most
    essential of which are task size and similarity to the pretrained image dataset. Becaise Deep CNN
    (DCNN) features are more generic in early layers and more dataset-specific in later layers, the
    module is capable of only fine-tuning the higher level layers of the network.
    This is driven by the understanding that the prior features of a DCNN contain more generic image
    features that are useful to many tasks, but later layers of the Deep CNN become progressively more
    specific to the exact elements of the classes relevant in the given task.
    
    Parameters
    ----------
    Pretrained model (model_name): select (default=squeezenet)
        Model architecture trained on a pretrained dataset.
    
        ``values: ['squeezenet', 'resnet50', 'xception', 'efficientnet-b0', 'efficientnet-b4']``
    Batch size (batch_size): int (default=32)
        Mini-batch size for training.
    
        ``values: [0, 32]``
    Number of epoch (epoch): int (default=1000)
        Number of epochs for training.
    
        ``values: [0, 1000]``
    Early Stop parameter (earlystop_patience): int (default=5)
        Early stopping window. Number of epochs that stops the model from training when the validation
        score consecutively fails to improve
    
        ``values: [0, 1000]``
    reduce_lr_on_plateau (reduce_lr_on_plateau): selectgrid (default=False)
        When True, reduces the learning rate when a metric has stopped improving.
    reduce_lr_patience (reduce_lr_patience): intgrid (default=3)
        Number of epochs to wait before reducing the learning rate.
    reduce_lr_factor (reduce_lr_factor): float (default=0.2)
        Factor by which the learning rate will be reduced. Specifically `new_lr = lr * factor`.
    
        ``values: [0, 1]``
    weights_initialization (weights_initialization): select (default=pretrained)
        Whether to use a randomly initialized model or pretrained weights as a starting point before
        fine-tuning the model base.
    
        ``values: ['random', 'pretrained']``
    optimizer (optimizer): select (default=rmsprop)
        Name of the optimizer.
    
        ``values: ['rmsprop', 'adadelta', 'adagrad', 'adam', 'momentum', 'sgd', 'adam_tf']``
    use_discriminative_learning_rate (use_discriminative_learning_rate): selectgrid (default=False)
        When True (default is False), uses a different learning rate for each trainable layer. By
        default, DataRobot uses cosine learning rate decay structure (decayed to 0.0 learning rate) to
        decay learning rate by layer. If not all layers are trainable, learning rate decay proceeds by
        layer number. If all the layers are trainable, learning rate decays by convolutional
        blocks/groups specific to the model architecture chosen.
    
        ``values: ['True', 'False']``
    learning rate (learning_rate_init): float (default='auto')
        Initial learning rate. When value is set to auto, we set the initial learning rate is based on
        the chosen optimizer.
    
        ``values: {'float': [0, 1], 'select': ['auto']}``
    Trainable scope (trainable_scope): multi (default='all')
        Number of layers to enable training the weights of the base CNN model in fine-tune modelers.
        Either:
    
        integers: enable the last convolutional layers of the chosen network to be trainable
    
        all: All learnable layers are trainable
    
        chain_thaw: First, fine-tunes any new layers (often only a Softmax layer)
        to the target task until convergence on a validation set. Then, fine-tunes each
        layer individually starting from the first layer in the network. Last, the entire model is
        trained with all layers. Each time the model converges as measured on the validation set, the
        weights are reloaded to the best setting, thereby preventing overfitting to any layer.
    
        ``values: {'int': [0, 100], 'select': ['all', 'chain_thaw']}``
    featurizer_pool (featurizer_pool): select (default=avg)
        Type of summarizer to use to squash the multi-dimensional CNN features
        applied on initial, intermediate, and top convolutional layers of the network.
    
        ``values: ['avg', 'gem', 'max']``
    image_aug_list_id (image_aug_list_id): select (default=None)
        ID of the augmentation list used to control the transformations applied to the images
    
        ``values: [None]``
    loss (loss): select (default=crossentropy)
        The type of loss used for tuning the model:
    
        focal_loss: penalizes hard-to-classify examples more heavily relative to easy-to-classify
        examples.
    
        crossentropy: is the basic log loss and is used by default.
    
        blend_loss: uses a weighted average between crossentropy and focal loss.
    
        ``values: ['crossentropy', 'focal_loss', 'blend_loss']``
    
    References
    ----------
    .. [1] Jeremy Howard and Sebastian Ruder.
       ""Universal language  model  fine-tuning  for  text  classification"".
       arXiv preprint arXiv:1801.06146.
       `[link]
       <https://arxiv.org/pdf/1801.06146v5.pdf>`__
    .. [2] Pan, S. J., and Yang, Q.
       ""A Survey on Transfer Learning""
       Knowledge and Data Engineering, IEEE Transactionson 22 (10): 1345-1359.
       `[link]
       <https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf>`__
    
    See Also
    --------
    Source:
        `Convolutional Neural Network wikipedia
        <https://en.wikipedia.org/wiki/Convolutional_neural_network>`_",no url,Keras-based Image Fine-Tune Classification.,IMGFTC,Multi-class Classification,no documentation retrieved,NUM,Fine-Tuned Image Classifier (All Layers),Image Classfication using pre-trained deep neural network models.
"LightGBM Trees Classifier with Grid Search and Early Stopping support
    
    LightGBM is a gradient boosting framework. It uses a tree-based algorithm and is designed to be
    distributed and efficient, providing the following advantages:
    
    #. Faster training speed and higher efficiency
    #. Lower memory usage
    #. Better accuracy
    #. Support for parallel learning
    #. Handling of large-scale data
    
    **Gradient Boosting Machines:**
    
    Gradient Boosting Machines (or Generalized Boosted Models, depending on who you
    ask to explain the acronym 'GBM') are an advanced algorithm for fitting
    extremely accurate predictive models. GBMs have won a number of recent predictive
    modeling competitions and are considered by many data scientists to be the
    most versatile and useful predictive modeling algorithm. GBMs require very
    little preprocessing, elegantly handle missing data, strike a good balance between
    bias and variance, and are typically able to find complicated interaction terms, making them a
    useful ""Swiss army knife"" of predictive models.
    
    GBMs are a generalization of Freund and Schapire's adaboost algorithm (1995) that handles
    arbitrary loss functions. They are very similar in concept to random forests, in that
    they fit individual decision trees to random re-samples of input data, where each
    tree sees a bootstrap sample of the rows of the dataset and N arbitrarily chosen
    columns, where N is a configurable parameter of the model. GBMs differ from random
    forests in a single major aspect: rather than fitting the trees independently, the
    GBM fits each successive tree to the residual errors from all the previous trees
    combined. This is advantageous, as the model focuses each iteration on the examples
    that are most difficult to predict (and therefore most useful to get correct).
    
    Due to their iterative nature, GBMs are almost guaranteed to overfit the training data,
    given enough iterations. Therefore, the 2 critical parameters of the algorithm are the
    learning rate (or how fast the model fits the data) and the number of trees the model
    is allowed to fit. It is critical to tune one of these 2 parameters, and
    when done correctly, GBMs are capable of finding the exact point in the training data
    where overfitting begins, and halt one iteration prior to that point. In this manner GBMs
    are usually capable of squeezing every last bit of information out of the training
    set and producing a model with the highest possible accuracy without overfitting.
    
    **Early Stopping Support:**
    
    Early stopping is a method for determining the number of trees to use
    for a boosted trees model. The training data is split into a training set and a test set, and at
    each iteration the model is scored on the test set. If test set performance decreases for 200
    iterations (tunable in Advanced Tuning), the training procedure stops and the model returns
    the fit from the best tree seen so far. The approach saves time by not continuing past the point
    where it is clear that the model is overfitting and further trees will not result in more accuracy.
    
    
    Note that the early stopping test set uses a 90/10 train/test split *within* the training data
    for a given model. For example, a 64% model on the Leaderboard will internally use 57.6% of the
    data for training, and 6.4% of the data for early stopping. A 100% model on the Leaderboard will
    internally use 90% of the data for training and 10% of the data for early stopping.
    
    Since the early stopping test set was used for early stopping, it cannot be used for training.
    
    This limitation also applies to grid search: within the grid search train/test split, the model will
    use a 90/10 train/test split for early stopping.
    
    **Grid Search Support:**
    
    Grid search is supported in this task. During training, grid search is run to estimate the optimal
    model parameter values that yield the best performance (evaluated by the configured loss function
    ). The grid search runs on a 70/30 train/test split within the training data; the estimated score
    uses 30% of the training data split. After the grid search completes and the best
    tuning parameters are found, the final model is retrained on 100% of training data. Validation
    scores of the final model are different from the validation scores of the grid search.
    
    Grid search is run on the task parameter with one of the following types:
    'intgrid', 'floatgrid', 'listgrid(int)', 'listgrid(float)', 'selectgrid', or 'multi'. Refer to the
    **Parameters** section for details of task parameter definitions.
    
    For each grid search parameter, the search space is defined by the parameter values. Refer to the
    **Parameters** section for details of task parameter definitions.
    
    Parameters
    ----------
    learning_rate (lr): floatgrid (default='0.1')
        Shrink the contribution of each tree by `learning_rate`.
        There is a trade-off between learning_rate (lr) and n_estimators(n).
        In dart, it also affects normalization
        ``values: [1e-7, 1e2]``
    n_estimators (n): intgrid (default='10')
        Number of boosting stages to perform.
        Gradient boosting is fairly robust to overfitting so a large number usually results in better
        performance.
        ``values: [1, 1e6]``
    num_leaves (nl): intgrid (default='31')
        Number of leaves in one tree.
        ``values: [2, 1e4]``
    max_depth (md): intgrid (default='none')
        Maximum depth of the individual regression estimators.
        The maximum depth limits the number of nodes in the tree. Tune this parameter
        for best performance; the best value depends on the interaction
        of the input variables. Deeper the tree the more variable interactions
        the model can capture. Tree still grow by leaf-wise.
        <0 means no limit
        ``values: ['none', [1, 1e4]]``
    max_bin (mb): intgrid (default='255')
        Max number of bin that feature values will bucket in.
        Small bin may reduce training accuracy but may increase general power (deal with overfit).
        LightGBM will auto compress memory according max_bin. For example, LightGBM will use
        uint8_t for feature value if max_bin=255.
        ``values: [3, 1e4]``
    subsample_for_bin (ssfb): int (default=50000)
        Number of samples for constructing bins.
        ``values: [1, 1e6]``
    min_split_gain (msg): floatgrid (default='0')
        Minimum loss reduction required to make a further partition on a leaf node of the tree.
        ``values: [0, 100]``
    min_child_weight (mcw): intgrid (default='5')
        Minimum sum of instance weight(hessian) needed in a child(leaf).
        ``values: [0, 1e2]``
    min_child_samples (mcs): int (default='10')
        Minimum number of data need in a child(leaf).
        ``values: [0, 1e3]``
    subsample (ss): floatgrid (default='1.0')
        Subsample ratio of the training instance.
        ``values: [0.01, 1]``
    subsample_freq (ssf): intgrid (default='1')
        Frequency of subsample 'none' means it is not enabled.
        ``values: ['none', [1, 1e3]]``
    colsample_bytree (cbt): floatgrid (default='1.0')
        Subsample ratio of columns when constructing each tree.
        By default, the value of ``colsample_bytree`` for LightGBM classes is 1.0. However, based on the
        training data, DataRobot may choose a different initial value for this parameter.
        ``values: [0, 1]``
    reg_alpha (ra): floatgrid (default='0')
        L1 regularization term on weights.
        ``values: [0, 1e6]``
    reg_lambda (rl): floatgrid (default='0')
        L2 regularization term on weights.
        ``values: [0, 1e6]``
    sigmoid (s): floatgrid (default='1.0')
        Parameter for sigmoid function. Used in binary classification and LambdaRank.
        ``values: [1e-06, 1e03]``
    is_unbalance (iu): select (default=False)
        Set to true if training data are unbalanced. Used in binary classification.
        ``values: [True, False]``
    early_stopping_rounds (esr): int (default='10')
        Will stop training if one metric of one validation data doesn't improve in last
        early_stopping_round rounds.
        ``values: [0, 1e3]``
    
    References
    ----------
    .. [1] Chen, T, and He, T.
       Higgs Boson Discovery with Boosted Trees."" Cowan et al.,
       editor, JMLR: Workshop and Conference Proceedings. No. 42. 2015.
       `[link]
       <http://proceedings.mlr.press/v42/chen14.pdf>`__
    .. [2] Freund, Yoav, and Robert E. Schapire.
       ""A decision-theoretic generalization of on-line learning and an application to boosting.""
       Journal of computer and system sciences
       55.1 (1997): 119-139.
       `[link]
       <https://doi.org/10.1006/jcss.1997.1504>`__
    .. [3] Friedman, Jerome H.
       ""Greedy function approximation: a gradient boosting machine.""
       Annals of statistics (2001): 1189-1232.
       `[link]
       <https://statweb.stanford.edu/~jhf/ftp/trebst.pdf>`__
    .. [4] Breiman, Leo. Arcing the edge.
       Technical Report 486, Statistics Department,
       University of California at Berkeley, 1997.
       `[link]
       <https://www.stat.berkeley.edu/~breiman/arcing-the-edge.pdf>`__
    
    See Also
    --------
    Source:
        `LightGBM on GitHub
        <https://github.com/Microsoft/LightGBM>`_
    Source:
        `Gradient boosting wikipedia
        <https://en.wikipedia.org/wiki/Gradient_boosting>`_",no url,LightGBM Trees Classifier with Grid Search and Early Stopping support,ESLGBMTC,Multi-class Classification,no documentation retrieved,NUM,Light Gradient Boosted Trees Classifier with Early Stopping,Light GBM Classifier with Early Stopping with GBDT
Logistic Regression with no penalty.  Logistic regression is a generalized linear model that uses a logistic link function. Based on scikit-learn.,http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression,Logistic Regression with no penalty,LR,Multi-class Classification,"













LogisticRegression — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.linear_model
LogisticRegression









LogisticRegression#


class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)[source]#
Logistic Regression (aka logit, MaxEnt) classifier.
In the multiclass case, the training algorithm uses the one-vs-rest (OvR)
scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the
cross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’.
(Currently the ‘multinomial’ option is supported only by the ‘lbfgs’,
‘sag’, ‘saga’ and ‘newton-cg’ solvers.)
This class implements regularized logistic regression using the
‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers. Note
that regularization is applied by default. It can handle both dense
and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit
floats for optimal performance; any other input format will be converted
(and copied).
The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization
with primal formulation, or no regularization. The ‘liblinear’ solver
supports both L1 and L2 regularization, with a dual formulation only for
the L2 penalty. The Elastic-Net regularization is only supported by the
‘saga’ solver.
Read more in the User Guide.

Parameters:

penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’Specify the norm of the penalty:

None: no penalty is added;
'l2': add a L2 penalty term and it is the default choice;
'l1': add a L1 penalty term;
'elasticnet': both L1 and L2 penalty terms are added.


Warning
Some penalties may not work with some solvers. See the parameter
solver below, to know the compatibility between the penalty and
solver.


Added in version 0.19: l1 penalty with SAGA solver (allowing ‘multinomial’ + L1)


dualbool, default=FalseDual (constrained) or primal (regularized, see also
this equation) formulation. Dual formulation
is only implemented for l2 penalty with liblinear solver. Prefer dual=False when
n_samples > n_features.

tolfloat, default=1e-4Tolerance for stopping criteria.

Cfloat, default=1.0Inverse of regularization strength; must be a positive float.
Like in support vector machines, smaller values specify stronger
regularization.

fit_interceptbool, default=TrueSpecifies if a constant (a.k.a. bias or intercept) should be
added to the decision function.

intercept_scalingfloat, default=1Useful only when the solver ‘liblinear’ is used
and self.fit_intercept is set to True. In this case, x becomes
[x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equal to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic_feature_weight.
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.

class_weightdict or ‘balanced’, default=NoneWeights associated with classes in the form {class_label: weight}.
If not given, all classes are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as n_samples / (n_classes * np.bincount(y)).
Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.

Added in version 0.17: class_weight=’balanced’


random_stateint, RandomState instance, default=NoneUsed when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the
data. See Glossary for details.

solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’},             default=’lbfgs’Algorithm to use in the optimization problem. Default is ‘lbfgs’.
To choose a solver, you might want to consider the following aspects:

For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’
and ‘saga’ are faster for large ones;
For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and
‘lbfgs’ handle multinomial loss;
‘liblinear’ and ‘newton-cholesky’ can only handle binary classification
by default. To apply a one-versus-rest scheme for the multiclass setting
one can wrapt it with the OneVsRestClassifier.
‘newton-cholesky’ is a good choice for n_samples >> n_features,
especially with one-hot encoded categorical features with rare
categories. Be aware that the memory usage of this solver has a quadratic
dependency on n_features because it explicitly computes the Hessian
matrix.


Warning
The choice of the algorithm depends on the penalty chosen and on
(multinomial) multiclass support:


solver
penalty
multinomial multiclass



‘lbfgs’
‘l2’, None
yes

‘liblinear’
‘l1’, ‘l2’
no

‘newton-cg’
‘l2’, None
yes

‘newton-cholesky’
‘l2’, None
no

‘sag’
‘l2’, None
yes

‘saga’
‘elasticnet’, ‘l1’, ‘l2’, None
yes






Note
‘sag’ and ‘saga’ fast convergence is only guaranteed on features
with approximately the same scale. You can preprocess the data with
a scaler from sklearn.preprocessing.


See also
Refer to the User Guide for more information regarding
LogisticRegression and more specifically the
Table
summarizing solver/penalty supports.


Added in version 0.17: Stochastic Average Gradient descent solver.


Added in version 0.19: SAGA solver.


Changed in version 0.22: The default solver changed from ‘liblinear’ to ‘lbfgs’ in 0.22.


Added in version 1.2: newton-cholesky solver.


max_iterint, default=100Maximum number of iterations taken for the solvers to converge.

multi_class{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’If the option chosen is ‘ovr’, then a binary problem is fit for each
label. For ‘multinomial’ the loss minimised is the multinomial loss fit
across the entire probability distribution, even when the data is
binary. ‘multinomial’ is unavailable when solver=’liblinear’.
‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,
and otherwise selects ‘multinomial’.

Added in version 0.18: Stochastic Average Gradient descent solver for ‘multinomial’ case.


Changed in version 0.22: Default changed from ‘ovr’ to ‘auto’ in 0.22.


Deprecated since version 1.5: multi_class was deprecated in version 1.5 and will be removed in 1.7.
From then on, the recommended ‘multinomial’ will always be used for
n_classes >= 3.
Solvers that do not support ‘multinomial’ will raise an error.
Use sklearn.multiclass.OneVsRestClassifier(LogisticRegression()) if you
still want to use OvR.


verboseint, default=0For the liblinear and lbfgs solvers set verbose to any positive
number for verbosity.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
Useless for liblinear solver. See the Glossary.

Added in version 0.17: warm_start to support lbfgs, newton-cg, sag, saga solvers.


n_jobsint, default=NoneNumber of CPU cores used when parallelizing over classes if
multi_class=’ovr’”. This parameter is ignored when the solver is
set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or
not. None means 1 unless in a joblib.parallel_backend
context. -1 means using all processors.
See Glossary for more details.

l1_ratiofloat, default=NoneThe Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only
used if penalty='elasticnet'. Setting l1_ratio=0 is equivalent
to using penalty='l2', while setting l1_ratio=1 is equivalent
to using penalty='l1'. For 0 < l1_ratio <1, the penalty is a
combination of L1 and L2.



Attributes:

classes_ndarray of shape (n_classes, )A list of class labels known to the classifier.

coef_ndarray of shape (1, n_features) or (n_classes, n_features)Coefficient of the features in the decision function.
coef_ is of shape (1, n_features) when the given problem is binary.
In particular, when multi_class='multinomial', coef_ corresponds
to outcome 1 (True) and -coef_ corresponds to outcome 0 (False).

intercept_ndarray of shape (1,) or (n_classes,)Intercept (a.k.a. bias) added to the decision function.
If fit_intercept is set to False, the intercept is set to zero.
intercept_ is of shape (1,) when the given problem is binary.
In particular, when multi_class='multinomial', intercept_
corresponds to outcome 1 (True) and -intercept_ corresponds to
outcome 0 (False).

n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.


n_iter_ndarray of shape (n_classes,) or (1, )Actual number of iterations for all classes. If binary or multinomial,
it returns only 1 element. For liblinear solver, only the maximum
number of iteration across all classes is given.

Changed in version 0.20: In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
max_iter. n_iter_ will now report at most max_iter.






See also

SGDClassifierIncrementally trained logistic regression (when given the parameter loss=""log_loss"").

LogisticRegressionCVLogistic regression with built-in cross validation.



Notes
The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.
Predict output may not match that of standalone liblinear in certain
cases. See differences from liblinear
in the narrative documentation.
References

L-BFGS-B – Software for Large-scale Bound-constrained OptimizationCiyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.
http://users.iems.northwestern.edu/~nocedal/lbfgsb.html

LIBLINEAR – A Library for Large Linear Classificationhttps://www.csie.ntu.edu.tw/~cjlin/liblinear/

SAG – Mark Schmidt, Nicolas Le Roux, and Francis BachMinimizing Finite Sums with the Stochastic Average Gradient
https://hal.inria.fr/hal-00860051/document

SAGA – Defazio, A., Bach F. & Lacoste-Julien S. (2014).“SAGA: A Fast Incremental Gradient Method With Support
for Non-Strongly Convex Composite Objectives”

Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descentmethods for logistic regression and maximum entropy models.
Machine Learning 85(1-2):41-75.
https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf


Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import LogisticRegression
>>> X, y = load_iris(return_X_y=True)
>>> clf = LogisticRegression(random_state=0).fit(X, y)
>>> clf.predict(X[:2, :])
array([0, 0])
>>> clf.predict_proba(X[:2, :])
array([[9.8...e-01, 1.8...e-02, 1.4...e-08],
       [9.7...e-01, 2.8...e-02, ...e-08]])
>>> clf.score(X, y)
0.97...




decision_function(X)[source]#
Predict confidence scores for samples.
The confidence score for a sample is proportional to the signed
distance of that sample to the hyperplane.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the confidence scores.



Returns:

scoresndarray of shape (n_samples,) or (n_samples, n_classes)Confidence scores per (n_samples, n_classes) combination. In the
binary case, confidence score for self.classes_[1] where >0 means
this class would be predicted.







densify()[source]#
Convert coefficient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the
default format of coef_ and is required for fitting, so calling
this method is only required on models that have previously been
sparsified; otherwise, it is a no-op.

Returns:

selfFitted estimator.







fit(X, y, sample_weight=None)[source]#
Fit the model according to the given training data.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)Training vector, where n_samples is the number of samples and
n_features is the number of features.

yarray-like of shape (n_samples,)Target vector relative to X.

sample_weightarray-like of shape (n_samples,) default=NoneArray of weights that are assigned to individual samples.
If not provided, then each sample is given unit weight.

Added in version 0.17: sample_weight support to LogisticRegression.




Returns:

selfFitted estimator.




Notes
The SAGA solver supports both float64 and float32 bit arrays.



get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Returns:

routingMetadataRequestA MetadataRequest encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict class labels for samples in X.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The data matrix for which we want to get the predictions.



Returns:

y_predndarray of shape (n_samples,)Vector containing the class labels for each sample.







predict_log_proba(X)[source]#
Predict logarithm of probability estimates.
The returned estimates for all classes are ordered by the
label of classes.

Parameters:

Xarray-like of shape (n_samples, n_features)Vector to be scored, where n_samples is the number of samples and
n_features is the number of features.



Returns:

Tarray-like of shape (n_samples, n_classes)Returns the log-probability of the sample for each class in the
model, where classes are ordered as they are in self.classes_.







predict_proba(X)[source]#
Probability estimates.
The returned estimates for all classes are ordered by the
label of classes.
For a multi_class problem, if multi_class is set to be “multinomial”
the softmax function is used to find the predicted probability of
each class.
Else use a one-vs-rest approach, i.e. calculate the probability
of each class assuming it to be positive using the logistic function
and normalize these values across all the classes.

Parameters:

Xarray-like of shape (n_samples, n_features)Vector to be scored, where n_samples is the number of samples and
n_features is the number of features.



Returns:

Tarray-like of shape (n_samples, n_classes)Returns the probability of the sample for each class in the model,
where classes are ordered as they are in self.classes_.







score(X, y, sample_weight=None)[source]#
Return the mean accuracy on the given test data and labels.
In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.

Parameters:

Xarray-like of shape (n_samples, n_features)Test samples.

yarray-like of shape (n_samples,) or (n_samples, n_outputs)True labels for X.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights.



Returns:

scorefloatMean accuracy of self.predict(X) w.r.t. y.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → LogisticRegression[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







set_score_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → LogisticRegression[source]#
Request metadata passed to the score method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to score.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in score.



Returns:

selfobjectThe updated object.







sparsify()[source]#
Convert coefficient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for
L1-regularized models can be much more memory- and storage-efficient
than the usual numpy.ndarray representation.
The intercept_ member is not converted.

Returns:

selfFitted estimator.




Notes
For non-sparse models, i.e. when there are not many zeros in coef_,
this may actually increase memory usage, so use this method with
care. A rule of thumb is that the number of zero elements, which can
be computed with (coef_ == 0).sum(), must be more than 50% for this
to provide significant benefits.
After calling this method, further fitting with the partial_fit
method (if any) will not work until you call densify.



Gallery examples#

Release Highlights for scikit-learn 1.5
Release Highlights for scikit-learn 1.5

Release Highlights for scikit-learn 1.3
Release Highlights for scikit-learn 1.3

Release Highlights for scikit-learn 1.1
Release Highlights for scikit-learn 1.1

Release Highlights for scikit-learn 1.0
Release Highlights for scikit-learn 1.0

Release Highlights for scikit-learn 0.24
Release Highlights for scikit-learn 0.24

Release Highlights for scikit-learn 0.23
Release Highlights for scikit-learn 0.23

Release Highlights for scikit-learn 0.22
Release Highlights for scikit-learn 0.22

Probability Calibration curves
Probability Calibration curves

Plot classification probability
Plot classification probability

Feature transformations with ensembles of trees
Feature transformations with ensembles of trees

Plot class probabilities calculated by the VotingClassifier
Plot class probabilities calculated by the VotingClassifier

Model-based and sequential feature selection
Model-based and sequential feature selection

Recursive feature elimination
Recursive feature elimination

Recursive feature elimination with cross-validation
Recursive feature elimination with cross-validation

Comparing various online solvers
Comparing various online solvers

L1 Penalty and Sparsity in Logistic Regression
L1 Penalty and Sparsity in Logistic Regression

Logistic Regression 3-class Classifier
Logistic Regression 3-class Classifier

Logistic function
Logistic function

MNIST classification using multinomial logistic + L1
MNIST classification using multinomial logistic + L1

Multiclass sparse logistic regression on 20newgroups
Multiclass sparse logistic regression on 20newgroups

Plot multinomial and One-vs-Rest Logistic Regression
Plot multinomial and One-vs-Rest Logistic Regression

Regularization path of L1- Logistic Regression
Regularization path of L1- Logistic Regression

Displaying Pipelines
Displaying Pipelines

Displaying estimators and complex pipelines
Displaying estimators and complex pipelines

Introducing the set_output API
Introducing the set_output API

Visualizations with Display Objects
Visualizations with Display Objects

Class Likelihood Ratios to measure classification performance
Class Likelihood Ratios to measure classification performance

Multiclass Receiver Operating Characteristic (ROC)
Multiclass Receiver Operating Characteristic (ROC)

Post-hoc tuning the cut-off point of decision function
Post-hoc tuning the cut-off point of decision function

Post-tuning the decision threshold for cost-sensitive learning
Post-tuning the decision threshold for cost-sensitive learning

Multilabel classification using a classifier chain
Multilabel classification using a classifier chain

Restricted Boltzmann Machine features for digit classification
Restricted Boltzmann Machine features for digit classification

Column Transformer with Mixed Types
Column Transformer with Mixed Types

Pipelining: chaining a PCA and a logistic regression
Pipelining: chaining a PCA and a logistic regression

Feature discretization
Feature discretization

Digits Classification Exercise
Digits Classification Exercise

Classification of text documents using sparse features
Classification of text documents using sparse features










previous
sklearn.linear_model




next
LogisticRegressionCV










 On this page
  


LogisticRegression
decision_function
densify
fit
get_metadata_routing
get_params
predict
predict_log_proba
predict_proba
score
set_fit_request
set_params
set_score_request
sparsify


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Logistic Regression,Logistic Regression with no penalty.  Logistic regression is a generalized linear model that uses a logistic link function. Based on scikit-learn.
Keras Neural Network Multi-Class Classifier,no url,Keras Neural Network Multi-Class Classifier,KERASMULTIC,Multi-class Classification,no documentation retrieved,NUM,Keras Neural Network Classifier,Keras Neural Network Multi-Class Classifier
Random Forests based on scikit-learn. Random forests are an ensemble method where hundreds (or thousands) of individual decision trees are fit to boostrap re-samples of the original dataset.  ExtraTrees are a variant of RandomForests with even more randomness.,,Random Forests based on scikit-learn,RFC,Multi-class Classification,,NUM,ExtraTrees Classifier (Gini),Random Forests based on scikit-learn. Random forests are an ensemble method where hundreds (or thousands) of individual decision trees are fit to boostrap re-samples of the original dataset.  ExtraTrees are a variant of RandomForests with even more randomness.
Gradient Boosting Classifier (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.,http://xgboost.readthedocs.org/,Gradient Boosting Classifier (xgboost) with Early-Stopping,ESXGBC2,Multi-class Classification,"




XGBoost Documentation — xgboost 2.1.1 documentation



































                stable
              









Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
R Package
JVM Package
Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost






xgboost






XGBoost Documentation

 Edit on GitHub







XGBoost Documentation
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
It implements machine learning algorithms under the Gradient Boosting framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.

Contents


Installation Guide
Building From Source
Get Started with XGBoost
XGBoost Tutorials
Introduction to Boosted Trees
Introduction to Model IO
Learning to Rank
DART booster
Monotonic Constraints
Feature Interaction Constraints
Survival Analysis with Accelerated Failure Time
Categorical Data
Multiple Outputs
Random Forests(TM) in XGBoost
Distributed XGBoost on Kubernetes
Distributed XGBoost with XGBoost4J-Spark
Distributed XGBoost with XGBoost4J-Spark-GPU
Distributed XGBoost with Dask
Distributed XGBoost with PySpark
Distributed XGBoost with Ray
Using XGBoost External Memory Version
C API Tutorial
Text Input Format of DMatrix
Notes on Parameter Tuning
Custom Objective and Evaluation Metric
Intercept
Privacy Preserving Inference with Concrete ML


Frequently Asked Questions
XGBoost User Forum
GPU Support
XGBoost Parameters
Prediction
Tree Methods
Python Package
Python Package Introduction
Using the Scikit-Learn Estimator Interface
Python API Reference
Callback Functions
Model
XGBoost Python Feature Walkthrough
XGBoost Dask Feature Walkthrough
Survival Analysis Walkthrough
GPU Acceleration Demo
Using XGBoost with RAPIDS Memory Manager (RMM) plugin (EXPERIMENTAL)


R Package
Introduction to XGBoost in R
Understanding your dataset with XGBoost
Handling of indexable elements


JVM Package
Getting Started with XGBoost4J
XGBoost4J-Spark Tutorial
XGBoost4J-Spark-GPU Tutorial
Code Examples
API docs


Ruby Package
Swift Package
Julia Package
C Package
C++ Interface
CLI Interface
Contribute to XGBoost
Community Guideline
Donations
Coding Guideline
Consistency for Language Bindings
Notes on packaging XGBoost’s Python package
Adding and running tests
Docs and Examples
XGBoost Internal Feature Map
Git Workflow Howtos
XGBoost Release Policy
Automated testing in XGBoost project









Next 



© Copyright 2022, xgboost developers.
      Revision ae68466e.
      


  Built with Sphinx using a
    theme
    provided by Read the Docs.
   








",NUM,eXtreme Gradient Boosted Trees Classifier with Early Stopping,Gradient Boosting Classifier (xgboost) with Early-Stopping. Gradient Boosting Machines are a cutting-edge algorithm for fitting extremely accurate predictive models. GBMs have won a number of recent predictive modeling competitions.
Light GBM Classifier with Early Stopping with GBDT and Boosting on Residuals,https://github.com/Microsoft/LightGBM/blob/master/docs/README.md,Light GBM Classifier with Early Stopping with GBDT and Boosting on Residuals,RES_ESLGBMTC,Boosted Multi-class,no documentation retrieved,NUM,Light Gradient Boosting Classification on ElasticNet Predictions,Light GBM Classifier with Early Stopping with GBDT and Boosting on Residuals
Light GBM Classifier with GBDT with Boosting on Residuals,https://github.com/Microsoft/LightGBM/blob/master/docs/README.md,Light GBM Classifier with GBDT with Boosting on Residuals,RES_PLGBMTC,Boosted Multi-class,no documentation retrieved,NUM,Light Gradient Boosting on ElasticNet Predictions,Light GBM Classifier with GBDT with Boosting on Residuals
Keras Convolutional Neural Network Multi-Label Classifier for Text,no url,Keras Convolutional Neural Network Multi-Label Classifier for Text,KERAS_CNN_TEXT_MULTILABELC,Multi-label Classification,no documentation retrieved,NUM,Keras Text Convolutional Neural Network Multi-label Classifier,Keras Convolutional Neural Network Multi-Label Classifier for Text
"Unsupervised Outlier Detection using Local Outlier Factor (LOF)
    
    This anomaly detection algorithm identifies points that are relatively
    isolated compared to neighboring points in the dataset.
    
    The anomaly score of each sample is called the ""local outlier factor.""
    It measures the local density deviation of a given sample with respect
    to its neighbors (the anomaly score depends on how
    isolated the object is within the surrounding neighborhood). More
    precisely, locality is given by k-nearest neighbors, whose distance is
    used to estimate the local density. Comparing the local density of a
    sample to the local densities of its neighbors identifies samples
    that have a substantially lower density than their neighbors and are therefore outliers.
    
    DataRobot normalizes the training scores between
    0 and 1, where 1 is considered an anomaly. When predicting on new data,
    the anomaly score may be greater than 1 when a row is considered more
    anomalous than those in the training data.
    
    Parameters
    ----------
    n_neighbors (k) : int (default='100')
        Number of neighbors to use by default for k-neighbors queries.
        If n_neighbors is larger than the number of samples provided, all
        samples will be used.
        ``values': [10, 10000]``
    algorithm (alg) : select (default='auto')
        Algorithm used to compute the nearest neighbors:
        - 'ball_tree' will use :class:`BallTree`
        - 'kd_tree' will use :class:`KDTree`
        - 'brute' will use a brute-force search.
        - 'auto' will attempt to decide the most appropriate algorithm
        based on the values passed to :meth:`fit` method.
        Note: fitting on sparse input will override the setting of
        this parameter, using brute force.
        ``values: ['auto', 'ball_tree', 'kd_tree', 'brute']``
    leaf_size (ls) : int (default='30')
        Leaf size passed to BallTree or KDTree.  This can affect the
        speed of the construction and query, as well as the memory
        required to store the tree. The optimal value depends on the
        nature of the problem.
        ``values: [1, 100]``
    metric (m) : select (default='minkowski')
        Distance metric to use for the tree. The default metric is
        minkowski, and with p=2, is equivalent to the standard Euclidean
        metric. See the documentation of the DistanceMetric class for a
        list of available metrics.
        ``values: ['euclidean','manhattan','chebyshev','minkowski']``
    p (p): int (default='2')
        Power parameter for the Minkowski metric. When p = 1, this is
        equivalent to using manhattan_distance (l1), and euclidean_distance
        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
        ``values: [1, 10]``
    expected_outlier_fraction (eof) : float (default='0.1')
        Fraction of data to be defined as outliers, with results represented in the anomaly
        detection insights table.
        ``values: [0.01, 0.25]``
    
    References
    ----------
    .. [1] Breunig, M. et al
       ""LOF: Identifying Density-Based Local Outliers.""
       PROCEEDINGS OF THE 2000 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA.
       `[link]
       <http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf>`__
    
    See Also
    --------
    Source:
        `sklearn.neighbors.LocalOutlierFactor
        <http://scikit-learn.org/dev/modules/generated/sklearn.neighbors.LocalOutlierFactor
        .html>`_",no url,Unsupervised Outlier Detection using Local Outlier Factor (LOF),ADLOF,Anomaly Detection,no documentation retrieved,NUM,Local Outlier Factor Anomaly Detection,Anomaly Detection Local Outlier Factor
Keras Variational Autoencoder for Anomaly Detection,no url,Keras Variational Autoencoder for Anomaly Detection,KERAS_VARIATIONAL_AUTOENCODER,Anomaly Detection,no documentation retrieved,NUM,Keras Variational Autoencoder,Keras Variational Autoencoder for Anomaly Detection
"Mahalanobis distance using PCA
    
    The Mahalanobis distance is a measure of the distance between a point P and
    a distribution D. This distance can be used to identify anomalous points in a data set.
    
    The Mahalanobis distance is a multi-dimensional generalization of the idea of measuring
    how many standard deviations away the point is from the mean of the distribution.
    This distance is zero if the point is at the mean and increases as the point moves
    away from the mean. It measures the number of standard deviations from the point to
    the distribution mean along each principal component axis. Mahalanobis distance
    is unitless and scale-invariant, and takes into account the correlations of the data set.
    
    The distance is calculated as Euclidean distance of the point from the mean of the distribution
    after scaling the variables to have zero mean, unit standard deviation, and decomposing
    them into principal components.
    
    DataRobot uses a user-defined threshold distance for determining anomalies. If a row's distance
    is smaller than this threshold distance, it is given an anomaly score of 0.
    For the training rows exceeding the threshold distance, DataRobot normalizes the scores so the
    greatest distance away from the mean corresponds to an anomaly score of 1.
    When predicting on new data, the anomaly score may be greater than 1 when a row has a
    larger distance than the rows in the training data.
    
    Parameters
    ----------
    expected_outlier_fraction (eof) : float (default='0.1')
        The amount of contamination of the data set, i.e. the proportion of
        outliers in the data set - Expected Outlier Fraction
        Used when fitting to define the threshold on a decision function.
        ``values: [0.01,0.25]``
    thresh (th) : float (default='1.5')
        The distance to use as the threshold. Points with a Mahalanobis distance
        below the threshold are considered normal.
        ``values:[1.,50.]``
    
    fraction_of_variance_explained (fve): float (default='0.99')
        Keep enough PCA components to explain this fraction of the variance
        ``values: [0.01,1]``
    
    References
    ----------
    .. [1] Mahalanobis, Prasanta Chandra (1936).
       ""On the generalised distance in statistics"". Proceedings of the National Institute
       of Sciences of India. 2 (1): 49-55
    .. [2] M. Tipping and C. Bishop (1999).
       ""Probabilistic Principal Component Analysis"". Journal of the Royal Statistical Society,
       Series B, 61, Part 3, pp. 611-622
    
    See Also
    --------",no url,Mahalanobis distance using PCA,ADMahalPCA,Anomaly Detection,no documentation retrieved,NUM,Mahalanobis Distance Ranked Anomaly Detection with PCA,Anomaly Detection Ranked Mahalanobis Distance with PCA
Anomaly Detection with Supervised Learning and Calibration,no url,Anomaly Detection with Supervised Learning and Calibration,ADXGB2_CAL,Anomaly Detection,no documentation retrieved,NUM,Anomaly Detection with Supervised Learning (XGB) and Calibration,Anomaly Detection with Supervised Learning and Calibration
Anomaly Detection with Supervised Learning,no url,Anomaly Detection with Supervised Learning,ADXGB,Anomaly Detection,no documentation retrieved,NUM,Anomaly Detection with Supervised Learning (XGB),Anomaly Detection with Supervised Learning
"Double Median Absolute Deviation
    
    This model applies a robust statistical test to identify outliers in a dataset.
    
    If the underlying distribution of the feature is asymmetric, using the
    standard-deviations-from-mean and MADs-from-median strategies to
    flag outliers both fail. The problem is that these strategies apply
    the same cutoff to both tails of a sample, even if one tail is far
    longer than the other.
    
    One solution is to take the Double MAD. This is a pair of statistics:
    (1) the median absolute deviation from the median of all points less
    than or equal to the median and
    (2) the median absolute deviation from the median of all points greater
    than or equal to the median.
    
    Use the first of these to calculate the distance
    from the median of all points less than or equal to the median;
    use the second to calculate that distance for points that are greater than
    the median.
    
    If more than 50% of the data have identical values, MAD will equal zero.
    All points in the dataset, except those that equal the median, will then be flagged
    as outliers, regardless of the outlier cutoff level.
    
    DataRobot normalizes the training scores between 0 and 1, where 1 is considered
    an anomaly. When predicting on new data, the anomaly score may be greater than 1
    when a row is considered more anomalous than those in the training data.
    
    Parameters
    ----------
    expected_outlier_fraction (eof) : float (default='0.1')
        Amount of contamination of the dataset, i.e., the proportion of
        outliers in the dataset (""Expected Outlier Fraction"").
        Used when fitting to define the threshold on the decision function.
        ``values: [0.01,0.25]``
    thresh (th) : float (default='3.5')
        Distance to use as the threshold. Points with a distance
        below the threshold are considered normal.
        ``values:[1.,50.]``
    
    References
    ----------
    .. [1] Leys, C., et al
       ""Detecting outliers: Do not use standard deviation around the mean, use
       absolute deviation around the median.""
       Journal of Experimental Social Psychology, Volume 49,
       Issue 4, July 2013, pp. 764-766
       `[link]
       <http://www.academia.edu/3448313/Detecting_outliers_Do_not_use_standard
       _deviations_around_the_mean_do_use_the_median_absolute_deviation_around
       _the_median>`__
    
    See Also
    --------",no url,Double Median Absolute Deviation,ADDMAD,Anomaly Detection,no documentation retrieved,NUM,Double Median Absolute Deviation Anomaly Detection,Anomaly Detection Univariate Double Median Absolute Deviation
"One Class Support Vector Machine
    
    One-class SVM is used for novelty detection. That is, given a set
    of samples, it will detect the soft boundary of that set so as to
    classify new points as belonging to that set or not. This is similar
    to SVM applied to binary classification, where the algorithm identifies
    a boundary to separate two classes with a maximum margin. In the
    case of one-class SVM, the training set is assumed to consist largely of
    normal data of one class and the algorithm will identify a boundary that
    encapsulates a majority of the training points. Points lying outside this
    boundary are considered anomalous. This implementation is based on `libsvm`.
    
    DataRobot normalizes training scores between
    0 and 1, where 1 is considered an anomaly. When predicting on new data,
    the anomaly score may be greater than 1 when a row is considered more
    anomalous than those in the training data. When the one-class SVM has
    difficulty converging, there may be as many anomalies as there are normal rows.
    
    Parameters
    ----------
    expected_outlier_fraction (eof) : float (default='0.1')
        Amount of contamination of the dataset, i.e., the proportion of
        outliers in the data set (""Expected Outlier Fraction"").
        Used when fitting to define the threshold on the decision function.
        ``values: [0.01,0.25]``
    
        nu is set by ``0.95 * expected_outlier_fraction + 0.05``
    kernel (kl) : select (default='rbf')
        Kernel type to be used in the algorithm.
        ``values: ['linear', 'poly', 'rbf', 'sigmoid']``
    degree (dg) : int (default='3')
        Degree of the polynomial kernel function (when kernel = 'poly').
        Ignored by all other kernels.
        ``values: [1, 10]``
    coef0 (co) : float (default='0')
        Independent term in kernel function.
        Applicable only when kernel is set to 'poly' or 'sigmoid'.
        ``values: [1e-10, 1e10]``
    shrinking (sh) : bool (default='True')
        When True (the default), uses the shrinking heuristic.
        ``values: [False, True]``
    tol (t) : float (default='None')
        Tolerance for stopping criterion.
        ``values: [1e-10, 1e10]``
    max_iter (mi) : int (default='-1')
        Hard limit on iterations within solver, or -1 for no limit.
        ``values: [-1, 999]``
    random_state (rs) : int (default='369')
        Seed of the pseudo-random number generator to use when
        shuffling the data for probability estimation.
        ``values: [0, int(1e10)]``
    gamma (gm) : float (default='1e-5')
        Coefficient when kernel is set to 'rbf', 'poly', or 'sigmoid'.
        If gamma is 'auto' ,then 1/n_features will be used instead.
        ``values: [0, int(1e-5)]``
    
    References
    ----------
    .. [1] Chang, Chih-Chung, and Chih-Jen Lin. ""LIBSVM: A library for support
       vector machines."" ACM Transactions on Intelligent Systems and Technology
       (TIST) 2.3 (2011): 27. `[link] <http://ntucsu.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`__
    
    See Also
    --------
    Source:
        `sklearn.svm.OneClassSVM
        <http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html>`_",no url,One Class Support Vector Machine,ADOSVM,Anomaly Detection,no documentation retrieved,NUM,One-Class SVM Anomaly Detection,Anomaly Detection One Class SVM
"Unsupervised Outlier Detection using Local Outlier Factor (LOF) with Calibration
    
    This anomaly detection algorithm identifies points that are relatively
    isolated compared to neighboring points in the dataset.
    
    The anomaly score of each sample is called the ""local outlier factor.""
    It measures the local density deviation of a given sample with respect
    to its neighbors (the anomaly score depends on how
    isolated the object is within the surrounding neighborhood). More
    precisely, locality is given by k-nearest neighbors, whose distance is
    used to estimate the local density. Comparing the local density of a
    sample to the local densities of its neighbors identifies samples
    that have a substantially lower density than their neighbors and are therefore outliers.
    
    When performing anomaly detection you often want not only to predict
    the class label, but also obtain a probability of the respective label.
    Although anomaly detection is unsupervised, we can use outlier detection
    on the uncalibrated anomaly scores to generate anomaly labels. After
    generating the labels the scores can be calibrated using Platt's
    method as for a classifier. This probability gives you some kind of
    confidence on the prediction. It effectively tells you the probability
    that the raw anomaly score is an outlier, given the distribution of
    scores seen in the training set.
    
    Parameters
    ----------
    n_neighbors (k) : int (default='100')
        Number of neighbors to use by default for k-neighbors queries.
        If n_neighbors is larger than the number of samples provided, all
        samples will be used.
        ``values': [10, 10000]``
    algorithm (alg) : select (default='auto')
        Algorithm used to compute the nearest neighbors:
        - 'ball_tree' will use :class:`BallTree`
        - 'kd_tree' will use :class:`KDTree`
        - 'brute' will use a brute-force search.
        - 'auto' will attempt to decide the most appropriate algorithm
        based on the values passed to :meth:`fit` method.
        Note: fitting on sparse input will override the setting of
        this parameter, using brute force.
        ``values: ['auto', 'ball_tree', 'kd_tree', 'brute']``
    leaf_size (ls) : int (default='30')
        Leaf size passed to BallTree or KDTree.  This can affect the
        speed of the construction and query, as well as the memory
        required to store the tree. The optimal value depends on the
        nature of the problem.
        ``values: [1, 100]``
    metric (m) : select (default='minkowski')
        Distance metric to use for the tree. The default metric is
        minkowski, and with p=2, is equivalent to the standard Euclidean
        metric. See the documentation of the DistanceMetric class for a
        list of available metrics.
        ``values: ['euclidean','manhattan','chebyshev','minkowski']``
    p (p): int (default='2')
        Power parameter for the Minkowski metric. When p = 1, this is
        equivalent to using manhattan_distance (l1), and euclidean_distance
        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
        ``values: [1, 10]``
    expected_outlier_fraction (eof) : float (default='0.1')
        Fraction of data to be defined as outliers, with results represented in the anomaly
        detection insights table.
        ``values: [0.01, 0.25]``
    calibration_outlier_fraction : multi (default='None')
        Fraction that determines the proportion of data to be treated as outliers during the
        calibration stage. If 'None', the proportion of outliers is automatically inferred based on the distribution of
        raw anomaly scores. If float, samples with the top highest raw anomaly scores are selected
        and this proportion of outlier samples is enforced during the calibration stage.
        ``values: {'select':['None'], 'float':[1e-5, 1.]}``
    
    References
    ----------
    .. [1] Breunig, M. et al
       ""LOF: Identifying Density-Based Local Outliers.""
       PROCEEDINGS OF THE 2000 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA.
       `[link]
       <http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf>`__
    
    See Also
    --------
    Source:
        `sklearn.neighbors.LocalOutlierFactor
        <http://scikit-learn.org/dev/modules/generated/sklearn.neighbors.LocalOutlierFactor
        .html>`_",no url,Unsupervised Outlier Detection using Local Outlier Factor (LOF) with Calibration,ADLOF_CAL,Anomaly Detection,no documentation retrieved,NUM,Local Outlier Factor Anomaly Detection with Calibration,Anomaly Detection Local Outlier Factor with calibration
"Double Median Absolute Deviation with Calibration
    
    This model applies a robust statistical test to identify outliers in a dataset.
    
    If the underlying distribution of the feature is asymmetric, using the
    standard-deviations-from-mean and MADs-from-median strategies to
    flag outliers both fail. The problem is that these strategies apply
    the same cutoff to both tails of a sample, even if one tail is far
    longer than the other.
    
    One solution is to take the Double MAD. This is a pair of statistics:
    (1) the median absolute deviation from the median of all points less
    than or equal to the median and
    (2) the median absolute deviation from the median of all points greater
    than or equal to the median.
    
    Use the first of these to calculate the distance
    from the median of all points less than or equal to the median;
    use the second to calculate that distance for points that are greater than
    the median.
    
    If more than 50% of the data have identical values, MAD will equal zero.
    All points in the dataset, except those that equal the median, will then be flagged
    as outliers, regardless of the outlier cutoff level.
    
    When performing anomaly detection you often want not only to predict
    the class label, but also obtain a probability of the respective label.
    Although anomaly detection is unsupervised, we can use outlier detection
    on the uncalibrated anomaly scores to generate anomaly labels. After
    generating the labels the scores can be calibrated using Platt's
    method as for a classifier. This probability gives you some kind of
    confidence on the prediction. It effectively tells you the probability
    that the raw anomaly score is an outlier, given the distribution of
    scores seen in the training set.
    
    Parameters
    ----------
    expected_outlier_fraction (eof) : float (default='0.1')
        Amount of contamination of the dataset, i.e., the proportion of
        outliers in the dataset (""Expected Outlier Fraction"").
        Used when fitting to define the threshold on the decision function.
        ``values: [0.01,0.25]``
    thresh (th) : float (default='3.5')
        Distance to use as the threshold. Points with a distance
        below the threshold are considered normal.
        ``values:[1.,50.]``
    
    References
    ----------
    .. [1] Leys, C., et al
       ""Detecting outliers: Do not use standard deviation around the mean, use
       absolute deviation around the median.""
       Journal of Experimental Social Psychology, Volume 49,
       Issue 4, July 2013, pp. 764-766
       `[link]
       <http://www.academia.edu/3448313/Detecting_outliers_Do_not_use_standard
       _deviations_around_the_mean_do_use_the_median_absolute_deviation_around
       _the_median>`__
    
    See Also
    --------",no url,Double Median Absolute Deviation with Calibration,ADDMAD_CAL,Anomaly Detection,no documentation retrieved,NUM,Double Median Absolute Deviation Anomaly Detection with Calibration,Anomaly Detection Univariate Double Median Absolute Deviation with calibration
Keras Autoencoder for Anomaly Detection,no url,Keras Autoencoder for Anomaly Detection,KERAS_AUTOENCODER,Anomaly Detection,no documentation retrieved,NUM,Keras Autoencoder,Keras Autoencoder for Anomaly Detection
Anomaly Detection Ranked Mahalanobis Distance with PCA and calibration,https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.mahalanobis.html,Anomaly Detection Ranked Mahalanobis Distance with PCA and calibration,ADMAHAL_PCA_CAL,Anomaly Detection,"



scipy.spatial.distance.mahalanobis — SciPy v0.14.0 Reference Guide




























Scipy.org
Docs
SciPy v0.14.0 Reference Guide
Spatial algorithms and data structures (scipy.spatial)
Distance computations (scipy.spatial.distance)



index


modules


next


previous










scipy.spatial.distance.mahalanobis¶


scipy.spatial.distance.mahalanobis(u, v, VI)[source]¶
Computes the Mahalanobis distance between two 1-D arrays.
The Mahalanobis distance between 1-D arrays u and v, is defined as


where V is the covariance matrix.  Note that the argument VI
is the inverse of V.




Parameters:u : (N,) array_like

Input array.

v : (N,) array_like

Input array.

VI : ndarray

The inverse of the covariance matrix.



Returns:mahalanobis : double

The Mahalanobis distance between vectors u and v.












Previous topic
scipy.spatial.distance.kulsinski
Next topic
scipy.spatial.distance.matching














        © Copyright 2008-2009, The Scipy community.
      

      Last updated on May 11, 2014.
      

      Created using Sphinx 1.2.2.
      





",NUM,Mahalanobis Distance Ranked Anomaly Detection with PCA and Calibration,Anomaly Detection Ranked Mahalanobis Distance with PCA and calibration
Keras Autoencoder for Anomaly Detection with Calibration,no url,Keras Autoencoder for Anomaly Detection with Calibration,KERAS_AUTOENCODER_CAL,Anomaly Detection,no documentation retrieved,NUM,Keras Autoencoder with Calibration,Keras Autoencoder for Anomaly Detection with Calibration
Anomaly Detection Isolation Forest,http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html,Anomaly Detection Isolation Forest,ADISOFOR,Anomaly Detection,"













IsolationForest — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.ensemble
IsolationForest









IsolationForest#


class sklearn.ensemble.IsolationForest(*, n_estimators=100, max_samples='auto', contamination='auto', max_features=1.0, bootstrap=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)[source]#
Isolation Forest Algorithm.
Return the anomaly score of each sample using the IsolationForest algorithm
The IsolationForest ‘isolates’ observations by randomly selecting a feature
and then randomly selecting a split value between the maximum and minimum
values of the selected feature.
Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.
This path length, averaged over a forest of such random trees, is a
measure of normality and our decision function.
Random partitioning produces noticeably shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path
lengths for particular samples, they are highly likely to be anomalies.
Read more in the User Guide.

Added in version 0.18.


Parameters:

n_estimatorsint, default=100The number of base estimators in the ensemble.

max_samples“auto”, int or float, default=”auto”
The number of samples to draw from X to train each base estimator.
If int, then draw max_samples samples.
If float, then draw max_samples * X.shape[0] samples.
If “auto”, then max_samples=min(256, n_samples).



If max_samples is larger than the number of samples provided,
all samples will be used for all trees (no sampling).

contamination‘auto’ or float, default=’auto’The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. Used when fitting to define the threshold
on the scores of the samples.


If ‘auto’, the threshold is determined as in the
original paper.
If float, the contamination should be in the range (0, 0.5].



Changed in version 0.22: The default value of contamination changed from 0.1
to 'auto'.


max_featuresint or float, default=1.0The number of features to draw from X to train each base estimator.


If int, then draw max_features features.
If float, then draw max(1, int(max_features * n_features_in_)) features.


Note: using a float number less than 1.0 or integer less than number of
features will enable feature subsampling and leads to a longer runtime.

bootstrapbool, default=FalseIf True, individual trees are fit on random subsets of the training
data sampled with replacement. If False, sampling without replacement
is performed.

n_jobsint, default=NoneThe number of jobs to run in parallel for both fit and
predict. None means 1 unless in a
joblib.parallel_backend context. -1 means using all
processors. See Glossary for more details.

random_stateint, RandomState instance or None, default=NoneControls the pseudo-randomness of the selection of the feature
and split values for each branching step and each tree in the forest.
Pass an int for reproducible results across multiple function calls.
See Glossary.

verboseint, default=0Controls the verbosity of the tree building process.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See the Glossary.

Added in version 0.21.




Attributes:

estimator_ExtraTreeRegressor instanceThe child estimator template used to create the collection of
fitted sub-estimators.

Added in version 1.2: base_estimator_ was renamed to estimator_.


estimators_list of ExtraTreeRegressor instancesThe collection of fitted sub-estimators.

estimators_features_list of ndarrayThe subset of drawn features for each base estimator.

estimators_samples_list of ndarrayThe subset of drawn samples for each base estimator.

max_samples_intThe actual number of samples.

offset_floatOffset used to define the decision function from the raw scores. We
have the relation: decision_function = score_samples - offset_.
offset_ is defined as follows. When the contamination parameter is
set to “auto”, the offset is equal to -0.5 as the scores of inliers are
close to 0 and the scores of outliers are close to -1. When a
contamination parameter different than “auto” is provided, the offset
is defined in such a way we obtain the expected number of outliers
(samples with decision function < 0) in training.

Added in version 0.20.


n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

sklearn.covariance.EllipticEnvelopeAn object for detecting outliers in a Gaussian distributed dataset.

sklearn.svm.OneClassSVMUnsupervised Outlier Detection. Estimate the support of a high-dimensional distribution. The implementation is based on libsvm.

sklearn.neighbors.LocalOutlierFactorUnsupervised Outlier Detection using Local Outlier Factor (LOF).



Notes
The implementation is based on an ensemble of ExtraTreeRegressor. The
maximum depth of each tree is set to ceil(log_2(n)) where
\(n\) is the number of samples used to build the tree
(see (Liu et al., 2008) for more details).
References


[1]
Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”
Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on.


[2]
Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation-based
anomaly detection.” ACM Transactions on Knowledge Discovery from
Data (TKDD) 6.1 (2012): 3.


Examples
>>> from sklearn.ensemble import IsolationForest
>>> X = [[-1.1], [0.3], [0.5], [100]]
>>> clf = IsolationForest(random_state=0).fit(X)
>>> clf.predict([[0.1], [0], [90]])
array([ 1,  1, -1])


For an example of using isolation forest for anomaly detection see
IsolationForest example.


decision_function(X)[source]#
Average anomaly score of X of the base classifiers.
The anomaly score of an input sample is computed as
the mean anomaly score of the trees in the forest.
The measure of normality of an observation given a tree is the depth
of the leaf containing this observation, which is equivalent to
the number of splittings required to isolate this point. In case of
several observations n_left in the leaf, the average path length of
a n_left samples isolation tree is added.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

scoresndarray of shape (n_samples,)The anomaly score of the input samples.
The lower, the more abnormal. Negative scores represent outliers,
positive scores represent inliers.







property estimators_samples_#
The subset of drawn samples for each base estimator.
Returns a dynamically generated list of indices identifying
the samples used for fitting each member of the ensemble, i.e.,
the in-bag samples.
Note: the list is re-created at each call to the property in order
to reduce the object memory footprint by not storing the sampling
data. Thus fetching the property may be slower than expected.



fit(X, y=None, sample_weight=None)[source]#
Fit estimator.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Use dtype=np.float32 for maximum
efficiency. Sparse matrices are also supported, use sparse
csc_matrix for maximum efficiency.

yIgnoredNot used, present for API consistency by convention.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights. If None, then samples are equally weighted.



Returns:

selfobjectFitted estimator.







fit_predict(X, y=None, **kwargs)[source]#
Perform fit on X and returns labels for X.
Returns -1 for outliers and 1 for inliers.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples.

yIgnoredNot used, present for API consistency by convention.

**kwargsdictArguments to be passed to fit.

Added in version 1.4.




Returns:

yndarray of shape (n_samples,)1 for inliers, -1 for outliers.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Added in version 1.5.


Returns:

routingMetadataRouterA MetadataRouter encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict if a particular sample is an outlier or not.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

is_inlierndarray of shape (n_samples,)For each observation, tells whether or not (+1 or -1) it should
be considered as an inlier according to the fitted model.







score_samples(X)[source]#
Opposite of the anomaly score defined in the original paper.
The anomaly score of an input sample is computed as
the mean anomaly score of the trees in the forest.
The measure of normality of an observation given a tree is the depth
of the leaf containing this observation, which is equivalent to
the number of splittings required to isolate this point. In case of
several observations n_left in the leaf, the average path length of
a n_left samples isolation tree is added.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples.



Returns:

scoresndarray of shape (n_samples,)The anomaly score of the input samples.
The lower, the more abnormal.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → IsolationForest[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







Gallery examples#

IsolationForest example
IsolationForest example

Comparing anomaly detection algorithms for outlier detection on toy datasets
Comparing anomaly detection algorithms for outlier detection on toy datasets

Evaluation of outlier detection estimators
Evaluation of outlier detection estimators










previous
HistGradientBoostingRegressor




next
RandomForestClassifier










 On this page
  


IsolationForest
decision_function
estimators_samples_
fit
fit_predict
get_metadata_routing
get_params
predict
score_samples
set_fit_request
set_params


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Isolation Forest Anomaly Detection,Anomaly Detection Isolation Forest
Anomaly Detection Isolation Forest with calibration,http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html,Anomaly Detection Isolation Forest with calibration,ADISOFOR_CAL,Anomaly Detection,"













IsolationForest — scikit-learn 1.5.2 documentation
















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

sklearn
config_context
get_config
set_config
show_versions


sklearn.base
BaseEstimator
BiclusterMixin
ClassNamePrefixFeaturesOutMixin
ClassifierMixin
ClusterMixin
DensityMixin
MetaEstimatorMixin
OneToOneFeatureMixin
OutlierMixin
RegressorMixin
TransformerMixin
clone
is_classifier
is_regressor


sklearn.calibration
CalibratedClassifierCV
calibration_curve
CalibrationDisplay


sklearn.cluster
AffinityPropagation
AgglomerativeClustering
Birch
BisectingKMeans
DBSCAN
FeatureAgglomeration
HDBSCAN
KMeans
MeanShift
MiniBatchKMeans
OPTICS
SpectralBiclustering
SpectralClustering
SpectralCoclustering
affinity_propagation
cluster_optics_dbscan
cluster_optics_xi
compute_optics_graph
dbscan
estimate_bandwidth
k_means
kmeans_plusplus
mean_shift
spectral_clustering
ward_tree


sklearn.compose
ColumnTransformer
TransformedTargetRegressor
make_column_selector
make_column_transformer


sklearn.covariance
EllipticEnvelope
EmpiricalCovariance
GraphicalLasso
GraphicalLassoCV
LedoitWolf
MinCovDet
OAS
ShrunkCovariance
empirical_covariance
graphical_lasso
ledoit_wolf
ledoit_wolf_shrinkage
oas
shrunk_covariance


sklearn.cross_decomposition
CCA
PLSCanonical
PLSRegression
PLSSVD


sklearn.datasets
clear_data_home
dump_svmlight_file
fetch_20newsgroups
fetch_20newsgroups_vectorized
fetch_california_housing
fetch_covtype
fetch_kddcup99
fetch_lfw_pairs
fetch_lfw_people
fetch_olivetti_faces
fetch_openml
fetch_rcv1
fetch_species_distributions
get_data_home
load_breast_cancer
load_diabetes
load_digits
load_files
load_iris
load_linnerud
load_sample_image
load_sample_images
load_svmlight_file
load_svmlight_files
load_wine
make_biclusters
make_blobs
make_checkerboard
make_circles
make_classification
make_friedman1
make_friedman2
make_friedman3
make_gaussian_quantiles
make_hastie_10_2
make_low_rank_matrix
make_moons
make_multilabel_classification
make_regression
make_s_curve
make_sparse_coded_signal
make_sparse_spd_matrix
make_sparse_uncorrelated
make_spd_matrix
make_swiss_roll


sklearn.decomposition
DictionaryLearning
FactorAnalysis
FastICA
IncrementalPCA
KernelPCA
LatentDirichletAllocation
MiniBatchDictionaryLearning
MiniBatchNMF
MiniBatchSparsePCA
NMF
PCA
SparseCoder
SparsePCA
TruncatedSVD
dict_learning
dict_learning_online
fastica
non_negative_factorization
sparse_encode


sklearn.discriminant_analysis
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis


sklearn.dummy
DummyClassifier
DummyRegressor


sklearn.ensemble
AdaBoostClassifier
AdaBoostRegressor
BaggingClassifier
BaggingRegressor
ExtraTreesClassifier
ExtraTreesRegressor
GradientBoostingClassifier
GradientBoostingRegressor
HistGradientBoostingClassifier
HistGradientBoostingRegressor
IsolationForest
RandomForestClassifier
RandomForestRegressor
RandomTreesEmbedding
StackingClassifier
StackingRegressor
VotingClassifier
VotingRegressor


sklearn.exceptions
ConvergenceWarning
DataConversionWarning
DataDimensionalityWarning
EfficiencyWarning
FitFailedWarning
InconsistentVersionWarning
NotFittedError
UndefinedMetricWarning


sklearn.experimental
enable_halving_search_cv
enable_iterative_imputer


sklearn.feature_extraction
DictVectorizer
FeatureHasher
PatchExtractor
extract_patches_2d
grid_to_graph
img_to_graph
reconstruct_from_patches_2d
CountVectorizer
HashingVectorizer
TfidfTransformer
TfidfVectorizer


sklearn.feature_selection
GenericUnivariateSelect
RFE
RFECV
SelectFdr
SelectFpr
SelectFromModel
SelectFwe
SelectKBest
SelectPercentile
SelectorMixin
SequentialFeatureSelector
VarianceThreshold
chi2
f_classif
f_regression
mutual_info_classif
mutual_info_regression
r_regression


sklearn.gaussian_process
GaussianProcessClassifier
GaussianProcessRegressor
CompoundKernel
ConstantKernel
DotProduct
ExpSineSquared
Exponentiation
Hyperparameter
Kernel
Matern
PairwiseKernel
Product
RBF
RationalQuadratic
Sum
WhiteKernel


sklearn.impute
IterativeImputer
KNNImputer
MissingIndicator
SimpleImputer


sklearn.inspection
partial_dependence
permutation_importance
DecisionBoundaryDisplay
PartialDependenceDisplay


sklearn.isotonic
IsotonicRegression
check_increasing
isotonic_regression


sklearn.kernel_approximation
AdditiveChi2Sampler
Nystroem
PolynomialCountSketch
RBFSampler
SkewedChi2Sampler


sklearn.kernel_ridge
KernelRidge


sklearn.linear_model
LogisticRegression
LogisticRegressionCV
PassiveAggressiveClassifier
Perceptron
RidgeClassifier
RidgeClassifierCV
SGDClassifier
SGDOneClassSVM
LinearRegression
Ridge
RidgeCV
SGDRegressor
ElasticNet
ElasticNetCV
Lars
LarsCV
Lasso
LassoCV
LassoLars
LassoLarsCV
LassoLarsIC
OrthogonalMatchingPursuit
OrthogonalMatchingPursuitCV
ARDRegression
BayesianRidge
MultiTaskElasticNet
MultiTaskElasticNetCV
MultiTaskLasso
MultiTaskLassoCV
HuberRegressor
QuantileRegressor
RANSACRegressor
TheilSenRegressor
GammaRegressor
PoissonRegressor
TweedieRegressor
PassiveAggressiveRegressor
enet_path
lars_path
lars_path_gram
lasso_path
orthogonal_mp
orthogonal_mp_gram
ridge_regression


sklearn.manifold
Isomap
LocallyLinearEmbedding
MDS
SpectralEmbedding
TSNE
locally_linear_embedding
smacof
spectral_embedding
trustworthiness


sklearn.metrics
check_scoring
get_scorer
get_scorer_names
make_scorer
accuracy_score
auc
average_precision_score
balanced_accuracy_score
brier_score_loss
class_likelihood_ratios
classification_report
cohen_kappa_score
confusion_matrix
d2_log_loss_score
dcg_score
det_curve
f1_score
fbeta_score
hamming_loss
hinge_loss
jaccard_score
log_loss
matthews_corrcoef
multilabel_confusion_matrix
ndcg_score
precision_recall_curve
precision_recall_fscore_support
precision_score
recall_score
roc_auc_score
roc_curve
top_k_accuracy_score
zero_one_loss
d2_absolute_error_score
d2_pinball_score
d2_tweedie_score
explained_variance_score
max_error
mean_absolute_error
mean_absolute_percentage_error
mean_gamma_deviance
mean_pinball_loss
mean_poisson_deviance
mean_squared_error
mean_squared_log_error
mean_tweedie_deviance
median_absolute_error
r2_score
root_mean_squared_error
root_mean_squared_log_error
coverage_error
label_ranking_average_precision_score
label_ranking_loss
adjusted_mutual_info_score
adjusted_rand_score
calinski_harabasz_score
contingency_matrix
pair_confusion_matrix
completeness_score
davies_bouldin_score
fowlkes_mallows_score
homogeneity_completeness_v_measure
homogeneity_score
mutual_info_score
normalized_mutual_info_score
rand_score
silhouette_samples
silhouette_score
v_measure_score
consensus_score
DistanceMetric
additive_chi2_kernel
chi2_kernel
cosine_distances
cosine_similarity
distance_metrics
euclidean_distances
haversine_distances
kernel_metrics
laplacian_kernel
linear_kernel
manhattan_distances
nan_euclidean_distances
paired_cosine_distances
paired_distances
paired_euclidean_distances
paired_manhattan_distances
pairwise_kernels
polynomial_kernel
rbf_kernel
sigmoid_kernel
pairwise_distances
pairwise_distances_argmin
pairwise_distances_argmin_min
pairwise_distances_chunked
ConfusionMatrixDisplay
DetCurveDisplay
PrecisionRecallDisplay
PredictionErrorDisplay
RocCurveDisplay


sklearn.mixture
BayesianGaussianMixture
GaussianMixture


sklearn.model_selection
GroupKFold
GroupShuffleSplit
KFold
LeaveOneGroupOut
LeaveOneOut
LeavePGroupsOut
LeavePOut
PredefinedSplit
RepeatedKFold
RepeatedStratifiedKFold
ShuffleSplit
StratifiedGroupKFold
StratifiedKFold
StratifiedShuffleSplit
TimeSeriesSplit
check_cv
train_test_split
GridSearchCV
HalvingGridSearchCV
HalvingRandomSearchCV
ParameterGrid
ParameterSampler
RandomizedSearchCV
FixedThresholdClassifier
TunedThresholdClassifierCV
cross_val_predict
cross_val_score
cross_validate
learning_curve
permutation_test_score
validation_curve
LearningCurveDisplay
ValidationCurveDisplay


sklearn.multiclass
OneVsOneClassifier
OneVsRestClassifier
OutputCodeClassifier


sklearn.multioutput
ClassifierChain
MultiOutputClassifier
MultiOutputRegressor
RegressorChain


sklearn.naive_bayes
BernoulliNB
CategoricalNB
ComplementNB
GaussianNB
MultinomialNB


sklearn.neighbors
BallTree
KDTree
KNeighborsClassifier
KNeighborsRegressor
KNeighborsTransformer
KernelDensity
LocalOutlierFactor
NearestCentroid
NearestNeighbors
NeighborhoodComponentsAnalysis
RadiusNeighborsClassifier
RadiusNeighborsRegressor
RadiusNeighborsTransformer
kneighbors_graph
radius_neighbors_graph
sort_graph_by_row_values


sklearn.neural_network
BernoulliRBM
MLPClassifier
MLPRegressor


sklearn.pipeline
FeatureUnion
Pipeline
make_pipeline
make_union


sklearn.preprocessing
Binarizer
FunctionTransformer
KBinsDiscretizer
KernelCenterer
LabelBinarizer
LabelEncoder
MaxAbsScaler
MinMaxScaler
MultiLabelBinarizer
Normalizer
OneHotEncoder
OrdinalEncoder
PolynomialFeatures
PowerTransformer
QuantileTransformer
RobustScaler
SplineTransformer
StandardScaler
TargetEncoder
add_dummy_feature
binarize
label_binarize
maxabs_scale
minmax_scale
normalize
power_transform
quantile_transform
robust_scale
scale


sklearn.random_projection
GaussianRandomProjection
SparseRandomProjection
johnson_lindenstrauss_min_dim


sklearn.semi_supervised
LabelPropagation
LabelSpreading
SelfTrainingClassifier


sklearn.svm
LinearSVC
LinearSVR
NuSVC
NuSVR
OneClassSVM
SVC
SVR
l1_min_c


sklearn.tree
DecisionTreeClassifier
DecisionTreeRegressor
ExtraTreeClassifier
ExtraTreeRegressor
export_graphviz
export_text
plot_tree


sklearn.utils
Bunch
_safe_indexing
as_float_array
assert_all_finite
deprecated
estimator_html_repr
gen_batches
gen_even_slices
indexable
murmurhash3_32
resample
safe_mask
safe_sqr
shuffle
check_X_y
check_array
check_consistent_length
check_random_state
check_scalar
check_is_fitted
check_memory
check_symmetric
column_or_1d
has_fit_parameter
available_if
compute_class_weight
compute_sample_weight
is_multilabel
type_of_target
unique_labels
density
fast_logdet
randomized_range_finder
randomized_svd
safe_sparse_dot
weighted_mode
incr_mean_variance_axis
inplace_column_scale
inplace_csr_column_scale
inplace_row_scale
inplace_swap_column
inplace_swap_row
mean_variance_axis
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l2
single_source_shortest_path_length
sample_without_replacement
min_pos
MetadataRequest
MetadataRouter
MethodMapping
get_routing_for_object
process_routing
all_displays
all_estimators
all_functions
check_estimator
parametrize_with_checks
Parallel
delayed


Recently Deprecated
parallel_backend
register_parallel_backend
























API Reference
sklearn.ensemble
IsolationForest









IsolationForest#


class sklearn.ensemble.IsolationForest(*, n_estimators=100, max_samples='auto', contamination='auto', max_features=1.0, bootstrap=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)[source]#
Isolation Forest Algorithm.
Return the anomaly score of each sample using the IsolationForest algorithm
The IsolationForest ‘isolates’ observations by randomly selecting a feature
and then randomly selecting a split value between the maximum and minimum
values of the selected feature.
Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.
This path length, averaged over a forest of such random trees, is a
measure of normality and our decision function.
Random partitioning produces noticeably shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path
lengths for particular samples, they are highly likely to be anomalies.
Read more in the User Guide.

Added in version 0.18.


Parameters:

n_estimatorsint, default=100The number of base estimators in the ensemble.

max_samples“auto”, int or float, default=”auto”
The number of samples to draw from X to train each base estimator.
If int, then draw max_samples samples.
If float, then draw max_samples * X.shape[0] samples.
If “auto”, then max_samples=min(256, n_samples).



If max_samples is larger than the number of samples provided,
all samples will be used for all trees (no sampling).

contamination‘auto’ or float, default=’auto’The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. Used when fitting to define the threshold
on the scores of the samples.


If ‘auto’, the threshold is determined as in the
original paper.
If float, the contamination should be in the range (0, 0.5].



Changed in version 0.22: The default value of contamination changed from 0.1
to 'auto'.


max_featuresint or float, default=1.0The number of features to draw from X to train each base estimator.


If int, then draw max_features features.
If float, then draw max(1, int(max_features * n_features_in_)) features.


Note: using a float number less than 1.0 or integer less than number of
features will enable feature subsampling and leads to a longer runtime.

bootstrapbool, default=FalseIf True, individual trees are fit on random subsets of the training
data sampled with replacement. If False, sampling without replacement
is performed.

n_jobsint, default=NoneThe number of jobs to run in parallel for both fit and
predict. None means 1 unless in a
joblib.parallel_backend context. -1 means using all
processors. See Glossary for more details.

random_stateint, RandomState instance or None, default=NoneControls the pseudo-randomness of the selection of the feature
and split values for each branching step and each tree in the forest.
Pass an int for reproducible results across multiple function calls.
See Glossary.

verboseint, default=0Controls the verbosity of the tree building process.

warm_startbool, default=FalseWhen set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See the Glossary.

Added in version 0.21.




Attributes:

estimator_ExtraTreeRegressor instanceThe child estimator template used to create the collection of
fitted sub-estimators.

Added in version 1.2: base_estimator_ was renamed to estimator_.


estimators_list of ExtraTreeRegressor instancesThe collection of fitted sub-estimators.

estimators_features_list of ndarrayThe subset of drawn features for each base estimator.

estimators_samples_list of ndarrayThe subset of drawn samples for each base estimator.

max_samples_intThe actual number of samples.

offset_floatOffset used to define the decision function from the raw scores. We
have the relation: decision_function = score_samples - offset_.
offset_ is defined as follows. When the contamination parameter is
set to “auto”, the offset is equal to -0.5 as the scores of inliers are
close to 0 and the scores of outliers are close to -1. When a
contamination parameter different than “auto” is provided, the offset
is defined in such a way we obtain the expected number of outliers
(samples with decision function < 0) in training.

Added in version 0.20.


n_features_in_intNumber of features seen during fit.

Added in version 0.24.


feature_names_in_ndarray of shape (n_features_in_,)Names of features seen during fit. Defined only when X
has feature names that are all strings.

Added in version 1.0.






See also

sklearn.covariance.EllipticEnvelopeAn object for detecting outliers in a Gaussian distributed dataset.

sklearn.svm.OneClassSVMUnsupervised Outlier Detection. Estimate the support of a high-dimensional distribution. The implementation is based on libsvm.

sklearn.neighbors.LocalOutlierFactorUnsupervised Outlier Detection using Local Outlier Factor (LOF).



Notes
The implementation is based on an ensemble of ExtraTreeRegressor. The
maximum depth of each tree is set to ceil(log_2(n)) where
\(n\) is the number of samples used to build the tree
(see (Liu et al., 2008) for more details).
References


[1]
Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”
Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on.


[2]
Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation-based
anomaly detection.” ACM Transactions on Knowledge Discovery from
Data (TKDD) 6.1 (2012): 3.


Examples
>>> from sklearn.ensemble import IsolationForest
>>> X = [[-1.1], [0.3], [0.5], [100]]
>>> clf = IsolationForest(random_state=0).fit(X)
>>> clf.predict([[0.1], [0], [90]])
array([ 1,  1, -1])


For an example of using isolation forest for anomaly detection see
IsolationForest example.


decision_function(X)[source]#
Average anomaly score of X of the base classifiers.
The anomaly score of an input sample is computed as
the mean anomaly score of the trees in the forest.
The measure of normality of an observation given a tree is the depth
of the leaf containing this observation, which is equivalent to
the number of splittings required to isolate this point. In case of
several observations n_left in the leaf, the average path length of
a n_left samples isolation tree is added.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

scoresndarray of shape (n_samples,)The anomaly score of the input samples.
The lower, the more abnormal. Negative scores represent outliers,
positive scores represent inliers.







property estimators_samples_#
The subset of drawn samples for each base estimator.
Returns a dynamically generated list of indices identifying
the samples used for fitting each member of the ensemble, i.e.,
the in-bag samples.
Note: the list is re-created at each call to the property in order
to reduce the object memory footprint by not storing the sampling
data. Thus fetching the property may be slower than expected.



fit(X, y=None, sample_weight=None)[source]#
Fit estimator.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Use dtype=np.float32 for maximum
efficiency. Sparse matrices are also supported, use sparse
csc_matrix for maximum efficiency.

yIgnoredNot used, present for API consistency by convention.

sample_weightarray-like of shape (n_samples,), default=NoneSample weights. If None, then samples are equally weighted.



Returns:

selfobjectFitted estimator.







fit_predict(X, y=None, **kwargs)[source]#
Perform fit on X and returns labels for X.
Returns -1 for outliers and 1 for inliers.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples.

yIgnoredNot used, present for API consistency by convention.

**kwargsdictArguments to be passed to fit.

Added in version 1.4.




Returns:

yndarray of shape (n_samples,)1 for inliers, -1 for outliers.







get_metadata_routing()[source]#
Get metadata routing of this object.
Please check User Guide on how the routing
mechanism works.

Added in version 1.5.


Returns:

routingMetadataRouterA MetadataRouter encapsulating
routing information.







get_params(deep=True)[source]#
Get parameters for this estimator.

Parameters:

deepbool, default=TrueIf True, will return the parameters for this estimator and
contained subobjects that are estimators.



Returns:

paramsdictParameter names mapped to their values.







predict(X)[source]#
Predict if a particular sample is an outlier or not.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples. Internally, it will be converted to
dtype=np.float32 and if a sparse matrix is provided
to a sparse csr_matrix.



Returns:

is_inlierndarray of shape (n_samples,)For each observation, tells whether or not (+1 or -1) it should
be considered as an inlier according to the fitted model.







score_samples(X)[source]#
Opposite of the anomaly score defined in the original paper.
The anomaly score of an input sample is computed as
the mean anomaly score of the trees in the forest.
The measure of normality of an observation given a tree is the depth
of the leaf containing this observation, which is equivalent to
the number of splittings required to isolate this point. In case of
several observations n_left in the leaf, the average path length of
a n_left samples isolation tree is added.

Parameters:

X{array-like, sparse matrix} of shape (n_samples, n_features)The input samples.



Returns:

scoresndarray of shape (n_samples,)The anomaly score of the input samples.
The lower, the more abnormal.







set_fit_request(*, sample_weight: bool | None | str = '$UNCHANGED$') → IsolationForest[source]#
Request metadata passed to the fit method.
Note that this method is only relevant if
enable_metadata_routing=True (see sklearn.set_config).
Please see User Guide on how the routing
mechanism works.
The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.
False: metadata is not requested and the meta-estimator will not pass it to fit.
None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the
existing request. This allows you to change the request for some
parameters and not others.

Added in version 1.3.


Note
This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
Pipeline. Otherwise it has no effect.


Parameters:

sample_weightstr, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGEDMetadata routing for sample_weight parameter in fit.



Returns:

selfobjectThe updated object.







set_params(**params)[source]#
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as Pipeline). The latter have
parameters of the form <component>__<parameter> so that it’s
possible to update each component of a nested object.

Parameters:

**paramsdictEstimator parameters.



Returns:

selfestimator instanceEstimator instance.







Gallery examples#

IsolationForest example
IsolationForest example

Comparing anomaly detection algorithms for outlier detection on toy datasets
Comparing anomaly detection algorithms for outlier detection on toy datasets

Evaluation of outlier detection estimators
Evaluation of outlier detection estimators










previous
HistGradientBoostingRegressor




next
RandomForestClassifier










 On this page
  


IsolationForest
decision_function
estimators_samples_
fit
fit_predict
get_metadata_routing
get_params
predict
score_samples
set_fit_request
set_params


Gallery examples





 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,Isolation Forest Anomaly Detection with Calibration,Anomaly Detection Isolation Forest with calibration
Keras Variational Autoencoder for Anomaly Detection with Calibration,no url,Keras Variational Autoencoder for Anomaly Detection with Calibration,KERAS_VARIATIONAL_AUTOENCODER_CAL,Anomaly Detection,no documentation retrieved,NUM,Keras Variational Autoencoder with Calibration,Keras Variational Autoencoder for Anomaly Detection with Calibration
"One Class Support Vector Machine with Calibration
    
    One-class SVM is used for novelty detection. That is, given a set
    of samples, it will detect the soft boundary of that set so as to
    classify new points as belonging to that set or not. This is similar
    to SVM applied to binary classification, where the algorithm identifies
    a boundary to separate two classes with a maximum margin. In the
    case of one-class SVM, the training set is assumed to consist largely of
    normal data of one class and the algorithm will identify a boundary that
    encapsulates a majority of the training points. Points lying outside this
    boundary are considered anomalous. This implementation is based on `libsvm`.
    
    When performing anomaly detection you often want not only to predict
    the class label, but also obtain a probability of the respective label.
    Although anomaly detection is unsupervised, we can use outlier detection
    on the uncalibrated anomaly scores to generate anomaly labels. After
    generating the labels the scores can be calibrated using Platt's
    method as for a classifier. This probability gives you some kind of
    confidence on the prediction. It effectively tells you the probability
    that the raw anomaly score is an outlier, given the distribution of
    scores seen in the training set.
    
    Parameters
    ----------
    expected_outlier_fraction (eof) : float (default='0.1')
        Amount of contamination of the dataset, i.e., the proportion of
        outliers in the dataset (""Expected Outlier Fraction"").
        Used when fitting to define the threshold on the decision function.
        ``values: [0.01,0.25]``
    
        nu is set by ``0.95 * expected_outlier_fraction + 0.05``
    kernel (kl) : select (default='rbf')
        Kernel type used in the algorithm.
        ``values: ['linear', 'poly', 'rbf']``
    degree (dg) : int (default='3')
        Degree of the polynomial kernel function (when kernel = 'poly').
        Ignored by all other kernels.
        ``values: [1, 10]``
    coef0 (co) : float (default='0')
        Independent term in kernel function.
        Applicable only when kernel is set to 'poly' or 'sigmoid'.
        ``values: [1e-10, 1e10]``
    shrinking (sh) : bool (default='True')
        When True (the default), uses the shrinking heuristic.
        ``values: [False, True]``
    tol (t) : float (default='None')
        Tolerance for stopping criterion.
        ``values: [1e-10, 1e10]``
    max_iter (mi) : int (default='-1')
        Hard limit on iterations within solver, or -1 for no limit.
        ``values: [-1, 999]``
    random_state (rs) : int (default='369')
        Seed of the pseudo-random number generator to use when
        shuffling the data for probability estimation.
        ``values: [0, int(1e10)]``
    gamma (gm) : float (default='1e-5')
        Coefficient when kernel is set to 'rbf', 'poly', or 'sigmoid'.
        If gamma is 'auto' ,then 1/n_features will be used instead.
        ``values: [0, int(1e-5)]``
    
    References
    ----------
    .. [1] Chang, Chih-Chung, and Chih-Jen Lin. ""LIBSVM: A library for support
       vector machines."" ACM Transactions on Intelligent Systems and Technology
       (TIST) 2.3 (2011): 27. `[link] <http://ntucsu.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`__
    
    See Also
    --------
    Source:
        `sklearn.svm.OneClassSVM
        <http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html>`_",no url,One Class Support Vector Machine with Calibration,ADOSVM_CAL,Anomaly Detection,no documentation retrieved,NUM,One-Class SVM Anomaly Detection with Calibration,Anomaly Detection One Class SVM with calibration
K-Means Clustering as a multiclass estimator. Will be changed to unsupervised later.,http://scikit-learn.org/stable/modules/clustering.html#k-means,K-Means Clustering as a multiclass estimator,KMEANS_CL,Clustering,"













2.3. Clustering — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

1. Supervised learning
1.1. Linear Models
1.2. Linear and Quadratic Discriminant Analysis
1.3. Kernel ridge regression
1.4. Support Vector Machines
1.5. Stochastic Gradient Descent
1.6. Nearest Neighbors
1.7. Gaussian Processes
1.8. Cross decomposition
1.9. Naive Bayes
1.10. Decision Trees
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking
1.12. Multiclass and multioutput algorithms
1.13. Feature selection
1.14. Semi-supervised learning
1.15. Isotonic regression
1.16. Probability calibration
1.17. Neural network models (supervised)


2. Unsupervised learning
2.1. Gaussian mixture models
2.2. Manifold learning
2.3. Clustering
2.4. Biclustering
2.5. Decomposing signals in components (matrix factorization problems)
2.6. Covariance estimation
2.7. Novelty and Outlier Detection
2.8. Density Estimation
2.9. Neural network models (unsupervised)


3. Model selection and evaluation
3.1. Cross-validation: evaluating estimator performance
3.2. Tuning the hyper-parameters of an estimator
3.3. Tuning the decision threshold for class prediction
3.4. Metrics and scoring: quantifying the quality of predictions
3.5. Validation curves: plotting scores to evaluate models


4. Inspection
4.1. Partial Dependence and Individual Conditional Expectation plots
4.2. Permutation feature importance


5. Visualizations
6. Dataset transformations
6.1. Pipelines and composite estimators
6.2. Feature extraction
6.3. Preprocessing data
6.4. Imputation of missing values
6.5. Unsupervised dimensionality reduction
6.6. Random Projection
6.7. Kernel Approximation
6.8. Pairwise metrics, Affinities and Kernels
6.9. Transforming the prediction target (y)


7. Dataset loading utilities
7.1. Toy datasets
7.2. Real world datasets
7.3. Generated datasets
7.4. Loading other datasets


8. Computing with scikit-learn
8.1. Strategies to scale computationally: bigger data
8.2. Computational Performance
8.3. Parallelism, resource management, and configuration


9. Model persistence
10. Common pitfalls and recommended practices
11. Dispatching
11.1. Array API support (experimental)


12. Choosing the right estimator
13. External Resources, Videos and Talks






















User Guide
2. Unsupervised learning










2.3. Clustering#
Clustering of
unlabeled data can be performed with the module sklearn.cluster.
Each clustering algorithm comes in two variants: a class, that implements
the fit method to learn the clusters on train data, and a function,
that, given train data, returns an array of integer labels corresponding
to the different clusters. For the class, the labels over the training
data can be found in the labels_ attribute.

Input data
One important thing to note is that the algorithms implemented in
this module can take different kinds of matrix as input. All the
methods accept standard data matrices of shape (n_samples, n_features).
These can be obtained from the classes in the sklearn.feature_extraction
module. For AffinityPropagation, SpectralClustering
and DBSCAN one can also input similarity matrices of shape
(n_samples, n_samples). These can be obtained from the functions
in the sklearn.metrics.pairwise module.


2.3.1. Overview of clustering methods#




A comparison of the clustering algorithms in scikit-learn#











Method name
Parameters
Scalability
Usecase
Geometry (metric used)



K-Means
number of clusters
Very large n_samples, medium n_clusters with
MiniBatch code
General-purpose, even cluster size, flat geometry,
not too many clusters, inductive
Distances between points

Affinity propagation
damping, sample preference
Not scalable with n_samples
Many clusters, uneven cluster size, non-flat geometry, inductive
Graph distance (e.g. nearest-neighbor graph)

Mean-shift
bandwidth
Not scalable with n_samples
Many clusters, uneven cluster size, non-flat geometry, inductive
Distances between points

Spectral clustering
number of clusters
Medium n_samples, small n_clusters
Few clusters, even cluster size, non-flat geometry, transductive
Graph distance (e.g. nearest-neighbor graph)

Ward hierarchical clustering
number of clusters or distance threshold
Large n_samples and n_clusters
Many clusters, possibly connectivity constraints, transductive
Distances between points

Agglomerative clustering
number of clusters or distance threshold, linkage type, distance
Large n_samples and n_clusters
Many clusters, possibly connectivity constraints, non Euclidean
distances, transductive
Any pairwise distance

DBSCAN
neighborhood size
Very large n_samples, medium n_clusters
Non-flat geometry, uneven cluster sizes, outlier removal,
transductive
Distances between nearest points

HDBSCAN
minimum cluster membership, minimum point neighbors
large n_samples, medium n_clusters
Non-flat geometry, uneven cluster sizes, outlier removal,
transductive, hierarchical, variable cluster density
Distances between nearest points

OPTICS
minimum cluster membership
Very large n_samples, large n_clusters
Non-flat geometry, uneven cluster sizes, variable cluster density,
outlier removal, transductive
Distances between points

Gaussian mixtures
many
Not scalable
Flat geometry, good for density estimation, inductive
Mahalanobis distances to  centers

BIRCH
branching factor, threshold, optional global clusterer.
Large n_clusters and n_samples
Large dataset, outlier removal, data reduction, inductive
Euclidean distance between points

Bisecting K-Means
number of clusters
Very large n_samples, medium n_clusters
General-purpose, even cluster size, flat geometry,
no empty clusters, inductive, hierarchical
Distances between points




Non-flat geometry clustering is useful when the clusters have a specific
shape, i.e. a non-flat manifold, and the standard euclidean distance is
not the right metric. This case arises in the two top rows of the figure
above.
Gaussian mixture models, useful for clustering, are described in
another chapter of the documentation dedicated to
mixture models. KMeans can be seen as a special case of Gaussian mixture
model with equal covariance per component.
Transductive clustering methods (in contrast to
inductive clustering methods) are not designed to be applied to new,
unseen data.


2.3.2. K-means#
The KMeans algorithm clusters data by trying to separate samples in n
groups of equal variance, minimizing a criterion known as the inertia or
within-cluster sum-of-squares (see below). This algorithm requires the number
of clusters to be specified. It scales well to large numbers of samples and has
been used across a large range of application areas in many different fields.
The k-means algorithm divides a set of \(N\) samples \(X\) into
\(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\)
of the samples in the cluster. The means are commonly called the cluster
“centroids”; note that they are not, in general, points from \(X\),
although they live in the same space.
The K-means algorithm aims to choose centroids that minimise the inertia,
or within-cluster sum-of-squares criterion:

\[\sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2)\]
Inertia can be recognized as a measure of how internally coherent clusters are.
It suffers from various drawbacks:

Inertia makes the assumption that clusters are convex and isotropic,
which is not always the case. It responds poorly to elongated clusters,
or manifolds with irregular shapes.
Inertia is not a normalized metric: we just know that lower values are
better and zero is optimal. But in very high-dimensional spaces, Euclidean
distances tend to become inflated
(this is an instance of the so-called “curse of dimensionality”).
Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to
k-means clustering can alleviate this problem and speed up the
computations.



For more detailed descriptions of the issues shown above and how to address them,
refer to the examples Demonstration of k-means assumptions
and Selecting the number of clusters with silhouette analysis on KMeans clustering.
K-means is often referred to as Lloyd’s algorithm. In basic terms, the
algorithm has three steps. The first step chooses the initial centroids, with
the most basic method being to choose \(k\) samples from the dataset
\(X\). After initialization, K-means consists of looping between the
two other steps. The first step assigns each sample to its nearest centroid.
The second step creates new centroids by taking the mean value of all of the
samples assigned to each previous centroid. The difference between the old
and the new centroids are computed and the algorithm repeats these last two
steps until this value is less than a threshold. In other words, it repeats
until the centroids do not move significantly.


K-means is equivalent to the expectation-maximization algorithm
with a small, all-equal, diagonal covariance matrix.
The algorithm can also be understood through the concept of Voronoi diagrams. First the Voronoi diagram of
the points is calculated using the current centroids. Each segment in the
Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated
to the mean of each segment. The algorithm then repeats this until a stopping
criterion is fulfilled. Usually, the algorithm stops when the relative decrease
in the objective function between iterations is less than the given tolerance
value. This is not the case in this implementation: iteration stops when
centroids move less than the tolerance.
Given enough time, K-means will always converge, however this may be to a local
minimum. This is highly dependent on the initialization of the centroids.
As a result, the computation is often done several times, with different
initializations of the centroids. One method to help address this issue is the
k-means++ initialization scheme, which has been implemented in scikit-learn
(use the init='k-means++' parameter). This initializes the centroids to be
(generally) distant from each other, leading to probably better results than
random initialization, as shown in the reference. For a detailed example of
comaparing different initialization schemes, refer to
A demo of K-Means clustering on the handwritten digits data.
K-means++ can also be called independently to select seeds for other
clustering algorithms, see sklearn.cluster.kmeans_plusplus for details
and example usage.
The algorithm supports sample weights, which can be given by a parameter
sample_weight. This allows to assign more weight to some samples when
computing cluster centers and values of inertia. For example, assigning a
weight of 2 to a sample is equivalent to adding a duplicate of that sample
to the dataset \(X\).
K-means can be used for vector quantization. This is achieved using the
transform method of a trained model of KMeans. For an example of
performing vector quantization on an image refer to
Color Quantization using K-Means.
Examples

K-means Clustering: Example usage of
KMeans using the iris dataset
Clustering text documents using k-means: Document clustering
using KMeans and MiniBatchKMeans based on sparse data


2.3.2.1. Low-level parallelism#
KMeans benefits from OpenMP based parallelism through Cython. Small
chunks of data (256 samples) are processed in parallel, which in addition
yields a low memory footprint. For more details on how to control the number of
threads, please refer to our Parallelism notes.
Examples

Demonstration of k-means assumptions: Demonstrating when
k-means performs intuitively and when it does not
A demo of K-Means clustering on the handwritten digits data: Clustering handwritten digits



References#

“k-means++: The advantages of careful seeding”
Arthur, David, and Sergei Vassilvitskii,
Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
algorithms, Society for Industrial and Applied Mathematics (2007)




2.3.2.2. Mini Batch K-Means#
The MiniBatchKMeans is a variant of the KMeans algorithm
which uses mini-batches to reduce the computation time, while still attempting
to optimise the same objective function. Mini-batches are subsets of the input
data, randomly sampled in each training iteration. These mini-batches
drastically reduce the amount of computation required to converge to a local
solution. In contrast to other algorithms that reduce the convergence time of
k-means, mini-batch k-means produces results that are generally only slightly
worse than the standard algorithm.
The algorithm iterates between two major steps, similar to vanilla k-means.
In the first step, \(b\) samples are drawn randomly from the dataset, to form
a mini-batch. These are then assigned to the nearest centroid. In the second
step, the centroids are updated. In contrast to k-means, this is done on a
per-sample basis. For each sample in the mini-batch, the assigned centroid
is updated by taking the streaming average of the sample and all previous
samples assigned to that centroid. This has the effect of decreasing the
rate of change for a centroid over time. These steps are performed until
convergence or a predetermined number of iterations is reached.
MiniBatchKMeans converges faster than KMeans, but the quality
of the results is reduced. In practice this difference in quality can be quite
small, as shown in the example and cited reference.




Examples

Comparison of the K-Means and MiniBatchKMeans clustering algorithms: Comparison of
KMeans and MiniBatchKMeans
Clustering text documents using k-means: Document clustering
using KMeans and MiniBatchKMeans based on sparse data
Online learning of a dictionary of parts of faces



References#

“Web Scale K-Means clustering”
D. Sculley, Proceedings of the 19th international conference on World
wide web (2010)





2.3.3. Affinity Propagation#
AffinityPropagation creates clusters by sending messages between
pairs of samples until convergence. A dataset is then described using a small
number of exemplars, which are identified as those most representative of other
samples. The messages sent between pairs represent the suitability for one
sample to be the exemplar of the other, which is updated in response to the
values from other pairs. This updating happens iteratively until convergence,
at which point the final exemplars are chosen, and hence the final clustering
is given.




Affinity Propagation can be interesting as it chooses the number of
clusters based on the data provided. For this purpose, the two important
parameters are the preference, which controls how many exemplars are
used, and the damping factor which damps the responsibility and
availability messages to avoid numerical oscillations when updating these
messages.
The main drawback of Affinity Propagation is its complexity. The
algorithm has a time complexity of the order \(O(N^2 T)\), where \(N\)
is the number of samples and \(T\) is the number of iterations until
convergence. Further, the memory complexity is of the order
\(O(N^2)\) if a dense similarity matrix is used, but reducible if a
sparse similarity matrix is used. This makes Affinity Propagation most
appropriate for small to medium sized datasets.


Algorithm description#
The messages sent between points belong to one of two categories. The first is
the responsibility \(r(i, k)\), which is the accumulated evidence that
sample \(k\) should be the exemplar for sample \(i\). The second is the
availability \(a(i, k)\) which is the accumulated evidence that sample
\(i\) should choose sample \(k\) to be its exemplar, and considers the
values for all other samples that \(k\) should be an exemplar. In this way,
exemplars are chosen by samples if they are (1) similar enough to many samples
and (2) chosen by many samples to be representative of themselves.
More formally, the responsibility of a sample \(k\) to be the exemplar of
sample \(i\) is given by:

\[r(i, k) \leftarrow s(i, k) - max [ a(i, k') + s(i, k') \forall k' \neq k ]\]
Where \(s(i, k)\) is the similarity between samples \(i\) and \(k\).
The availability of sample \(k\) to be the exemplar of sample \(i\) is
given by:

\[a(i, k) \leftarrow min [0, r(k, k) + \sum_{i'~s.t.~i' \notin \{i, k\}}{r(i',
k)}]\]
To begin with, all values for \(r\) and \(a\) are set to zero, and the
calculation of each iterates until convergence. As discussed above, in order to
avoid numerical oscillations when updating the messages, the damping factor
\(\lambda\) is introduced to iteration process:

\[r_{t+1}(i, k) = \lambda\cdot r_{t}(i, k) + (1-\lambda)\cdot r_{t+1}(i, k)\]

\[a_{t+1}(i, k) = \lambda\cdot a_{t}(i, k) + (1-\lambda)\cdot a_{t+1}(i, k)\]
where \(t\) indicates the iteration times.

Examples

Demo of affinity propagation clustering algorithm: Affinity
Propagation on a synthetic 2D datasets with 3 classes
Visualizing the stock market structure Affinity Propagation
on financial time series to find groups of companies



2.3.4. Mean Shift#
MeanShift clustering aims to discover blobs in a smooth density of
samples. It is a centroid based algorithm, which works by updating candidates
for centroids to be the mean of the points within a given region. These
candidates are then filtered in a post-processing stage to eliminate
near-duplicates to form the final set of centroids.


Mathematical details#
The position of centroid candidates is iteratively adjusted using a technique
called hill climbing, which finds local maxima of the estimated probability
density. Given a candidate centroid \(x\) for iteration \(t\), the
candidate is updated according to the following equation:

\[x^{t+1} = x^t + m(x^t)\]
Where \(m\) is the mean shift vector that is computed for each centroid
that points towards a region of the maximum increase in the density of points.
To compute \(m\) we define \(N(x)\) as the neighborhood of samples
within a given distance around \(x\). Then \(m\) is computed using the
following equation, effectively updating a centroid to be the mean of the
samples within its neighborhood:

\[m(x) = \frac{1}{|N(x)|} \sum_{x_j \in N(x)}x_j - x\]
In general, the equation for \(m\) depends on a kernel used for density
estimation. The generic formula is:

\[m(x) = \frac{\sum_{x_j \in N(x)}K(x_j - x)x_j}{\sum_{x_j \in N(x)}K(x_j -
x)} - x\]
In our implementation, \(K(x)\) is equal to 1 if \(x\) is small enough
and is equal to 0 otherwise. Effectively \(K(y - x)\) indicates whether
\(y\) is in the neighborhood of \(x\).

The algorithm automatically sets the number of clusters, instead of relying on a
parameter bandwidth, which dictates the size of the region to search through.
This parameter can be set manually, but can be estimated using the provided
estimate_bandwidth function, which is called if the bandwidth is not set.
The algorithm is not highly scalable, as it requires multiple nearest neighbor
searches during the execution of the algorithm. The algorithm is guaranteed to
converge, however the algorithm will stop iterating when the change in centroids
is small.
Labelling a new sample is performed by finding the nearest centroid for a
given sample.




Examples

A demo of the mean-shift clustering algorithm: Mean Shift clustering
on a synthetic 2D datasets with 3 classes.



References#

“Mean shift: A robust approach toward feature space analysis” D. Comaniciu and P. Meer, IEEE Transactions on Pattern
Analysis and Machine Intelligence (2002)




2.3.5. Spectral clustering#
SpectralClustering performs a low-dimension embedding of the
affinity matrix between samples, followed by clustering, e.g., by KMeans,
of the components of the eigenvectors in the low dimensional space.
It is especially computationally efficient if the affinity matrix is sparse
and the amg solver is used for the eigenvalue problem (Note, the amg solver
requires that the pyamg module is installed.)
The present version of SpectralClustering requires the number of clusters
to be specified in advance. It works well for a small number of clusters,
but is not advised for many clusters.
For two clusters, SpectralClustering solves a convex relaxation of the
normalized cuts
problem on the similarity graph: cutting the graph in two so that the weight of
the edges cut is small compared to the weights of the edges inside each
cluster. This criteria is especially interesting when working on images, where
graph vertices are pixels, and weights of the edges of the similarity graph are
computed using a function of a gradient of the image.

 
Warning
Transforming distance to well-behaved similarities
Note that if the values of your similarity matrix are not well
distributed, e.g. with negative values or with a distance matrix
rather than a similarity, the spectral problem will be singular and
the problem not solvable. In which case it is advised to apply a
transformation to the entries of the matrix. For instance, in the
case of a signed distance matrix, is common to apply a heat kernel:
similarity = np.exp(-beta * distance / distance.std())


See the examples for such an application.

Examples

Spectral clustering for image segmentation: Segmenting objects
from a noisy background using spectral clustering.
Segmenting the picture of greek coins in regions: Spectral clustering
to split the image of coins in regions.


2.3.5.1. Different label assignment strategies#
Different label assignment strategies can be used, corresponding to the
assign_labels parameter of SpectralClustering.
""kmeans"" strategy can match finer details, but can be unstable.
In particular, unless you control the random_state, it may not be
reproducible from run-to-run, as it depends on random initialization.
The alternative ""discretize"" strategy is 100% reproducible, but tends
to create parcels of fairly even and geometrical shape.
The recently added ""cluster_qr"" option is a deterministic alternative that
tends to create the visually best partitioning on the example application
below.


assign_labels=""kmeans""
assign_labels=""discretize""
assign_labels=""cluster_qr""












References#

“Multiclass spectral clustering”
Stella X. Yu, Jianbo Shi, 2003
“Simple, direct, and efficient multi-way spectral clustering”
Anil Damle, Victor Minden, Lexing Ying, 2019




2.3.5.2. Spectral Clustering Graphs#
Spectral Clustering can also be used to partition graphs via their spectral
embeddings.  In this case, the affinity matrix is the adjacency matrix of the
graph, and SpectralClustering is initialized with affinity='precomputed':
>>> from sklearn.cluster import SpectralClustering
>>> sc = SpectralClustering(3, affinity='precomputed', n_init=100,
...                         assign_labels='discretize')
>>> sc.fit_predict(adjacency_matrix)  




References#

“A Tutorial on Spectral Clustering” Ulrike
von Luxburg, 2007
“Normalized cuts and image segmentation” Jianbo
Shi, Jitendra Malik, 2000
“A Random Walks View of Spectral Segmentation”
Marina Meila, Jianbo Shi, 2001
“On Spectral Clustering: Analysis and an algorithm”
Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001
“Preconditioned Spectral Clustering for Stochastic Block Partition
Streaming Graph Challenge” David Zhuzhunashvili, Andrew Knyazev





2.3.6. Hierarchical clustering#
Hierarchical clustering is a general family of clustering algorithms that
build nested clusters by merging or splitting them successively. This
hierarchy of clusters is represented as a tree (or dendrogram). The root of the
tree is the unique cluster that gathers all the samples, the leaves being the
clusters with only one sample. See the Wikipedia page for more details.
The AgglomerativeClustering object performs a hierarchical clustering
using a bottom up approach: each observation starts in its own cluster, and
clusters are successively merged together. The linkage criteria determines the
metric used for the merge strategy:

Ward minimizes the sum of squared differences within all clusters. It is a
variance-minimizing approach and in this sense is similar to the k-means
objective function but tackled with an agglomerative hierarchical
approach.
Maximum or complete linkage minimizes the maximum distance between
observations of pairs of clusters.
Average linkage minimizes the average of the distances between all
observations of pairs of clusters.
Single linkage minimizes the distance between the closest
observations of pairs of clusters.

AgglomerativeClustering can also scale to large number of samples
when it is used jointly with a connectivity matrix, but is computationally
expensive when no connectivity constraints are added between samples: it
considers at each step all the possible merges.

FeatureAgglomeration
The FeatureAgglomeration uses agglomerative clustering to
group together features that look very similar, thus decreasing the
number of features. It is a dimensionality reduction tool, see
Unsupervised dimensionality reduction.


2.3.6.1. Different linkage type: Ward, complete, average, and single linkage#
AgglomerativeClustering supports Ward, single, average, and complete
linkage strategies.


Agglomerative cluster has a “rich get richer” behavior that leads to
uneven cluster sizes. In this regard, single linkage is the worst
strategy, and Ward gives the most regular sizes. However, the affinity
(or distance used in clustering) cannot be varied with Ward, thus for non
Euclidean metrics, average linkage is a good alternative. Single linkage,
while not robust to noisy data, can be computed very efficiently and can
therefore be useful to provide hierarchical clustering of larger datasets.
Single linkage can also perform well on non-globular data.
Examples

Various Agglomerative Clustering on a 2D embedding of digits: exploration of the
different linkage strategies in a real dataset.

Comparing different hierarchical linkage methods on toy datasets: exploration of
the different linkage strategies in toy datasets.





2.3.6.2. Visualization of cluster hierarchy#
It’s possible to visualize the tree representing the hierarchical merging of clusters
as a dendrogram. Visual inspection can often be useful for understanding the structure
of the data, though more so in the case of small sample sizes.


Examples

Plot Hierarchical Clustering Dendrogram



2.3.6.3. Adding connectivity constraints#
An interesting aspect of AgglomerativeClustering is that
connectivity constraints can be added to this algorithm (only adjacent
clusters can be merged together), through a connectivity matrix that defines
for each sample the neighboring samples following a given structure of the
data. For instance, in the swiss-roll example below, the connectivity
constraints forbid the merging of points that are not adjacent on the swiss
roll, and thus avoid forming clusters that extend across overlapping folds of
the roll.

 These constraint are useful to impose a certain local structure, but they
also make the algorithm faster, especially when the number of the samples
is high.
The connectivity constraints are imposed via an connectivity matrix: a
scipy sparse matrix that has elements only at the intersection of a row
and a column with indices of the dataset that should be connected. This
matrix can be constructed from a-priori information: for instance, you
may wish to cluster web pages by only merging pages with a link pointing
from one to another. It can also be learned from the data, for instance
using sklearn.neighbors.kneighbors_graph to restrict
merging to nearest neighbors as in this example, or
using sklearn.feature_extraction.image.grid_to_graph to
enable only merging of neighboring pixels on an image, as in the
coin example.

Warning
Connectivity constraints with single, average and complete linkage
Connectivity constraints and single, complete or average linkage can enhance
the ‘rich getting richer’ aspect of agglomerative clustering,
particularly so if they are built with
sklearn.neighbors.kneighbors_graph. In the limit of a small
number of clusters, they tend to give a few macroscopically occupied
clusters and almost empty ones. (see the discussion in
Agglomerative clustering with and without structure).
Single linkage is the most brittle linkage option with regard to this issue.









Examples

A demo of structured Ward hierarchical clustering on an image of coins: Ward
clustering to split the image of coins in regions.
Hierarchical clustering: structured vs unstructured ward: Example
of Ward algorithm on a swiss-roll, comparison of structured approaches
versus unstructured approaches.
Feature agglomeration vs. univariate selection: Example
of dimensionality reduction with feature agglomeration based on Ward
hierarchical clustering.
Agglomerative clustering with and without structure



2.3.6.4. Varying the metric#
Single, average and complete linkage can be used with a variety of distances (or
affinities), in particular Euclidean distance (l2), Manhattan distance
(or Cityblock, or l1), cosine distance, or any precomputed affinity
matrix.

l1 distance is often good for sparse features, or sparse noise: i.e.
many of the features are zero, as in text mining using occurrences of
rare words.
cosine distance is interesting because it is invariant to global
scalings of the signal.

The guidelines for choosing a metric is to use one that maximizes the
distance between samples in different classes, and minimizes that within
each class.






Examples

Agglomerative clustering with different metrics



2.3.6.5. Bisecting K-Means#
The BisectingKMeans is an iterative variant of KMeans, using
divisive hierarchical clustering. Instead of creating all centroids at once, centroids
are picked progressively based on a previous clustering: a cluster is split into two
new clusters repeatedly until the target number of clusters is reached.
BisectingKMeans is more efficient than KMeans when the number of
clusters is large since it only works on a subset of the data at each bisection
while KMeans always works on the entire dataset.
Although BisectingKMeans can’t benefit from the advantages of the ""k-means++""
initialization by design, it will still produce comparable results than
KMeans(init=""k-means++"") in terms of inertia at cheaper computational costs, and will
likely produce better results than KMeans with a random initialization.
This variant is more efficient to agglomerative clustering if the number of clusters is
small compared to the number of data points.
This variant also does not produce empty clusters.

There exist two strategies for selecting the cluster to split:
bisecting_strategy=""largest_cluster"" selects the cluster having the most points
bisecting_strategy=""biggest_inertia"" selects the cluster with biggest inertia
(cluster with biggest Sum of Squared Errors within)



Picking by largest amount of data points in most cases produces result as
accurate as picking by inertia and is faster (especially for larger amount of data
points, where calculating error may be costly).
Picking by largest amount of data points will also likely produce clusters of similar
sizes while KMeans is known to produce clusters of different sizes.
Difference between Bisecting K-Means and regular K-Means can be seen on example
Bisecting K-Means and Regular K-Means Performance Comparison.
While the regular K-Means algorithm tends to create non-related clusters,
clusters from Bisecting K-Means are well ordered and create quite a visible hierarchy.


References#

“A Comparison of Document Clustering Techniques” Michael
Steinbach, George Karypis and Vipin Kumar, Department of Computer Science and
Egineering, University of Minnesota (June 2000)
“Performance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog
Data”
K.Abirami and Dr.P.Mayilvahanan, International Journal of Emerging
Technologies in Engineering Research (IJETER) Volume 4, Issue 8, (August 2016)
“Bisecting K-means Algorithm Based on K-valued Self-determining and
Clustering Center Optimization” Jian Di, Xinyue Gou School
of Control and Computer Engineering,North China Electric Power University,
Baoding, Hebei, China (August 2017)





2.3.7. DBSCAN#
The DBSCAN algorithm views clusters as areas of high density
separated by areas of low density. Due to this rather generic view, clusters
found by DBSCAN can be any shape, as opposed to k-means which assumes that
clusters are convex shaped. The central component to the DBSCAN is the concept
of core samples, which are samples that are in areas of high density. A
cluster is therefore a set of core samples, each close to each other
(measured by some distance measure)
and a set of non-core samples that are close to a core sample (but are not
themselves core samples). There are two parameters to the algorithm,
min_samples and eps,
which define formally what we mean when we say dense.
Higher min_samples or lower eps
indicate higher density necessary to form a cluster.
More formally, we define a core sample as being a sample in the dataset such
that there exist min_samples other samples within a distance of
eps, which are defined as neighbors of the core sample. This tells
us that the core sample is in a dense area of the vector space. A cluster
is a set of core samples that can be built by recursively taking a core
sample, finding all of its neighbors that are core samples, finding all of
their neighbors that are core samples, and so on. A cluster also has a
set of non-core samples, which are samples that are neighbors of a core sample
in the cluster but are not themselves core samples. Intuitively, these samples
are on the fringes of a cluster.
Any core sample is part of a cluster, by definition. Any sample that is not a
core sample, and is at least eps in distance from any core sample, is
considered an outlier by the algorithm.
While the parameter min_samples primarily controls how tolerant the
algorithm is towards noise (on noisy and large data sets it may be desirable
to increase this parameter), the parameter eps is crucial to choose
appropriately for the data set and distance function and usually cannot be
left at the default value. It controls the local neighborhood of the points.
When chosen too small, most data will not be clustered at all (and labeled
as -1 for “noise”). When chosen too large, it causes close clusters to
be merged into one cluster, and eventually the entire data set to be returned
as a single cluster. Some heuristics for choosing this parameter have been
discussed in the literature, for example based on a knee in the nearest neighbor
distances plot (as discussed in the references below).
In the figure below, the color indicates cluster membership, with large circles
indicating core samples found by the algorithm. Smaller circles are non-core
samples that are still part of a cluster. Moreover, the outliers are indicated
by black points below.

Examples

Demo of DBSCAN clustering algorithm



Implementation#
The DBSCAN algorithm is deterministic, always generating the same clusters when
given the same data in the same order.  However, the results can differ when
data is provided in a different order. First, even though the core samples will
always be assigned to the same clusters, the labels of those clusters will
depend on the order in which those samples are encountered in the data. Second
and more importantly, the clusters to which non-core samples are assigned can
differ depending on the data order.  This would happen when a non-core sample
has a distance lower than eps to two core samples in different clusters. By
the triangular inequality, those two core samples must be more distant than
eps from each other, or they would be in the same cluster. The non-core
sample is assigned to whichever cluster is generated first in a pass through the
data, and so the results will depend on the data ordering.
The current implementation uses ball trees and kd-trees to determine the
neighborhood of points, which avoids calculating the full distance matrix (as
was done in scikit-learn versions before 0.14). The possibility to use custom
metrics is retained; for details, see NearestNeighbors.



Memory consumption for large sample sizes#
This implementation is by default not memory efficient because it constructs a
full pairwise similarity matrix in the case where kd-trees or ball-trees cannot
be used (e.g., with sparse matrices). This matrix will consume \(n^2\)
floats. A couple of mechanisms for getting around this are:

Use OPTICS clustering in conjunction with the extract_dbscan
method. OPTICS clustering also calculates the full pairwise matrix, but only
keeps one row in memory at a time (memory complexity n).
A sparse radius neighborhood graph (where missing entries are presumed to be
out of eps) can be precomputed in a memory-efficient way and dbscan can be run
over this with metric='precomputed'.  See
sklearn.neighbors.NearestNeighbors.radius_neighbors_graph.
The dataset can be compressed, either by removing exact duplicates if these
occur in your data, or by using BIRCH. Then you only have a relatively small
number of representatives for a large number of points. You can then provide a
sample_weight when fitting DBSCAN.




References#


A Density-Based Algorithm for Discovering Clusters in Large Spatial
Databases with Noise
Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd
International Conference on Knowledge Discovery and Data Mining, Portland, OR,
AAAI Press, pp. 226-231. 1996
DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu,
X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19.



2.3.8. HDBSCAN#
The HDBSCAN algorithm can be seen as an extension of DBSCAN
and OPTICS. Specifically, DBSCAN assumes that the clustering
criterion (i.e. density requirement) is globally homogeneous.
In other words, DBSCAN may struggle to successfully capture clusters
with different densities.
HDBSCAN alleviates this assumption and explores all possible density
scales by building an alternative representation of the clustering problem.

Note
This implementation is adapted from the original implementation of HDBSCAN,
scikit-learn-contrib/hdbscan based on [LJ2017].

Examples

Demo of HDBSCAN clustering algorithm


2.3.8.1. Mutual Reachability Graph#
HDBSCAN first defines \(d_c(x_p)\), the core distance of a sample \(x_p\), as the
distance to its min_samples th-nearest neighbor, counting itself. For example,
if min_samples=5 and \(x_*\) is the 5th-nearest neighbor of \(x_p\)
then the core distance is:

\[d_c(x_p)=d(x_p, x_*).\]
Next it defines \(d_m(x_p, x_q)\), the mutual reachability distance of two points
\(x_p, x_q\), as:

\[d_m(x_p, x_q) = \max\{d_c(x_p), d_c(x_q), d(x_p, x_q)\}\]
These two notions allow us to construct the mutual reachability graph
\(G_{ms}\) defined for a fixed choice of min_samples by associating each
sample \(x_p\) with a vertex of the graph, and thus edges between points
\(x_p, x_q\) are the mutual reachability distance \(d_m(x_p, x_q)\)
between them. We may build subsets of this graph, denoted as
\(G_{ms,\varepsilon}\), by removing any edges with value greater than \(\varepsilon\):
from the original graph. Any points whose core distance is less than \(\varepsilon\):
are at this staged marked as noise. The remaining points are then clustered by
finding the connected components of this trimmed graph.

Note
Taking the connected components of a trimmed graph \(G_{ms,\varepsilon}\) is
equivalent to running DBSCAN* with min_samples and \(\varepsilon\). DBSCAN* is a
slightly modified version of DBSCAN mentioned in [CM2013].



2.3.8.2. Hierarchical Clustering#
HDBSCAN can be seen as an algorithm which performs DBSCAN* clustering across all
values of \(\varepsilon\). As mentioned prior, this is equivalent to finding the connected
components of the mutual reachability graphs for all values of \(\varepsilon\). To do this
efficiently, HDBSCAN first extracts a minimum spanning tree (MST) from the fully
-connected mutual reachability graph, then greedily cuts the edges with highest
weight. An outline of the HDBSCAN algorithm is as follows:

Extract the MST of \(G_{ms}\).
Extend the MST by adding a “self edge” for each vertex, with weight equal
to the core distance of the underlying sample.
Initialize a single cluster and label for the MST.
Remove the edge with the greatest weight from the MST (ties are
removed simultaneously).
Assign cluster labels to the connected components which contain the
end points of the now-removed edge. If the component does not have at least
one edge it is instead assigned a “null” label marking it as noise.
Repeat 4-5 until there are no more connected components.

HDBSCAN is therefore able to obtain all possible partitions achievable by
DBSCAN* for a fixed choice of min_samples in a hierarchical fashion.
Indeed, this allows HDBSCAN to perform clustering across multiple densities
and as such it no longer needs \(\varepsilon\) to be given as a hyperparameter. Instead
it relies solely on the choice of min_samples, which tends to be a more robust
hyperparameter.


HDBSCAN can be smoothed with an additional hyperparameter min_cluster_size
which specifies that during the hierarchical clustering, components with fewer
than minimum_cluster_size many samples are considered noise. In practice, one
can set minimum_cluster_size = min_samples to couple the parameters and
simplify the hyperparameter space.
References


[CM2013]
Campello, R.J.G.B., Moulavi, D., Sander, J. (2013). Density-Based
Clustering Based on Hierarchical Density Estimates. In: Pei, J., Tseng, V.S.,
Cao, L., Motoda, H., Xu, G. (eds) Advances in Knowledge Discovery and Data
Mining. PAKDD 2013. Lecture Notes in Computer Science(), vol 7819. Springer,
Berlin, Heidelberg. Density-Based Clustering Based on Hierarchical
Density Estimates


[LJ2017]
L. McInnes and J. Healy, (2017). Accelerated Hierarchical Density
Based Clustering. In: IEEE International Conference on Data Mining Workshops
(ICDMW), 2017, pp. 33-42. Accelerated Hierarchical Density Based
Clustering





2.3.9. OPTICS#
The OPTICS algorithm shares many similarities with the DBSCAN
algorithm, and can be considered a generalization of DBSCAN that relaxes the
eps requirement from a single value to a value range. The key difference
between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability
graph, which assigns each sample both a reachability_ distance, and a spot
within the cluster ordering_ attribute; these two attributes are assigned
when the model is fitted, and are used to determine cluster membership. If
OPTICS is run with the default value of inf set for max_eps, then DBSCAN
style cluster extraction can be performed repeatedly in linear time for any
given eps value using the cluster_optics_dbscan method. Setting
max_eps to a lower value will result in shorter run times, and can be
thought of as the maximum neighborhood radius from each point to find other
potential reachable points.

The reachability distances generated by OPTICS allow for variable density
extraction of clusters within a single data set. As shown in the above plot,
combining reachability distances and data set ordering_ produces a
reachability plot, where point density is represented on the Y-axis, and
points are ordered such that nearby points are adjacent. ‘Cutting’ the
reachability plot at a single value produces DBSCAN like results; all points
above the ‘cut’ are classified as noise, and each time that there is a break
when reading from left to right signifies a new cluster. The default cluster
extraction with OPTICS looks at the steep slopes within the graph to find
clusters, and the user can define what counts as a steep slope using the
parameter xi. There are also other possibilities for analysis on the graph
itself, such as generating hierarchical representations of the data through
reachability-plot dendrograms, and the hierarchy of clusters detected by the
algorithm can be accessed through the cluster_hierarchy_ parameter. The
plot above has been color-coded so that cluster colors in planar space match
the linear segment clusters of the reachability plot. Note that the blue and
red clusters are adjacent in the reachability plot, and can be hierarchically
represented as children of a larger parent cluster.
Examples

Demo of OPTICS clustering algorithm



Comparison with DBSCAN#
The results from OPTICS cluster_optics_dbscan method and DBSCAN are very
similar, but not always identical; specifically, labeling of periphery and noise
points. This is in part because the first samples of each dense area processed
by OPTICS have a large reachability value while being close to other points in
their area, and will thus sometimes be marked as noise rather than periphery.
This affects adjacent points when they are considered as candidates for being
marked as either periphery or noise.
Note that for any single value of eps, DBSCAN will tend to have a shorter
run time than OPTICS; however, for repeated runs at varying eps values, a
single run of OPTICS may require less cumulative runtime than DBSCAN. It is also
important to note that OPTICS’ output is close to DBSCAN’s only if eps and
max_eps are close.



Computational Complexity#
Spatial indexing trees are used to avoid calculating the full distance matrix,
and allow for efficient memory usage on large sets of samples. Different
distance metrics can be supplied via the metric keyword.
For large datasets, similar (but not identical) results can be obtained via
HDBSCAN. The HDBSCAN implementation is multithreaded, and has better
algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling.
For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS
will maintain \(n\) (as opposed to \(n^2\)) memory scaling; however,
tuning of the max_eps parameter will likely need to be used to give a
solution in a reasonable amount of wall time.



References#

“OPTICS: ordering points to identify the clustering structure.” Ankerst,
Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. In ACM Sigmod
Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.




2.3.10. BIRCH#
The Birch builds a tree called the Clustering Feature Tree (CFT)
for the given data. The data is essentially lossy compressed to a set of
Clustering Feature nodes (CF Nodes). The CF Nodes have a number of
subclusters called Clustering Feature subclusters (CF Subclusters)
and these CF Subclusters located in the non-terminal CF Nodes
can have CF Nodes as children.
The CF Subclusters hold the necessary information for clustering which prevents
the need to hold the entire input data in memory. This information includes:

Number of samples in a subcluster.
Linear Sum - An n-dimensional vector holding the sum of all samples
Squared Sum - Sum of the squared L2 norm of all samples.
Centroids - To avoid recalculation linear sum / n_samples.
Squared norm of the centroids.

The BIRCH algorithm has two parameters, the threshold and the branching factor.
The branching factor limits the number of subclusters in a node and the
threshold limits the distance between the entering sample and the existing
subclusters.
This algorithm can be viewed as an instance or data reduction method,
since it reduces the input data to a set of subclusters which are obtained directly
from the leaves of the CFT. This reduced data can be further processed by feeding
it into a global clusterer. This global clusterer can be set by n_clusters.
If n_clusters is set to None, the subclusters from the leaves are directly
read off, otherwise a global clustering step labels these subclusters into global
clusters (labels) and the samples are mapped to the global label of the nearest subcluster.


Algorithm description#

A new sample is inserted into the root of the CF Tree which is a CF Node. It
is then merged with the subcluster of the root, that has the smallest radius
after merging, constrained by the threshold and branching factor conditions.
If the subcluster has any child node, then this is done repeatedly till it
reaches a leaf. After finding the nearest subcluster in the leaf, the
properties of this subcluster and the parent subclusters are recursively
updated.
If the radius of the subcluster obtained by merging the new sample and the
nearest subcluster is greater than the square of the threshold and if the
number of subclusters is greater than the branching factor, then a space is
temporarily allocated to this new sample. The two farthest subclusters are
taken and the subclusters are divided into two groups on the basis of the
distance between these subclusters.
If this split node has a parent subcluster and there is room for a new
subcluster, then the parent is split into two. If there is no room, then this
node is again split into two and the process is continued recursively, till it
reaches the root.




BIRCH or MiniBatchKMeans?#

BIRCH does not scale very well to high dimensional data. As a rule of thumb if
n_features is greater than twenty, it is generally better to use MiniBatchKMeans.
If the number of instances of data needs to be reduced, or if one wants a
large number of subclusters either as a preprocessing step or otherwise,
BIRCH is more useful than MiniBatchKMeans.






How to use partial_fit?#
To avoid the computation of global clustering, for every call of partial_fit
the user is advised:

To set n_clusters=None initially.
Train all data by multiple calls to partial_fit.
Set n_clusters to a required value using
brc.set_params(n_clusters=n_clusters).
Call partial_fit finally with no arguments, i.e. brc.partial_fit()
which performs the global clustering.




References#

Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data
clustering method for large databases.
https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf
Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm
https://code.google.com/archive/p/jbirch




2.3.11. Clustering performance evaluation#
Evaluating the performance of a clustering algorithm is not as trivial as
counting the number of errors or the precision and recall of a supervised
classification algorithm. In particular any evaluation metric should not
take the absolute values of the cluster labels into account but rather
if this clustering define separations of the data similar to some ground
truth set of classes or satisfying some assumption such that members
belong to the same class are more similar than members of different
classes according to some similarity metric.

2.3.11.1. Rand index#
Given the knowledge of the ground truth class assignments
labels_true and our clustering algorithm assignments of the same
samples labels_pred, the (adjusted or unadjusted) Rand index
is a function that measures the similarity of the two assignments,
ignoring permutations:
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.rand_score(labels_true, labels_pred)
0.66...


The Rand index does not ensure to obtain a value close to 0.0 for a
random labelling. The adjusted Rand index corrects for chance and
will give such a baseline.
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
0.24...


As with all clustering metrics, one can permute 0 and 1 in the predicted
labels, rename 2 to 3, and get the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.rand_score(labels_true, labels_pred)
0.66...
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
0.24...


Furthermore, both rand_score adjusted_rand_score are
symmetric: swapping the argument does not change the scores. They can
thus be used as consensus measures:
>>> metrics.rand_score(labels_pred, labels_true)
0.66...
>>> metrics.adjusted_rand_score(labels_pred, labels_true)
0.24...


Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.rand_score(labels_true, labels_pred)
1.0
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
1.0


Poorly agreeing labels (e.g. independent labelings) have lower scores,
and for the adjusted Rand index the score will be negative or close to
zero. However, for the unadjusted Rand index the score, while lower,
will not necessarily be close to zero.:
>>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1]
>>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]
>>> metrics.rand_score(labels_true, labels_pred)
0.39...
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
-0.07...



Advantages:

Interpretability: The unadjusted Rand index is proportional to the
number of sample pairs whose labels are the same in both labels_pred and
labels_true, or are different in both.
Random (uniform) label assignments have an adjusted Rand index score close
to 0.0 for any value of n_clusters and n_samples (which is not the
case for the unadjusted Rand index or the V-measure for instance).
Bounded range: Lower values indicate different labelings, similar
clusterings have a high (adjusted or unadjusted) Rand index, 1.0 is the
perfect match score. The score range is [0, 1] for the unadjusted Rand index
and [-0.5, 1] for the adjusted Rand index.
No assumption is made on the cluster structure: The (adjusted or
unadjusted) Rand index can be used to compare all kinds of clustering
algorithms, and can be used to compare clustering algorithms such as k-means
which assumes isotropic blob shapes with results of spectral clustering
algorithms which can find cluster with “folded” shapes.



Drawbacks:

Contrary to inertia, the (adjusted or unadjusted) Rand index requires
knowledge of the ground truth classes which is almost never available in
practice or requires manual assignment by human annotators (as in the
supervised learning setting).
However (adjusted or unadjusted) Rand index can also be useful in a purely
unsupervised setting as a building block for a Consensus Index that can be
used for clustering model selection (TODO).

The unadjusted Rand index is often close to 1.0 even if the clusterings
themselves differ significantly. This can be understood when interpreting
the Rand index as the accuracy of element pair labeling resulting from the
clusterings: In practice there often is a majority of element pairs that are
assigned the different pair label under both the predicted and the
ground truth clustering resulting in a high proportion of pair labels that
agree, which leads subsequently to a high score.


Examples

Adjustment for chance in clustering performance evaluation:
Analysis of the impact of the dataset size on the value of
clustering measures for random assignments.



Mathematical formulation#
If C is a ground truth class assignment and K the clustering, let us define
\(a\) and \(b\) as:

\(a\), the number of pairs of elements that are in the same set in C and
in the same set in K
\(b\), the number of pairs of elements that are in different sets in C and
in different sets in K

The unadjusted Rand index is then given by:

\[\text{RI} = \frac{a + b}{C_2^{n_{samples}}}\]
where \(C_2^{n_{samples}}\) is the total number of possible pairs in the
dataset. It does not matter if the calculation is performed on ordered pairs or
unordered pairs as long as the calculation is performed consistently.
However, the Rand index does not guarantee that random label assignments will
get a value close to zero (esp. if the number of clusters is in the same order
of magnitude as the number of samples).
To counter this effect we can discount the expected RI \(E[\text{RI}]\) of
random labelings by defining the adjusted Rand index as follows:

\[\text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}\]



References#

Comparing Partitions L. Hubert and P.
Arabie, Journal of Classification 1985
Properties of the Hubert-Arabie adjusted Rand index D. Steinley, Psychological
Methods 2004
Wikipedia entry for the Rand index
Minimum adjusted Rand index for two clusterings of a given size, 2022, J. E. Chacón and A. I. Rastrojo




2.3.11.2. Mutual Information based scores#
Given the knowledge of the ground truth class assignments labels_true and
our clustering algorithm assignments of the same samples labels_pred, the
Mutual Information is a function that measures the agreement of the two
assignments, ignoring permutations.  Two different normalized versions of this
measure are available, Normalized Mutual Information (NMI) and Adjusted
Mutual Information (AMI). NMI is often used in the literature, while AMI was
proposed more recently and is normalized against chance:
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]

>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
0.22504...


One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get
the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
0.22504...


All, mutual_info_score, adjusted_mutual_info_score and
normalized_mutual_info_score are symmetric: swapping the argument does
not change the score. Thus they can be used as a consensus measure:
>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  
0.22504...


Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
1.0

>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  
1.0


This is not true for mutual_info_score, which is therefore harder to judge:
>>> metrics.mutual_info_score(labels_true, labels_pred)  
0.69...


Bad (e.g. independent labelings) have non-positive scores:
>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
-0.10526...



Advantages:

Random (uniform) label assignments have a AMI score close to 0.0 for any
value of n_clusters and n_samples (which is not the case for raw
Mutual Information or the V-measure for instance).
Upper bound  of 1:  Values close to zero indicate two label assignments
that are largely independent, while values close to one indicate significant
agreement. Further, an AMI of exactly 1 indicates that the two label
assignments are equal (with or without permutation).



Drawbacks:

Contrary to inertia, MI-based measures require the knowledge of the ground
truth classes while almost never available in practice or requires manual
assignment by human annotators (as in the supervised learning setting).
However MI-based measures can also be useful in purely unsupervised setting
as a building block for a Consensus Index that can be used for clustering
model selection.

NMI and MI are not adjusted against chance.


Examples

Adjustment for chance in clustering performance evaluation: Analysis
of the impact of the dataset size on the value of clustering measures for random
assignments. This example also includes the Adjusted Rand Index.



Mathematical formulation#
Assume two label assignments (of the same N objects), \(U\) and \(V\).
Their entropy is the amount of uncertainty for a partition set, defined by:

\[H(U) = - \sum_{i=1}^{|U|}P(i)\log(P(i))\]
where \(P(i) = |U_i| / N\) is the probability that an object picked at
random from \(U\) falls into class \(U_i\). Likewise for \(V\):

\[H(V) = - \sum_{j=1}^{|V|}P'(j)\log(P'(j))\]
With \(P'(j) = |V_j| / N\). The mutual information (MI) between \(U\)
and \(V\) is calculated by:

\[\text{MI}(U, V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}P(i, j)\log\left(\frac{P(i,j)}{P(i)P'(j)}\right)\]
where \(P(i, j) = |U_i \cap V_j| / N\) is the probability that an object
picked at random falls into both classes \(U_i\) and \(V_j\).
It also can be expressed in set cardinality formulation:

\[\text{MI}(U, V) = \sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i \cap V_j|}{N}\log\left(\frac{N|U_i \cap V_j|}{|U_i||V_j|}\right)\]
The normalized mutual information is defined as

\[\text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\text{mean}(H(U), H(V))}\]
This value of the mutual information and also the normalized variant is not
adjusted for chance and will tend to increase as the number of different labels
(clusters) increases, regardless of the actual amount of “mutual information”
between the label assignments.
The expected value for the mutual information can be calculated using the
following equation [VEB2009]. In this equation, \(a_i = |U_i|\) (the number
of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in
\(V_j\)).

\[E[\text{MI}(U,V)]=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \sum_{n_{ij}=(a_i+b_j-N)^+
}^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
(N-a_i-b_j+n_{ij})!}\]
Using the expected value, the adjusted mutual information can then be calculated
using a similar form to that of the adjusted Rand index:

\[\text{AMI} = \frac{\text{MI} - E[\text{MI}]}{\text{mean}(H(U), H(V)) - E[\text{MI}]}\]
For normalized mutual information and adjusted mutual information, the
normalizing value is typically some generalized mean of the entropies of each
clustering. Various generalized means exist, and no firm rules exist for
preferring one over the others.  The decision is largely a field-by-field basis;
for instance, in community detection, the arithmetic mean is most common. Each
normalizing method provides “qualitatively similar behaviours” [YAT2016]. In
our implementation, this is controlled by the average_method parameter.
Vinh et al. (2010) named variants of NMI and AMI by their averaging method
[VEB2010]. Their ‘sqrt’ and ‘sum’ averages are the geometric and arithmetic
means; we use these more broadly common names.
References

Strehl, Alexander, and Joydeep Ghosh (2002). “Cluster ensembles - a
knowledge reuse framework for combining multiple partitions”. Journal of
Machine Learning Research 3: 583-617. doi:10.1162/153244303321897735.
Wikipedia entry for the (normalized) Mutual Information
Wikipedia entry for the Adjusted Mutual Information



[VEB2009]
Vinh, Epps, and Bailey, (2009). “Information theoretic measures
for clusterings comparison”. Proceedings of the 26th Annual International
Conference on Machine Learning - ICML ‘09. doi:10.1145/1553374.1553511. ISBN
9781605585161.


[VEB2010]
Vinh, Epps, and Bailey, (2010). “Information Theoretic Measures
for Clusterings Comparison: Variants, Properties, Normalization and
Correction for Chance”. JMLR
<https://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>


[YAT2016]
Yang, Algesheimer, and Tessone, (2016). “A comparative analysis
of community detection algorithms on artificial networks”. Scientific
Reports 6: 30750. doi:10.1038/srep30750.





2.3.11.3. Homogeneity, completeness and V-measure#
Given the knowledge of the ground truth class assignments of the samples,
it is possible to define some intuitive metric using conditional entropy
analysis.
In particular Rosenberg and Hirschberg (2007) define the following two
desirable objectives for any cluster assignment:

homogeneity: each cluster contains only members of a single class.
completeness: all members of a given class are assigned to the same
cluster.

We can turn those concept as scores homogeneity_score and
completeness_score. Both are bounded below by 0.0 and above by
1.0 (higher is better):
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]

>>> metrics.homogeneity_score(labels_true, labels_pred)
0.66...

>>> metrics.completeness_score(labels_true, labels_pred)
0.42...


Their harmonic mean called V-measure is computed by
v_measure_score:
>>> metrics.v_measure_score(labels_true, labels_pred)
0.51...


This function’s formula is as follows:

\[v = \frac{(1 + \beta) \times \text{homogeneity} \times \text{completeness}}{(\beta \times \text{homogeneity} + \text{completeness})}\]
beta defaults to a value of 1.0, but for using a value less than 1 for beta:
>>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)
0.54...


more weight will be attributed to homogeneity, and using a value greater than 1:
>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)
0.48...


more weight will be attributed to completeness.
The V-measure is actually equivalent to the mutual information (NMI)
discussed above, with the aggregation function being the arithmetic mean [B2011].
Homogeneity, completeness and V-measure can be computed at once using
homogeneity_completeness_v_measure as follows:
>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
(0.66..., 0.42..., 0.51...)


The following clustering assignment is slightly better, since it is
homogeneous but not complete:
>>> labels_pred = [0, 0, 0, 1, 2, 2]
>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
(1.0, 0.68..., 0.81...)



Note
v_measure_score is symmetric: it can be used to evaluate
the agreement of two independent assignments on the same dataset.
This is not the case for completeness_score and
homogeneity_score: both are bound by the relationship:
homogeneity_score(a, b) == completeness_score(b, a)




Advantages:

Bounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score.
Intuitive interpretation: clustering with bad V-measure can be
qualitatively analyzed in terms of homogeneity and completeness to
better feel what ‘kind’ of mistakes is done by the assignment.
No assumption is made on the cluster structure: can be used to compare
clustering algorithms such as k-means which assumes isotropic blob shapes
with results of spectral clustering algorithms which can find cluster with
“folded” shapes.



Drawbacks:

The previously introduced metrics are not normalized with regards to
random labeling: this means that depending on the number of samples,
clusters and ground truth classes, a completely random labeling will not
always yield the same values for homogeneity, completeness and hence
v-measure. In particular random labeling won’t yield zero scores
especially when the number of clusters is large.
This problem can safely be ignored when the number of samples is more than a
thousand and the number of clusters is less than 10. For smaller sample
sizes or larger number of clusters it is safer to use an adjusted index such
as the Adjusted Rand Index (ARI).







These metrics require the knowledge of the ground truth classes while
almost never available in practice or requires manual assignment by human
annotators (as in the supervised learning setting).


Examples

Adjustment for chance in clustering performance evaluation: Analysis
of the impact of the dataset size on the value of clustering measures for
random assignments.



Mathematical formulation#
Homogeneity and completeness scores are formally given by:

\[h = 1 - \frac{H(C|K)}{H(C)}\]

\[c = 1 - \frac{H(K|C)}{H(K)}\]
where \(H(C|K)\) is the conditional entropy of the classes given the
cluster assignments and is given by:

\[H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n}
\cdot \log\left(\frac{n_{c,k}}{n_k}\right)\]
and \(H(C)\) is the entropy of the classes and is given by:

\[H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)\]
with \(n\) the total number of samples, \(n_c\) and \(n_k\) the
number of samples respectively belonging to class \(c\) and cluster
\(k\), and finally \(n_{c,k}\) the number of samples from class
\(c\) assigned to cluster \(k\).
The conditional entropy of clusters given class \(H(K|C)\) and the
entropy of clusters \(H(K)\) are defined in a symmetric manner.
Rosenberg and Hirschberg further define V-measure as the harmonic mean of
homogeneity and completeness:

\[v = 2 \cdot \frac{h \cdot c}{h + c}\]

References

V-Measure: A conditional entropy-based external cluster evaluation measure Andrew Rosenberg and Julia
Hirschberg, 2007



[B2011]
Identification and Characterization of Events in Social Media, Hila
Becker, PhD Thesis.




2.3.11.4. Fowlkes-Mallows scores#
The original Fowlkes-Mallows index (FMI) was intended to measure the similarity
between two clustering results, which is inherently an unsupervised comparison.
The supervised adaptation of the Fowlkes-Mallows index
(as implemented in sklearn.metrics.fowlkes_mallows_score) can be used
when the ground truth class assignments of the samples are known.
The FMI is defined as the geometric mean of the pairwise precision and recall:

\[\text{FMI} = \frac{\text{TP}}{\sqrt{(\text{TP} + \text{FP}) (\text{TP} + \text{FN})}}\]
In the above formula:

TP (True Positive): The number of pairs of points that are clustered together
both in the true labels and in the predicted labels.
FP (False Positive): The number of pairs of points that are clustered together
in the predicted labels but not in the true labels.
FN (False Negative): The number of pairs of points that are clustered together
in the true labels but not in the predicted labels.

The score ranges from 0 to 1. A high value indicates a good similarity
between two clusters.
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]


>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.47140...


One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get
the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]

>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.47140...


Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
1.0


Bad (e.g. independent labelings) have zero scores:
>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.0



Advantages:

Random (uniform) label assignments have a FMI score close to 0.0 for any
value of n_clusters and n_samples (which is not the case for raw
Mutual Information or the V-measure for instance).
Upper-bounded at 1:  Values close to zero indicate two label assignments
that are largely independent, while values close to one indicate significant
agreement. Further, values of exactly 0 indicate purely independent
label assignments and a FMI of exactly 1 indicates that the two label
assignments are equal (with or without permutation).
No assumption is made on the cluster structure: can be used to compare
clustering algorithms such as k-means which assumes isotropic blob shapes
with results of spectral clustering algorithms which can find cluster with
“folded” shapes.



Drawbacks:

Contrary to inertia, FMI-based measures require the knowledge of the
ground truth classes while almost never available in practice or requires
manual assignment by human annotators (as in the supervised learning
setting).




References#

E. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two
hierarchical clusterings”. Journal of the American Statistical Association.
https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008
Wikipedia entry for the Fowlkes-Mallows Index




2.3.11.5. Silhouette Coefficient#
If the ground truth labels are not known, evaluation must be performed using
the model itself. The Silhouette Coefficient
(sklearn.metrics.silhouette_score)
is an example of such an evaluation, where a
higher Silhouette Coefficient score relates to a model with better defined
clusters. The Silhouette Coefficient is defined for each sample and is composed
of two scores:

a: The mean distance between a sample and all other points in the same
class.
b: The mean distance between a sample and all other points in the next
nearest cluster.

The Silhouette Coefficient s for a single sample is then given as:

\[s = \frac{b - a}{max(a, b)}\]
The Silhouette Coefficient for a set of samples is given as the mean of the
Silhouette Coefficient for each sample.
>>> from sklearn import metrics
>>> from sklearn.metrics import pairwise_distances
>>> from sklearn import datasets
>>> X, y = datasets.load_iris(return_X_y=True)


In normal usage, the Silhouette Coefficient is applied to the results of a
cluster analysis.
>>> import numpy as np
>>> from sklearn.cluster import KMeans
>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans_model.labels_
>>> metrics.silhouette_score(X, labels, metric='euclidean')
0.55...



Advantages:

The score is bounded between -1 for incorrect clustering and +1 for highly
dense clustering. Scores around zero indicate overlapping clusters.
The score is higher when clusters are dense and well separated, which
relates to a standard concept of a cluster.



Drawbacks:

The Silhouette Coefficient is generally higher for convex clusters than
other concepts of clusters, such as density based clusters like those
obtained through DBSCAN.


Examples

Selecting the number of clusters with silhouette analysis on KMeans clustering : In
this example the silhouette analysis is used to choose an optimal value for
n_clusters.



References#

Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the
Interpretation and Validation of Cluster Analysis”.
Computational and Applied Mathematics 20: 53-65.




2.3.11.6. Calinski-Harabasz Index#
If the ground truth labels are not known, the Calinski-Harabasz index
(sklearn.metrics.calinski_harabasz_score) - also known as the Variance
Ratio Criterion - can be used to evaluate the model, where a higher
Calinski-Harabasz score relates to a model with better defined clusters.
The index is the ratio of the sum of between-clusters dispersion and of
within-cluster dispersion for all clusters (where dispersion is defined as the
sum of distances squared):
>>> from sklearn import metrics
>>> from sklearn.metrics import pairwise_distances
>>> from sklearn import datasets
>>> X, y = datasets.load_iris(return_X_y=True)


In normal usage, the Calinski-Harabasz index is applied to the results of a
cluster analysis:
>>> import numpy as np
>>> from sklearn.cluster import KMeans
>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans_model.labels_
>>> metrics.calinski_harabasz_score(X, labels)
561.59...



Advantages:

The score is higher when clusters are dense and well separated, which
relates to a standard concept of a cluster.
The score is fast to compute.



Drawbacks:

The Calinski-Harabasz index is generally higher for convex clusters than
other concepts of clusters, such as density based clusters like those
obtained through DBSCAN.




Mathematical formulation#
For a set of data \(E\) of size \(n_E\) which has been clustered into
\(k\) clusters, the Calinski-Harabasz score \(s\) is defined as the
ratio of the between-clusters dispersion mean and the within-cluster
dispersion:

\[s = \frac{\mathrm{tr}(B_k)}{\mathrm{tr}(W_k)} \times \frac{n_E - k}{k - 1}\]
where \(\mathrm{tr}(B_k)\) is trace of the between group dispersion matrix
and \(\mathrm{tr}(W_k)\) is the trace of the within-cluster dispersion
matrix defined by:

\[W_k = \sum_{q=1}^k \sum_{x \in C_q} (x - c_q) (x - c_q)^T\]

\[B_k = \sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T\]
with \(C_q\) the set of points in cluster \(q\), \(c_q\) the
center of cluster \(q\), \(c_E\) the center of \(E\), and
\(n_q\) the number of points in cluster \(q\).



References#

Caliński, T., & Harabasz, J. (1974). “A Dendrite Method for Cluster Analysis”.
Communications in Statistics-theory and Methods 3: 1-27.




2.3.11.7. Davies-Bouldin Index#
If the ground truth labels are not known, the Davies-Bouldin index
(sklearn.metrics.davies_bouldin_score) can be used to evaluate the
model, where a lower Davies-Bouldin index relates to a model with better
separation between the clusters.
This index signifies the average ‘similarity’ between clusters, where the
similarity is a measure that compares the distance between clusters with the
size of the clusters themselves.
Zero is the lowest possible score. Values closer to zero indicate a better
partition.
In normal usage, the Davies-Bouldin index is applied to the results of a
cluster analysis as follows:
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> X = iris.data
>>> from sklearn.cluster import KMeans
>>> from sklearn.metrics import davies_bouldin_score
>>> kmeans = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans.labels_
>>> davies_bouldin_score(X, labels)
0.666...



Advantages:

The computation of Davies-Bouldin is simpler than that of Silhouette scores.
The index is solely based on quantities and features inherent to the dataset
as its computation only uses point-wise distances.



Drawbacks:

The Davies-Boulding index is generally higher for convex clusters than other
concepts of clusters, such as density based clusters like those obtained
from DBSCAN.
The usage of centroid distance limits the distance metric to Euclidean
space.




Mathematical formulation#
The index is defined as the average similarity between each cluster \(C_i\)
for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of
this index, similarity is defined as a measure \(R_{ij}\) that trades off:

\(s_i\), the average distance between each point of cluster \(i\) and
the centroid of that cluster – also know as cluster diameter.
\(d_{ij}\), the distance between cluster centroids \(i\) and
\(j\).

A simple choice to construct \(R_{ij}\) so that it is nonnegative and
symmetric is:

\[R_{ij} = \frac{s_i + s_j}{d_{ij}}\]
Then the Davies-Bouldin index is defined as:

\[DB = \frac{1}{k} \sum_{i=1}^k \max_{i \neq j} R_{ij}\]



References#

Davies, David L.; Bouldin, Donald W. (1979). “A Cluster Separation
Measure” IEEE Transactions on Pattern Analysis
and Machine Intelligence. PAMI-1 (2): 224-227.
Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). “On
Clustering Validation Techniques” Journal of
Intelligent Information Systems, 17(2-3), 107-145.
Wikipedia entry for Davies-Bouldin index.




2.3.11.8. Contingency Matrix#
Contingency matrix (sklearn.metrics.cluster.contingency_matrix)
reports the intersection cardinality for every true/predicted cluster pair.
The contingency matrix provides sufficient statistics for all clustering
metrics where the samples are independent and identically distributed and
one doesn’t need to account for some instances not being clustered.
Here is an example:
>>> from sklearn.metrics.cluster import contingency_matrix
>>> x = [""a"", ""a"", ""a"", ""b"", ""b"", ""b""]
>>> y = [0, 0, 1, 1, 2, 2]
>>> contingency_matrix(x, y)
array([[2, 1, 0],
       [0, 1, 2]])


The first row of output array indicates that there are three samples whose
true cluster is “a”. Of them, two are in predicted cluster 0, one is in 1,
and none is in 2. And the second row indicates that there are three samples
whose true cluster is “b”. Of them, none is in predicted cluster 0, one is in
1 and two are in 2.
A confusion matrix for classification is a square
contingency matrix where the order of rows and columns correspond to a list
of classes.

Advantages:

Allows to examine the spread of each true cluster across predicted clusters
and vice versa.
The contingency table calculated is typically utilized in the calculation of
a similarity statistic (like the others listed in this document) between the
two clusterings.



Drawbacks:

Contingency matrix is easy to interpret for a small number of clusters, but
becomes very hard to interpret for a large number of clusters.
It doesn’t give a single metric to use as an objective for clustering
optimisation.




References#

Wikipedia entry for contingency matrix




2.3.11.9. Pair Confusion Matrix#
The pair confusion matrix
(sklearn.metrics.cluster.pair_confusion_matrix) is a 2x2
similarity matrix

\[\begin{split}C = \left[\begin{matrix}
C_{00} & C_{01} \\
C_{10} & C_{11}
\end{matrix}\right]\end{split}\]
between two clusterings computed by considering all pairs of samples and
counting pairs that are assigned into the same or into different clusters
under the true and predicted clusterings.
It has the following entries:
\(C_{00}\) : number of pairs with both clusterings having the samples
not clustered together
\(C_{10}\) : number of pairs with the true label clustering having the
samples clustered together but the other clustering not having the samples
clustered together
\(C_{01}\) : number of pairs with the true label clustering not having
the samples clustered together but the other clustering having the samples
clustered together
\(C_{11}\) : number of pairs with both clusterings having the samples
clustered together
Considering a pair of samples that is clustered together a positive pair,
then as in binary classification the count of true negatives is
\(C_{00}\), false negatives is \(C_{10}\), true positives is
\(C_{11}\) and false positives is \(C_{01}\).
Perfectly matching labelings have all non-zero entries on the
diagonal regardless of actual label values:
>>> from sklearn.metrics.cluster import pair_confusion_matrix
>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])
array([[8, 0],
       [0, 4]])


>>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])
array([[8, 0],
       [0, 4]])


Labelings that assign all classes members to the same clusters
are complete but may not always be pure, hence penalized, and
have some off-diagonal non-zero entries:
>>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])
array([[8, 2],
       [0, 2]])


The matrix is not symmetric:
>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])
array([[8, 0],
       [2, 2]])


If classes members are completely split across different clusters, the
assignment is totally incomplete, hence the matrix has all zero
diagonal entries:
>>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])
array([[ 0,  0],
       [12,  0]])




References#

“Comparing Partitions” L. Hubert and P. Arabie,
Journal of Classification 1985













previous
2.2. Manifold learning




next
2.4. Biclustering










 On this page
  


2.3.1. Overview of clustering methods
2.3.2. K-means
2.3.2.1. Low-level parallelism
2.3.2.2. Mini Batch K-Means


2.3.3. Affinity Propagation
2.3.4. Mean Shift
2.3.5. Spectral clustering
2.3.5.1. Different label assignment strategies
2.3.5.2. Spectral Clustering Graphs


2.3.6. Hierarchical clustering
2.3.6.1. Different linkage type: Ward, complete, average, and single linkage
2.3.6.2. Visualization of cluster hierarchy
2.3.6.3. Adding connectivity constraints
2.3.6.4. Varying the metric
2.3.6.5. Bisecting K-Means


2.3.7. DBSCAN
2.3.8. HDBSCAN
2.3.8.1. Mutual Reachability Graph
2.3.8.2. Hierarchical Clustering


2.3.9. OPTICS
2.3.10. BIRCH
2.3.11. Clustering performance evaluation
2.3.11.1. Rand index
2.3.11.2. Mutual Information based scores
2.3.11.3. Homogeneity, completeness and V-measure
2.3.11.4. Fowlkes-Mallows scores
2.3.11.5. Silhouette Coefficient
2.3.11.6. Calinski-Harabasz Index
2.3.11.7. Davies-Bouldin Index
2.3.11.8. Contingency Matrix
2.3.11.9. Pair Confusion Matrix







 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,K-Means Clustering,K-Means Clustering as a multiclass estimator. Will be changed to unsupervised later.
K-Means Clustering.,http://scikit-learn.org/stable/modules/clustering.html#k-means,K-Means Clustering,KMEANS,Clustering,"













2.3. Clustering — scikit-learn 1.5.2 documentation















































Skip to main content


Back to top










Ctrl+K
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



                    More
                



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  















GitHub
























    Install
  



    User Guide
  



    API
  



    Examples
  



    Community
  



    Getting Started
  



    Release History
  



    Glossary
  



    Development
  



    FAQ
  



    Support
  



    Related Projects
  



    Roadmap
  



    Governance
  



    About us
  










GitHub









Section Navigation

1. Supervised learning
1.1. Linear Models
1.2. Linear and Quadratic Discriminant Analysis
1.3. Kernel ridge regression
1.4. Support Vector Machines
1.5. Stochastic Gradient Descent
1.6. Nearest Neighbors
1.7. Gaussian Processes
1.8. Cross decomposition
1.9. Naive Bayes
1.10. Decision Trees
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking
1.12. Multiclass and multioutput algorithms
1.13. Feature selection
1.14. Semi-supervised learning
1.15. Isotonic regression
1.16. Probability calibration
1.17. Neural network models (supervised)


2. Unsupervised learning
2.1. Gaussian mixture models
2.2. Manifold learning
2.3. Clustering
2.4. Biclustering
2.5. Decomposing signals in components (matrix factorization problems)
2.6. Covariance estimation
2.7. Novelty and Outlier Detection
2.8. Density Estimation
2.9. Neural network models (unsupervised)


3. Model selection and evaluation
3.1. Cross-validation: evaluating estimator performance
3.2. Tuning the hyper-parameters of an estimator
3.3. Tuning the decision threshold for class prediction
3.4. Metrics and scoring: quantifying the quality of predictions
3.5. Validation curves: plotting scores to evaluate models


4. Inspection
4.1. Partial Dependence and Individual Conditional Expectation plots
4.2. Permutation feature importance


5. Visualizations
6. Dataset transformations
6.1. Pipelines and composite estimators
6.2. Feature extraction
6.3. Preprocessing data
6.4. Imputation of missing values
6.5. Unsupervised dimensionality reduction
6.6. Random Projection
6.7. Kernel Approximation
6.8. Pairwise metrics, Affinities and Kernels
6.9. Transforming the prediction target (y)


7. Dataset loading utilities
7.1. Toy datasets
7.2. Real world datasets
7.3. Generated datasets
7.4. Loading other datasets


8. Computing with scikit-learn
8.1. Strategies to scale computationally: bigger data
8.2. Computational Performance
8.3. Parallelism, resource management, and configuration


9. Model persistence
10. Common pitfalls and recommended practices
11. Dispatching
11.1. Array API support (experimental)


12. Choosing the right estimator
13. External Resources, Videos and Talks






















User Guide
2. Unsupervised learning










2.3. Clustering#
Clustering of
unlabeled data can be performed with the module sklearn.cluster.
Each clustering algorithm comes in two variants: a class, that implements
the fit method to learn the clusters on train data, and a function,
that, given train data, returns an array of integer labels corresponding
to the different clusters. For the class, the labels over the training
data can be found in the labels_ attribute.

Input data
One important thing to note is that the algorithms implemented in
this module can take different kinds of matrix as input. All the
methods accept standard data matrices of shape (n_samples, n_features).
These can be obtained from the classes in the sklearn.feature_extraction
module. For AffinityPropagation, SpectralClustering
and DBSCAN one can also input similarity matrices of shape
(n_samples, n_samples). These can be obtained from the functions
in the sklearn.metrics.pairwise module.


2.3.1. Overview of clustering methods#




A comparison of the clustering algorithms in scikit-learn#











Method name
Parameters
Scalability
Usecase
Geometry (metric used)



K-Means
number of clusters
Very large n_samples, medium n_clusters with
MiniBatch code
General-purpose, even cluster size, flat geometry,
not too many clusters, inductive
Distances between points

Affinity propagation
damping, sample preference
Not scalable with n_samples
Many clusters, uneven cluster size, non-flat geometry, inductive
Graph distance (e.g. nearest-neighbor graph)

Mean-shift
bandwidth
Not scalable with n_samples
Many clusters, uneven cluster size, non-flat geometry, inductive
Distances between points

Spectral clustering
number of clusters
Medium n_samples, small n_clusters
Few clusters, even cluster size, non-flat geometry, transductive
Graph distance (e.g. nearest-neighbor graph)

Ward hierarchical clustering
number of clusters or distance threshold
Large n_samples and n_clusters
Many clusters, possibly connectivity constraints, transductive
Distances between points

Agglomerative clustering
number of clusters or distance threshold, linkage type, distance
Large n_samples and n_clusters
Many clusters, possibly connectivity constraints, non Euclidean
distances, transductive
Any pairwise distance

DBSCAN
neighborhood size
Very large n_samples, medium n_clusters
Non-flat geometry, uneven cluster sizes, outlier removal,
transductive
Distances between nearest points

HDBSCAN
minimum cluster membership, minimum point neighbors
large n_samples, medium n_clusters
Non-flat geometry, uneven cluster sizes, outlier removal,
transductive, hierarchical, variable cluster density
Distances between nearest points

OPTICS
minimum cluster membership
Very large n_samples, large n_clusters
Non-flat geometry, uneven cluster sizes, variable cluster density,
outlier removal, transductive
Distances between points

Gaussian mixtures
many
Not scalable
Flat geometry, good for density estimation, inductive
Mahalanobis distances to  centers

BIRCH
branching factor, threshold, optional global clusterer.
Large n_clusters and n_samples
Large dataset, outlier removal, data reduction, inductive
Euclidean distance between points

Bisecting K-Means
number of clusters
Very large n_samples, medium n_clusters
General-purpose, even cluster size, flat geometry,
no empty clusters, inductive, hierarchical
Distances between points




Non-flat geometry clustering is useful when the clusters have a specific
shape, i.e. a non-flat manifold, and the standard euclidean distance is
not the right metric. This case arises in the two top rows of the figure
above.
Gaussian mixture models, useful for clustering, are described in
another chapter of the documentation dedicated to
mixture models. KMeans can be seen as a special case of Gaussian mixture
model with equal covariance per component.
Transductive clustering methods (in contrast to
inductive clustering methods) are not designed to be applied to new,
unseen data.


2.3.2. K-means#
The KMeans algorithm clusters data by trying to separate samples in n
groups of equal variance, minimizing a criterion known as the inertia or
within-cluster sum-of-squares (see below). This algorithm requires the number
of clusters to be specified. It scales well to large numbers of samples and has
been used across a large range of application areas in many different fields.
The k-means algorithm divides a set of \(N\) samples \(X\) into
\(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\)
of the samples in the cluster. The means are commonly called the cluster
“centroids”; note that they are not, in general, points from \(X\),
although they live in the same space.
The K-means algorithm aims to choose centroids that minimise the inertia,
or within-cluster sum-of-squares criterion:

\[\sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2)\]
Inertia can be recognized as a measure of how internally coherent clusters are.
It suffers from various drawbacks:

Inertia makes the assumption that clusters are convex and isotropic,
which is not always the case. It responds poorly to elongated clusters,
or manifolds with irregular shapes.
Inertia is not a normalized metric: we just know that lower values are
better and zero is optimal. But in very high-dimensional spaces, Euclidean
distances tend to become inflated
(this is an instance of the so-called “curse of dimensionality”).
Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to
k-means clustering can alleviate this problem and speed up the
computations.



For more detailed descriptions of the issues shown above and how to address them,
refer to the examples Demonstration of k-means assumptions
and Selecting the number of clusters with silhouette analysis on KMeans clustering.
K-means is often referred to as Lloyd’s algorithm. In basic terms, the
algorithm has three steps. The first step chooses the initial centroids, with
the most basic method being to choose \(k\) samples from the dataset
\(X\). After initialization, K-means consists of looping between the
two other steps. The first step assigns each sample to its nearest centroid.
The second step creates new centroids by taking the mean value of all of the
samples assigned to each previous centroid. The difference between the old
and the new centroids are computed and the algorithm repeats these last two
steps until this value is less than a threshold. In other words, it repeats
until the centroids do not move significantly.


K-means is equivalent to the expectation-maximization algorithm
with a small, all-equal, diagonal covariance matrix.
The algorithm can also be understood through the concept of Voronoi diagrams. First the Voronoi diagram of
the points is calculated using the current centroids. Each segment in the
Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated
to the mean of each segment. The algorithm then repeats this until a stopping
criterion is fulfilled. Usually, the algorithm stops when the relative decrease
in the objective function between iterations is less than the given tolerance
value. This is not the case in this implementation: iteration stops when
centroids move less than the tolerance.
Given enough time, K-means will always converge, however this may be to a local
minimum. This is highly dependent on the initialization of the centroids.
As a result, the computation is often done several times, with different
initializations of the centroids. One method to help address this issue is the
k-means++ initialization scheme, which has been implemented in scikit-learn
(use the init='k-means++' parameter). This initializes the centroids to be
(generally) distant from each other, leading to probably better results than
random initialization, as shown in the reference. For a detailed example of
comaparing different initialization schemes, refer to
A demo of K-Means clustering on the handwritten digits data.
K-means++ can also be called independently to select seeds for other
clustering algorithms, see sklearn.cluster.kmeans_plusplus for details
and example usage.
The algorithm supports sample weights, which can be given by a parameter
sample_weight. This allows to assign more weight to some samples when
computing cluster centers and values of inertia. For example, assigning a
weight of 2 to a sample is equivalent to adding a duplicate of that sample
to the dataset \(X\).
K-means can be used for vector quantization. This is achieved using the
transform method of a trained model of KMeans. For an example of
performing vector quantization on an image refer to
Color Quantization using K-Means.
Examples

K-means Clustering: Example usage of
KMeans using the iris dataset
Clustering text documents using k-means: Document clustering
using KMeans and MiniBatchKMeans based on sparse data


2.3.2.1. Low-level parallelism#
KMeans benefits from OpenMP based parallelism through Cython. Small
chunks of data (256 samples) are processed in parallel, which in addition
yields a low memory footprint. For more details on how to control the number of
threads, please refer to our Parallelism notes.
Examples

Demonstration of k-means assumptions: Demonstrating when
k-means performs intuitively and when it does not
A demo of K-Means clustering on the handwritten digits data: Clustering handwritten digits



References#

“k-means++: The advantages of careful seeding”
Arthur, David, and Sergei Vassilvitskii,
Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
algorithms, Society for Industrial and Applied Mathematics (2007)




2.3.2.2. Mini Batch K-Means#
The MiniBatchKMeans is a variant of the KMeans algorithm
which uses mini-batches to reduce the computation time, while still attempting
to optimise the same objective function. Mini-batches are subsets of the input
data, randomly sampled in each training iteration. These mini-batches
drastically reduce the amount of computation required to converge to a local
solution. In contrast to other algorithms that reduce the convergence time of
k-means, mini-batch k-means produces results that are generally only slightly
worse than the standard algorithm.
The algorithm iterates between two major steps, similar to vanilla k-means.
In the first step, \(b\) samples are drawn randomly from the dataset, to form
a mini-batch. These are then assigned to the nearest centroid. In the second
step, the centroids are updated. In contrast to k-means, this is done on a
per-sample basis. For each sample in the mini-batch, the assigned centroid
is updated by taking the streaming average of the sample and all previous
samples assigned to that centroid. This has the effect of decreasing the
rate of change for a centroid over time. These steps are performed until
convergence or a predetermined number of iterations is reached.
MiniBatchKMeans converges faster than KMeans, but the quality
of the results is reduced. In practice this difference in quality can be quite
small, as shown in the example and cited reference.




Examples

Comparison of the K-Means and MiniBatchKMeans clustering algorithms: Comparison of
KMeans and MiniBatchKMeans
Clustering text documents using k-means: Document clustering
using KMeans and MiniBatchKMeans based on sparse data
Online learning of a dictionary of parts of faces



References#

“Web Scale K-Means clustering”
D. Sculley, Proceedings of the 19th international conference on World
wide web (2010)





2.3.3. Affinity Propagation#
AffinityPropagation creates clusters by sending messages between
pairs of samples until convergence. A dataset is then described using a small
number of exemplars, which are identified as those most representative of other
samples. The messages sent between pairs represent the suitability for one
sample to be the exemplar of the other, which is updated in response to the
values from other pairs. This updating happens iteratively until convergence,
at which point the final exemplars are chosen, and hence the final clustering
is given.




Affinity Propagation can be interesting as it chooses the number of
clusters based on the data provided. For this purpose, the two important
parameters are the preference, which controls how many exemplars are
used, and the damping factor which damps the responsibility and
availability messages to avoid numerical oscillations when updating these
messages.
The main drawback of Affinity Propagation is its complexity. The
algorithm has a time complexity of the order \(O(N^2 T)\), where \(N\)
is the number of samples and \(T\) is the number of iterations until
convergence. Further, the memory complexity is of the order
\(O(N^2)\) if a dense similarity matrix is used, but reducible if a
sparse similarity matrix is used. This makes Affinity Propagation most
appropriate for small to medium sized datasets.


Algorithm description#
The messages sent between points belong to one of two categories. The first is
the responsibility \(r(i, k)\), which is the accumulated evidence that
sample \(k\) should be the exemplar for sample \(i\). The second is the
availability \(a(i, k)\) which is the accumulated evidence that sample
\(i\) should choose sample \(k\) to be its exemplar, and considers the
values for all other samples that \(k\) should be an exemplar. In this way,
exemplars are chosen by samples if they are (1) similar enough to many samples
and (2) chosen by many samples to be representative of themselves.
More formally, the responsibility of a sample \(k\) to be the exemplar of
sample \(i\) is given by:

\[r(i, k) \leftarrow s(i, k) - max [ a(i, k') + s(i, k') \forall k' \neq k ]\]
Where \(s(i, k)\) is the similarity between samples \(i\) and \(k\).
The availability of sample \(k\) to be the exemplar of sample \(i\) is
given by:

\[a(i, k) \leftarrow min [0, r(k, k) + \sum_{i'~s.t.~i' \notin \{i, k\}}{r(i',
k)}]\]
To begin with, all values for \(r\) and \(a\) are set to zero, and the
calculation of each iterates until convergence. As discussed above, in order to
avoid numerical oscillations when updating the messages, the damping factor
\(\lambda\) is introduced to iteration process:

\[r_{t+1}(i, k) = \lambda\cdot r_{t}(i, k) + (1-\lambda)\cdot r_{t+1}(i, k)\]

\[a_{t+1}(i, k) = \lambda\cdot a_{t}(i, k) + (1-\lambda)\cdot a_{t+1}(i, k)\]
where \(t\) indicates the iteration times.

Examples

Demo of affinity propagation clustering algorithm: Affinity
Propagation on a synthetic 2D datasets with 3 classes
Visualizing the stock market structure Affinity Propagation
on financial time series to find groups of companies



2.3.4. Mean Shift#
MeanShift clustering aims to discover blobs in a smooth density of
samples. It is a centroid based algorithm, which works by updating candidates
for centroids to be the mean of the points within a given region. These
candidates are then filtered in a post-processing stage to eliminate
near-duplicates to form the final set of centroids.


Mathematical details#
The position of centroid candidates is iteratively adjusted using a technique
called hill climbing, which finds local maxima of the estimated probability
density. Given a candidate centroid \(x\) for iteration \(t\), the
candidate is updated according to the following equation:

\[x^{t+1} = x^t + m(x^t)\]
Where \(m\) is the mean shift vector that is computed for each centroid
that points towards a region of the maximum increase in the density of points.
To compute \(m\) we define \(N(x)\) as the neighborhood of samples
within a given distance around \(x\). Then \(m\) is computed using the
following equation, effectively updating a centroid to be the mean of the
samples within its neighborhood:

\[m(x) = \frac{1}{|N(x)|} \sum_{x_j \in N(x)}x_j - x\]
In general, the equation for \(m\) depends on a kernel used for density
estimation. The generic formula is:

\[m(x) = \frac{\sum_{x_j \in N(x)}K(x_j - x)x_j}{\sum_{x_j \in N(x)}K(x_j -
x)} - x\]
In our implementation, \(K(x)\) is equal to 1 if \(x\) is small enough
and is equal to 0 otherwise. Effectively \(K(y - x)\) indicates whether
\(y\) is in the neighborhood of \(x\).

The algorithm automatically sets the number of clusters, instead of relying on a
parameter bandwidth, which dictates the size of the region to search through.
This parameter can be set manually, but can be estimated using the provided
estimate_bandwidth function, which is called if the bandwidth is not set.
The algorithm is not highly scalable, as it requires multiple nearest neighbor
searches during the execution of the algorithm. The algorithm is guaranteed to
converge, however the algorithm will stop iterating when the change in centroids
is small.
Labelling a new sample is performed by finding the nearest centroid for a
given sample.




Examples

A demo of the mean-shift clustering algorithm: Mean Shift clustering
on a synthetic 2D datasets with 3 classes.



References#

“Mean shift: A robust approach toward feature space analysis” D. Comaniciu and P. Meer, IEEE Transactions on Pattern
Analysis and Machine Intelligence (2002)




2.3.5. Spectral clustering#
SpectralClustering performs a low-dimension embedding of the
affinity matrix between samples, followed by clustering, e.g., by KMeans,
of the components of the eigenvectors in the low dimensional space.
It is especially computationally efficient if the affinity matrix is sparse
and the amg solver is used for the eigenvalue problem (Note, the amg solver
requires that the pyamg module is installed.)
The present version of SpectralClustering requires the number of clusters
to be specified in advance. It works well for a small number of clusters,
but is not advised for many clusters.
For two clusters, SpectralClustering solves a convex relaxation of the
normalized cuts
problem on the similarity graph: cutting the graph in two so that the weight of
the edges cut is small compared to the weights of the edges inside each
cluster. This criteria is especially interesting when working on images, where
graph vertices are pixels, and weights of the edges of the similarity graph are
computed using a function of a gradient of the image.

 
Warning
Transforming distance to well-behaved similarities
Note that if the values of your similarity matrix are not well
distributed, e.g. with negative values or with a distance matrix
rather than a similarity, the spectral problem will be singular and
the problem not solvable. In which case it is advised to apply a
transformation to the entries of the matrix. For instance, in the
case of a signed distance matrix, is common to apply a heat kernel:
similarity = np.exp(-beta * distance / distance.std())


See the examples for such an application.

Examples

Spectral clustering for image segmentation: Segmenting objects
from a noisy background using spectral clustering.
Segmenting the picture of greek coins in regions: Spectral clustering
to split the image of coins in regions.


2.3.5.1. Different label assignment strategies#
Different label assignment strategies can be used, corresponding to the
assign_labels parameter of SpectralClustering.
""kmeans"" strategy can match finer details, but can be unstable.
In particular, unless you control the random_state, it may not be
reproducible from run-to-run, as it depends on random initialization.
The alternative ""discretize"" strategy is 100% reproducible, but tends
to create parcels of fairly even and geometrical shape.
The recently added ""cluster_qr"" option is a deterministic alternative that
tends to create the visually best partitioning on the example application
below.


assign_labels=""kmeans""
assign_labels=""discretize""
assign_labels=""cluster_qr""












References#

“Multiclass spectral clustering”
Stella X. Yu, Jianbo Shi, 2003
“Simple, direct, and efficient multi-way spectral clustering”
Anil Damle, Victor Minden, Lexing Ying, 2019




2.3.5.2. Spectral Clustering Graphs#
Spectral Clustering can also be used to partition graphs via their spectral
embeddings.  In this case, the affinity matrix is the adjacency matrix of the
graph, and SpectralClustering is initialized with affinity='precomputed':
>>> from sklearn.cluster import SpectralClustering
>>> sc = SpectralClustering(3, affinity='precomputed', n_init=100,
...                         assign_labels='discretize')
>>> sc.fit_predict(adjacency_matrix)  




References#

“A Tutorial on Spectral Clustering” Ulrike
von Luxburg, 2007
“Normalized cuts and image segmentation” Jianbo
Shi, Jitendra Malik, 2000
“A Random Walks View of Spectral Segmentation”
Marina Meila, Jianbo Shi, 2001
“On Spectral Clustering: Analysis and an algorithm”
Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001
“Preconditioned Spectral Clustering for Stochastic Block Partition
Streaming Graph Challenge” David Zhuzhunashvili, Andrew Knyazev





2.3.6. Hierarchical clustering#
Hierarchical clustering is a general family of clustering algorithms that
build nested clusters by merging or splitting them successively. This
hierarchy of clusters is represented as a tree (or dendrogram). The root of the
tree is the unique cluster that gathers all the samples, the leaves being the
clusters with only one sample. See the Wikipedia page for more details.
The AgglomerativeClustering object performs a hierarchical clustering
using a bottom up approach: each observation starts in its own cluster, and
clusters are successively merged together. The linkage criteria determines the
metric used for the merge strategy:

Ward minimizes the sum of squared differences within all clusters. It is a
variance-minimizing approach and in this sense is similar to the k-means
objective function but tackled with an agglomerative hierarchical
approach.
Maximum or complete linkage minimizes the maximum distance between
observations of pairs of clusters.
Average linkage minimizes the average of the distances between all
observations of pairs of clusters.
Single linkage minimizes the distance between the closest
observations of pairs of clusters.

AgglomerativeClustering can also scale to large number of samples
when it is used jointly with a connectivity matrix, but is computationally
expensive when no connectivity constraints are added between samples: it
considers at each step all the possible merges.

FeatureAgglomeration
The FeatureAgglomeration uses agglomerative clustering to
group together features that look very similar, thus decreasing the
number of features. It is a dimensionality reduction tool, see
Unsupervised dimensionality reduction.


2.3.6.1. Different linkage type: Ward, complete, average, and single linkage#
AgglomerativeClustering supports Ward, single, average, and complete
linkage strategies.


Agglomerative cluster has a “rich get richer” behavior that leads to
uneven cluster sizes. In this regard, single linkage is the worst
strategy, and Ward gives the most regular sizes. However, the affinity
(or distance used in clustering) cannot be varied with Ward, thus for non
Euclidean metrics, average linkage is a good alternative. Single linkage,
while not robust to noisy data, can be computed very efficiently and can
therefore be useful to provide hierarchical clustering of larger datasets.
Single linkage can also perform well on non-globular data.
Examples

Various Agglomerative Clustering on a 2D embedding of digits: exploration of the
different linkage strategies in a real dataset.

Comparing different hierarchical linkage methods on toy datasets: exploration of
the different linkage strategies in toy datasets.





2.3.6.2. Visualization of cluster hierarchy#
It’s possible to visualize the tree representing the hierarchical merging of clusters
as a dendrogram. Visual inspection can often be useful for understanding the structure
of the data, though more so in the case of small sample sizes.


Examples

Plot Hierarchical Clustering Dendrogram



2.3.6.3. Adding connectivity constraints#
An interesting aspect of AgglomerativeClustering is that
connectivity constraints can be added to this algorithm (only adjacent
clusters can be merged together), through a connectivity matrix that defines
for each sample the neighboring samples following a given structure of the
data. For instance, in the swiss-roll example below, the connectivity
constraints forbid the merging of points that are not adjacent on the swiss
roll, and thus avoid forming clusters that extend across overlapping folds of
the roll.

 These constraint are useful to impose a certain local structure, but they
also make the algorithm faster, especially when the number of the samples
is high.
The connectivity constraints are imposed via an connectivity matrix: a
scipy sparse matrix that has elements only at the intersection of a row
and a column with indices of the dataset that should be connected. This
matrix can be constructed from a-priori information: for instance, you
may wish to cluster web pages by only merging pages with a link pointing
from one to another. It can also be learned from the data, for instance
using sklearn.neighbors.kneighbors_graph to restrict
merging to nearest neighbors as in this example, or
using sklearn.feature_extraction.image.grid_to_graph to
enable only merging of neighboring pixels on an image, as in the
coin example.

Warning
Connectivity constraints with single, average and complete linkage
Connectivity constraints and single, complete or average linkage can enhance
the ‘rich getting richer’ aspect of agglomerative clustering,
particularly so if they are built with
sklearn.neighbors.kneighbors_graph. In the limit of a small
number of clusters, they tend to give a few macroscopically occupied
clusters and almost empty ones. (see the discussion in
Agglomerative clustering with and without structure).
Single linkage is the most brittle linkage option with regard to this issue.









Examples

A demo of structured Ward hierarchical clustering on an image of coins: Ward
clustering to split the image of coins in regions.
Hierarchical clustering: structured vs unstructured ward: Example
of Ward algorithm on a swiss-roll, comparison of structured approaches
versus unstructured approaches.
Feature agglomeration vs. univariate selection: Example
of dimensionality reduction with feature agglomeration based on Ward
hierarchical clustering.
Agglomerative clustering with and without structure



2.3.6.4. Varying the metric#
Single, average and complete linkage can be used with a variety of distances (or
affinities), in particular Euclidean distance (l2), Manhattan distance
(or Cityblock, or l1), cosine distance, or any precomputed affinity
matrix.

l1 distance is often good for sparse features, or sparse noise: i.e.
many of the features are zero, as in text mining using occurrences of
rare words.
cosine distance is interesting because it is invariant to global
scalings of the signal.

The guidelines for choosing a metric is to use one that maximizes the
distance between samples in different classes, and minimizes that within
each class.






Examples

Agglomerative clustering with different metrics



2.3.6.5. Bisecting K-Means#
The BisectingKMeans is an iterative variant of KMeans, using
divisive hierarchical clustering. Instead of creating all centroids at once, centroids
are picked progressively based on a previous clustering: a cluster is split into two
new clusters repeatedly until the target number of clusters is reached.
BisectingKMeans is more efficient than KMeans when the number of
clusters is large since it only works on a subset of the data at each bisection
while KMeans always works on the entire dataset.
Although BisectingKMeans can’t benefit from the advantages of the ""k-means++""
initialization by design, it will still produce comparable results than
KMeans(init=""k-means++"") in terms of inertia at cheaper computational costs, and will
likely produce better results than KMeans with a random initialization.
This variant is more efficient to agglomerative clustering if the number of clusters is
small compared to the number of data points.
This variant also does not produce empty clusters.

There exist two strategies for selecting the cluster to split:
bisecting_strategy=""largest_cluster"" selects the cluster having the most points
bisecting_strategy=""biggest_inertia"" selects the cluster with biggest inertia
(cluster with biggest Sum of Squared Errors within)



Picking by largest amount of data points in most cases produces result as
accurate as picking by inertia and is faster (especially for larger amount of data
points, where calculating error may be costly).
Picking by largest amount of data points will also likely produce clusters of similar
sizes while KMeans is known to produce clusters of different sizes.
Difference between Bisecting K-Means and regular K-Means can be seen on example
Bisecting K-Means and Regular K-Means Performance Comparison.
While the regular K-Means algorithm tends to create non-related clusters,
clusters from Bisecting K-Means are well ordered and create quite a visible hierarchy.


References#

“A Comparison of Document Clustering Techniques” Michael
Steinbach, George Karypis and Vipin Kumar, Department of Computer Science and
Egineering, University of Minnesota (June 2000)
“Performance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog
Data”
K.Abirami and Dr.P.Mayilvahanan, International Journal of Emerging
Technologies in Engineering Research (IJETER) Volume 4, Issue 8, (August 2016)
“Bisecting K-means Algorithm Based on K-valued Self-determining and
Clustering Center Optimization” Jian Di, Xinyue Gou School
of Control and Computer Engineering,North China Electric Power University,
Baoding, Hebei, China (August 2017)





2.3.7. DBSCAN#
The DBSCAN algorithm views clusters as areas of high density
separated by areas of low density. Due to this rather generic view, clusters
found by DBSCAN can be any shape, as opposed to k-means which assumes that
clusters are convex shaped. The central component to the DBSCAN is the concept
of core samples, which are samples that are in areas of high density. A
cluster is therefore a set of core samples, each close to each other
(measured by some distance measure)
and a set of non-core samples that are close to a core sample (but are not
themselves core samples). There are two parameters to the algorithm,
min_samples and eps,
which define formally what we mean when we say dense.
Higher min_samples or lower eps
indicate higher density necessary to form a cluster.
More formally, we define a core sample as being a sample in the dataset such
that there exist min_samples other samples within a distance of
eps, which are defined as neighbors of the core sample. This tells
us that the core sample is in a dense area of the vector space. A cluster
is a set of core samples that can be built by recursively taking a core
sample, finding all of its neighbors that are core samples, finding all of
their neighbors that are core samples, and so on. A cluster also has a
set of non-core samples, which are samples that are neighbors of a core sample
in the cluster but are not themselves core samples. Intuitively, these samples
are on the fringes of a cluster.
Any core sample is part of a cluster, by definition. Any sample that is not a
core sample, and is at least eps in distance from any core sample, is
considered an outlier by the algorithm.
While the parameter min_samples primarily controls how tolerant the
algorithm is towards noise (on noisy and large data sets it may be desirable
to increase this parameter), the parameter eps is crucial to choose
appropriately for the data set and distance function and usually cannot be
left at the default value. It controls the local neighborhood of the points.
When chosen too small, most data will not be clustered at all (and labeled
as -1 for “noise”). When chosen too large, it causes close clusters to
be merged into one cluster, and eventually the entire data set to be returned
as a single cluster. Some heuristics for choosing this parameter have been
discussed in the literature, for example based on a knee in the nearest neighbor
distances plot (as discussed in the references below).
In the figure below, the color indicates cluster membership, with large circles
indicating core samples found by the algorithm. Smaller circles are non-core
samples that are still part of a cluster. Moreover, the outliers are indicated
by black points below.

Examples

Demo of DBSCAN clustering algorithm



Implementation#
The DBSCAN algorithm is deterministic, always generating the same clusters when
given the same data in the same order.  However, the results can differ when
data is provided in a different order. First, even though the core samples will
always be assigned to the same clusters, the labels of those clusters will
depend on the order in which those samples are encountered in the data. Second
and more importantly, the clusters to which non-core samples are assigned can
differ depending on the data order.  This would happen when a non-core sample
has a distance lower than eps to two core samples in different clusters. By
the triangular inequality, those two core samples must be more distant than
eps from each other, or they would be in the same cluster. The non-core
sample is assigned to whichever cluster is generated first in a pass through the
data, and so the results will depend on the data ordering.
The current implementation uses ball trees and kd-trees to determine the
neighborhood of points, which avoids calculating the full distance matrix (as
was done in scikit-learn versions before 0.14). The possibility to use custom
metrics is retained; for details, see NearestNeighbors.



Memory consumption for large sample sizes#
This implementation is by default not memory efficient because it constructs a
full pairwise similarity matrix in the case where kd-trees or ball-trees cannot
be used (e.g., with sparse matrices). This matrix will consume \(n^2\)
floats. A couple of mechanisms for getting around this are:

Use OPTICS clustering in conjunction with the extract_dbscan
method. OPTICS clustering also calculates the full pairwise matrix, but only
keeps one row in memory at a time (memory complexity n).
A sparse radius neighborhood graph (where missing entries are presumed to be
out of eps) can be precomputed in a memory-efficient way and dbscan can be run
over this with metric='precomputed'.  See
sklearn.neighbors.NearestNeighbors.radius_neighbors_graph.
The dataset can be compressed, either by removing exact duplicates if these
occur in your data, or by using BIRCH. Then you only have a relatively small
number of representatives for a large number of points. You can then provide a
sample_weight when fitting DBSCAN.




References#


A Density-Based Algorithm for Discovering Clusters in Large Spatial
Databases with Noise
Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd
International Conference on Knowledge Discovery and Data Mining, Portland, OR,
AAAI Press, pp. 226-231. 1996
DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu,
X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19.



2.3.8. HDBSCAN#
The HDBSCAN algorithm can be seen as an extension of DBSCAN
and OPTICS. Specifically, DBSCAN assumes that the clustering
criterion (i.e. density requirement) is globally homogeneous.
In other words, DBSCAN may struggle to successfully capture clusters
with different densities.
HDBSCAN alleviates this assumption and explores all possible density
scales by building an alternative representation of the clustering problem.

Note
This implementation is adapted from the original implementation of HDBSCAN,
scikit-learn-contrib/hdbscan based on [LJ2017].

Examples

Demo of HDBSCAN clustering algorithm


2.3.8.1. Mutual Reachability Graph#
HDBSCAN first defines \(d_c(x_p)\), the core distance of a sample \(x_p\), as the
distance to its min_samples th-nearest neighbor, counting itself. For example,
if min_samples=5 and \(x_*\) is the 5th-nearest neighbor of \(x_p\)
then the core distance is:

\[d_c(x_p)=d(x_p, x_*).\]
Next it defines \(d_m(x_p, x_q)\), the mutual reachability distance of two points
\(x_p, x_q\), as:

\[d_m(x_p, x_q) = \max\{d_c(x_p), d_c(x_q), d(x_p, x_q)\}\]
These two notions allow us to construct the mutual reachability graph
\(G_{ms}\) defined for a fixed choice of min_samples by associating each
sample \(x_p\) with a vertex of the graph, and thus edges between points
\(x_p, x_q\) are the mutual reachability distance \(d_m(x_p, x_q)\)
between them. We may build subsets of this graph, denoted as
\(G_{ms,\varepsilon}\), by removing any edges with value greater than \(\varepsilon\):
from the original graph. Any points whose core distance is less than \(\varepsilon\):
are at this staged marked as noise. The remaining points are then clustered by
finding the connected components of this trimmed graph.

Note
Taking the connected components of a trimmed graph \(G_{ms,\varepsilon}\) is
equivalent to running DBSCAN* with min_samples and \(\varepsilon\). DBSCAN* is a
slightly modified version of DBSCAN mentioned in [CM2013].



2.3.8.2. Hierarchical Clustering#
HDBSCAN can be seen as an algorithm which performs DBSCAN* clustering across all
values of \(\varepsilon\). As mentioned prior, this is equivalent to finding the connected
components of the mutual reachability graphs for all values of \(\varepsilon\). To do this
efficiently, HDBSCAN first extracts a minimum spanning tree (MST) from the fully
-connected mutual reachability graph, then greedily cuts the edges with highest
weight. An outline of the HDBSCAN algorithm is as follows:

Extract the MST of \(G_{ms}\).
Extend the MST by adding a “self edge” for each vertex, with weight equal
to the core distance of the underlying sample.
Initialize a single cluster and label for the MST.
Remove the edge with the greatest weight from the MST (ties are
removed simultaneously).
Assign cluster labels to the connected components which contain the
end points of the now-removed edge. If the component does not have at least
one edge it is instead assigned a “null” label marking it as noise.
Repeat 4-5 until there are no more connected components.

HDBSCAN is therefore able to obtain all possible partitions achievable by
DBSCAN* for a fixed choice of min_samples in a hierarchical fashion.
Indeed, this allows HDBSCAN to perform clustering across multiple densities
and as such it no longer needs \(\varepsilon\) to be given as a hyperparameter. Instead
it relies solely on the choice of min_samples, which tends to be a more robust
hyperparameter.


HDBSCAN can be smoothed with an additional hyperparameter min_cluster_size
which specifies that during the hierarchical clustering, components with fewer
than minimum_cluster_size many samples are considered noise. In practice, one
can set minimum_cluster_size = min_samples to couple the parameters and
simplify the hyperparameter space.
References


[CM2013]
Campello, R.J.G.B., Moulavi, D., Sander, J. (2013). Density-Based
Clustering Based on Hierarchical Density Estimates. In: Pei, J., Tseng, V.S.,
Cao, L., Motoda, H., Xu, G. (eds) Advances in Knowledge Discovery and Data
Mining. PAKDD 2013. Lecture Notes in Computer Science(), vol 7819. Springer,
Berlin, Heidelberg. Density-Based Clustering Based on Hierarchical
Density Estimates


[LJ2017]
L. McInnes and J. Healy, (2017). Accelerated Hierarchical Density
Based Clustering. In: IEEE International Conference on Data Mining Workshops
(ICDMW), 2017, pp. 33-42. Accelerated Hierarchical Density Based
Clustering





2.3.9. OPTICS#
The OPTICS algorithm shares many similarities with the DBSCAN
algorithm, and can be considered a generalization of DBSCAN that relaxes the
eps requirement from a single value to a value range. The key difference
between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability
graph, which assigns each sample both a reachability_ distance, and a spot
within the cluster ordering_ attribute; these two attributes are assigned
when the model is fitted, and are used to determine cluster membership. If
OPTICS is run with the default value of inf set for max_eps, then DBSCAN
style cluster extraction can be performed repeatedly in linear time for any
given eps value using the cluster_optics_dbscan method. Setting
max_eps to a lower value will result in shorter run times, and can be
thought of as the maximum neighborhood radius from each point to find other
potential reachable points.

The reachability distances generated by OPTICS allow for variable density
extraction of clusters within a single data set. As shown in the above plot,
combining reachability distances and data set ordering_ produces a
reachability plot, where point density is represented on the Y-axis, and
points are ordered such that nearby points are adjacent. ‘Cutting’ the
reachability plot at a single value produces DBSCAN like results; all points
above the ‘cut’ are classified as noise, and each time that there is a break
when reading from left to right signifies a new cluster. The default cluster
extraction with OPTICS looks at the steep slopes within the graph to find
clusters, and the user can define what counts as a steep slope using the
parameter xi. There are also other possibilities for analysis on the graph
itself, such as generating hierarchical representations of the data through
reachability-plot dendrograms, and the hierarchy of clusters detected by the
algorithm can be accessed through the cluster_hierarchy_ parameter. The
plot above has been color-coded so that cluster colors in planar space match
the linear segment clusters of the reachability plot. Note that the blue and
red clusters are adjacent in the reachability plot, and can be hierarchically
represented as children of a larger parent cluster.
Examples

Demo of OPTICS clustering algorithm



Comparison with DBSCAN#
The results from OPTICS cluster_optics_dbscan method and DBSCAN are very
similar, but not always identical; specifically, labeling of periphery and noise
points. This is in part because the first samples of each dense area processed
by OPTICS have a large reachability value while being close to other points in
their area, and will thus sometimes be marked as noise rather than periphery.
This affects adjacent points when they are considered as candidates for being
marked as either periphery or noise.
Note that for any single value of eps, DBSCAN will tend to have a shorter
run time than OPTICS; however, for repeated runs at varying eps values, a
single run of OPTICS may require less cumulative runtime than DBSCAN. It is also
important to note that OPTICS’ output is close to DBSCAN’s only if eps and
max_eps are close.



Computational Complexity#
Spatial indexing trees are used to avoid calculating the full distance matrix,
and allow for efficient memory usage on large sets of samples. Different
distance metrics can be supplied via the metric keyword.
For large datasets, similar (but not identical) results can be obtained via
HDBSCAN. The HDBSCAN implementation is multithreaded, and has better
algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling.
For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS
will maintain \(n\) (as opposed to \(n^2\)) memory scaling; however,
tuning of the max_eps parameter will likely need to be used to give a
solution in a reasonable amount of wall time.



References#

“OPTICS: ordering points to identify the clustering structure.” Ankerst,
Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. In ACM Sigmod
Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.




2.3.10. BIRCH#
The Birch builds a tree called the Clustering Feature Tree (CFT)
for the given data. The data is essentially lossy compressed to a set of
Clustering Feature nodes (CF Nodes). The CF Nodes have a number of
subclusters called Clustering Feature subclusters (CF Subclusters)
and these CF Subclusters located in the non-terminal CF Nodes
can have CF Nodes as children.
The CF Subclusters hold the necessary information for clustering which prevents
the need to hold the entire input data in memory. This information includes:

Number of samples in a subcluster.
Linear Sum - An n-dimensional vector holding the sum of all samples
Squared Sum - Sum of the squared L2 norm of all samples.
Centroids - To avoid recalculation linear sum / n_samples.
Squared norm of the centroids.

The BIRCH algorithm has two parameters, the threshold and the branching factor.
The branching factor limits the number of subclusters in a node and the
threshold limits the distance between the entering sample and the existing
subclusters.
This algorithm can be viewed as an instance or data reduction method,
since it reduces the input data to a set of subclusters which are obtained directly
from the leaves of the CFT. This reduced data can be further processed by feeding
it into a global clusterer. This global clusterer can be set by n_clusters.
If n_clusters is set to None, the subclusters from the leaves are directly
read off, otherwise a global clustering step labels these subclusters into global
clusters (labels) and the samples are mapped to the global label of the nearest subcluster.


Algorithm description#

A new sample is inserted into the root of the CF Tree which is a CF Node. It
is then merged with the subcluster of the root, that has the smallest radius
after merging, constrained by the threshold and branching factor conditions.
If the subcluster has any child node, then this is done repeatedly till it
reaches a leaf. After finding the nearest subcluster in the leaf, the
properties of this subcluster and the parent subclusters are recursively
updated.
If the radius of the subcluster obtained by merging the new sample and the
nearest subcluster is greater than the square of the threshold and if the
number of subclusters is greater than the branching factor, then a space is
temporarily allocated to this new sample. The two farthest subclusters are
taken and the subclusters are divided into two groups on the basis of the
distance between these subclusters.
If this split node has a parent subcluster and there is room for a new
subcluster, then the parent is split into two. If there is no room, then this
node is again split into two and the process is continued recursively, till it
reaches the root.




BIRCH or MiniBatchKMeans?#

BIRCH does not scale very well to high dimensional data. As a rule of thumb if
n_features is greater than twenty, it is generally better to use MiniBatchKMeans.
If the number of instances of data needs to be reduced, or if one wants a
large number of subclusters either as a preprocessing step or otherwise,
BIRCH is more useful than MiniBatchKMeans.






How to use partial_fit?#
To avoid the computation of global clustering, for every call of partial_fit
the user is advised:

To set n_clusters=None initially.
Train all data by multiple calls to partial_fit.
Set n_clusters to a required value using
brc.set_params(n_clusters=n_clusters).
Call partial_fit finally with no arguments, i.e. brc.partial_fit()
which performs the global clustering.




References#

Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data
clustering method for large databases.
https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf
Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm
https://code.google.com/archive/p/jbirch




2.3.11. Clustering performance evaluation#
Evaluating the performance of a clustering algorithm is not as trivial as
counting the number of errors or the precision and recall of a supervised
classification algorithm. In particular any evaluation metric should not
take the absolute values of the cluster labels into account but rather
if this clustering define separations of the data similar to some ground
truth set of classes or satisfying some assumption such that members
belong to the same class are more similar than members of different
classes according to some similarity metric.

2.3.11.1. Rand index#
Given the knowledge of the ground truth class assignments
labels_true and our clustering algorithm assignments of the same
samples labels_pred, the (adjusted or unadjusted) Rand index
is a function that measures the similarity of the two assignments,
ignoring permutations:
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.rand_score(labels_true, labels_pred)
0.66...


The Rand index does not ensure to obtain a value close to 0.0 for a
random labelling. The adjusted Rand index corrects for chance and
will give such a baseline.
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
0.24...


As with all clustering metrics, one can permute 0 and 1 in the predicted
labels, rename 2 to 3, and get the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.rand_score(labels_true, labels_pred)
0.66...
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
0.24...


Furthermore, both rand_score adjusted_rand_score are
symmetric: swapping the argument does not change the scores. They can
thus be used as consensus measures:
>>> metrics.rand_score(labels_pred, labels_true)
0.66...
>>> metrics.adjusted_rand_score(labels_pred, labels_true)
0.24...


Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.rand_score(labels_true, labels_pred)
1.0
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
1.0


Poorly agreeing labels (e.g. independent labelings) have lower scores,
and for the adjusted Rand index the score will be negative or close to
zero. However, for the unadjusted Rand index the score, while lower,
will not necessarily be close to zero.:
>>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1]
>>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]
>>> metrics.rand_score(labels_true, labels_pred)
0.39...
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
-0.07...



Advantages:

Interpretability: The unadjusted Rand index is proportional to the
number of sample pairs whose labels are the same in both labels_pred and
labels_true, or are different in both.
Random (uniform) label assignments have an adjusted Rand index score close
to 0.0 for any value of n_clusters and n_samples (which is not the
case for the unadjusted Rand index or the V-measure for instance).
Bounded range: Lower values indicate different labelings, similar
clusterings have a high (adjusted or unadjusted) Rand index, 1.0 is the
perfect match score. The score range is [0, 1] for the unadjusted Rand index
and [-0.5, 1] for the adjusted Rand index.
No assumption is made on the cluster structure: The (adjusted or
unadjusted) Rand index can be used to compare all kinds of clustering
algorithms, and can be used to compare clustering algorithms such as k-means
which assumes isotropic blob shapes with results of spectral clustering
algorithms which can find cluster with “folded” shapes.



Drawbacks:

Contrary to inertia, the (adjusted or unadjusted) Rand index requires
knowledge of the ground truth classes which is almost never available in
practice or requires manual assignment by human annotators (as in the
supervised learning setting).
However (adjusted or unadjusted) Rand index can also be useful in a purely
unsupervised setting as a building block for a Consensus Index that can be
used for clustering model selection (TODO).

The unadjusted Rand index is often close to 1.0 even if the clusterings
themselves differ significantly. This can be understood when interpreting
the Rand index as the accuracy of element pair labeling resulting from the
clusterings: In practice there often is a majority of element pairs that are
assigned the different pair label under both the predicted and the
ground truth clustering resulting in a high proportion of pair labels that
agree, which leads subsequently to a high score.


Examples

Adjustment for chance in clustering performance evaluation:
Analysis of the impact of the dataset size on the value of
clustering measures for random assignments.



Mathematical formulation#
If C is a ground truth class assignment and K the clustering, let us define
\(a\) and \(b\) as:

\(a\), the number of pairs of elements that are in the same set in C and
in the same set in K
\(b\), the number of pairs of elements that are in different sets in C and
in different sets in K

The unadjusted Rand index is then given by:

\[\text{RI} = \frac{a + b}{C_2^{n_{samples}}}\]
where \(C_2^{n_{samples}}\) is the total number of possible pairs in the
dataset. It does not matter if the calculation is performed on ordered pairs or
unordered pairs as long as the calculation is performed consistently.
However, the Rand index does not guarantee that random label assignments will
get a value close to zero (esp. if the number of clusters is in the same order
of magnitude as the number of samples).
To counter this effect we can discount the expected RI \(E[\text{RI}]\) of
random labelings by defining the adjusted Rand index as follows:

\[\text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}\]



References#

Comparing Partitions L. Hubert and P.
Arabie, Journal of Classification 1985
Properties of the Hubert-Arabie adjusted Rand index D. Steinley, Psychological
Methods 2004
Wikipedia entry for the Rand index
Minimum adjusted Rand index for two clusterings of a given size, 2022, J. E. Chacón and A. I. Rastrojo




2.3.11.2. Mutual Information based scores#
Given the knowledge of the ground truth class assignments labels_true and
our clustering algorithm assignments of the same samples labels_pred, the
Mutual Information is a function that measures the agreement of the two
assignments, ignoring permutations.  Two different normalized versions of this
measure are available, Normalized Mutual Information (NMI) and Adjusted
Mutual Information (AMI). NMI is often used in the literature, while AMI was
proposed more recently and is normalized against chance:
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]

>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
0.22504...


One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get
the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
0.22504...


All, mutual_info_score, adjusted_mutual_info_score and
normalized_mutual_info_score are symmetric: swapping the argument does
not change the score. Thus they can be used as a consensus measure:
>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  
0.22504...


Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
1.0

>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  
1.0


This is not true for mutual_info_score, which is therefore harder to judge:
>>> metrics.mutual_info_score(labels_true, labels_pred)  
0.69...


Bad (e.g. independent labelings) have non-positive scores:
>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
-0.10526...



Advantages:

Random (uniform) label assignments have a AMI score close to 0.0 for any
value of n_clusters and n_samples (which is not the case for raw
Mutual Information or the V-measure for instance).
Upper bound  of 1:  Values close to zero indicate two label assignments
that are largely independent, while values close to one indicate significant
agreement. Further, an AMI of exactly 1 indicates that the two label
assignments are equal (with or without permutation).



Drawbacks:

Contrary to inertia, MI-based measures require the knowledge of the ground
truth classes while almost never available in practice or requires manual
assignment by human annotators (as in the supervised learning setting).
However MI-based measures can also be useful in purely unsupervised setting
as a building block for a Consensus Index that can be used for clustering
model selection.

NMI and MI are not adjusted against chance.


Examples

Adjustment for chance in clustering performance evaluation: Analysis
of the impact of the dataset size on the value of clustering measures for random
assignments. This example also includes the Adjusted Rand Index.



Mathematical formulation#
Assume two label assignments (of the same N objects), \(U\) and \(V\).
Their entropy is the amount of uncertainty for a partition set, defined by:

\[H(U) = - \sum_{i=1}^{|U|}P(i)\log(P(i))\]
where \(P(i) = |U_i| / N\) is the probability that an object picked at
random from \(U\) falls into class \(U_i\). Likewise for \(V\):

\[H(V) = - \sum_{j=1}^{|V|}P'(j)\log(P'(j))\]
With \(P'(j) = |V_j| / N\). The mutual information (MI) between \(U\)
and \(V\) is calculated by:

\[\text{MI}(U, V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}P(i, j)\log\left(\frac{P(i,j)}{P(i)P'(j)}\right)\]
where \(P(i, j) = |U_i \cap V_j| / N\) is the probability that an object
picked at random falls into both classes \(U_i\) and \(V_j\).
It also can be expressed in set cardinality formulation:

\[\text{MI}(U, V) = \sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i \cap V_j|}{N}\log\left(\frac{N|U_i \cap V_j|}{|U_i||V_j|}\right)\]
The normalized mutual information is defined as

\[\text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\text{mean}(H(U), H(V))}\]
This value of the mutual information and also the normalized variant is not
adjusted for chance and will tend to increase as the number of different labels
(clusters) increases, regardless of the actual amount of “mutual information”
between the label assignments.
The expected value for the mutual information can be calculated using the
following equation [VEB2009]. In this equation, \(a_i = |U_i|\) (the number
of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in
\(V_j\)).

\[E[\text{MI}(U,V)]=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \sum_{n_{ij}=(a_i+b_j-N)^+
}^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
(N-a_i-b_j+n_{ij})!}\]
Using the expected value, the adjusted mutual information can then be calculated
using a similar form to that of the adjusted Rand index:

\[\text{AMI} = \frac{\text{MI} - E[\text{MI}]}{\text{mean}(H(U), H(V)) - E[\text{MI}]}\]
For normalized mutual information and adjusted mutual information, the
normalizing value is typically some generalized mean of the entropies of each
clustering. Various generalized means exist, and no firm rules exist for
preferring one over the others.  The decision is largely a field-by-field basis;
for instance, in community detection, the arithmetic mean is most common. Each
normalizing method provides “qualitatively similar behaviours” [YAT2016]. In
our implementation, this is controlled by the average_method parameter.
Vinh et al. (2010) named variants of NMI and AMI by their averaging method
[VEB2010]. Their ‘sqrt’ and ‘sum’ averages are the geometric and arithmetic
means; we use these more broadly common names.
References

Strehl, Alexander, and Joydeep Ghosh (2002). “Cluster ensembles - a
knowledge reuse framework for combining multiple partitions”. Journal of
Machine Learning Research 3: 583-617. doi:10.1162/153244303321897735.
Wikipedia entry for the (normalized) Mutual Information
Wikipedia entry for the Adjusted Mutual Information



[VEB2009]
Vinh, Epps, and Bailey, (2009). “Information theoretic measures
for clusterings comparison”. Proceedings of the 26th Annual International
Conference on Machine Learning - ICML ‘09. doi:10.1145/1553374.1553511. ISBN
9781605585161.


[VEB2010]
Vinh, Epps, and Bailey, (2010). “Information Theoretic Measures
for Clusterings Comparison: Variants, Properties, Normalization and
Correction for Chance”. JMLR
<https://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>


[YAT2016]
Yang, Algesheimer, and Tessone, (2016). “A comparative analysis
of community detection algorithms on artificial networks”. Scientific
Reports 6: 30750. doi:10.1038/srep30750.





2.3.11.3. Homogeneity, completeness and V-measure#
Given the knowledge of the ground truth class assignments of the samples,
it is possible to define some intuitive metric using conditional entropy
analysis.
In particular Rosenberg and Hirschberg (2007) define the following two
desirable objectives for any cluster assignment:

homogeneity: each cluster contains only members of a single class.
completeness: all members of a given class are assigned to the same
cluster.

We can turn those concept as scores homogeneity_score and
completeness_score. Both are bounded below by 0.0 and above by
1.0 (higher is better):
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]

>>> metrics.homogeneity_score(labels_true, labels_pred)
0.66...

>>> metrics.completeness_score(labels_true, labels_pred)
0.42...


Their harmonic mean called V-measure is computed by
v_measure_score:
>>> metrics.v_measure_score(labels_true, labels_pred)
0.51...


This function’s formula is as follows:

\[v = \frac{(1 + \beta) \times \text{homogeneity} \times \text{completeness}}{(\beta \times \text{homogeneity} + \text{completeness})}\]
beta defaults to a value of 1.0, but for using a value less than 1 for beta:
>>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)
0.54...


more weight will be attributed to homogeneity, and using a value greater than 1:
>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)
0.48...


more weight will be attributed to completeness.
The V-measure is actually equivalent to the mutual information (NMI)
discussed above, with the aggregation function being the arithmetic mean [B2011].
Homogeneity, completeness and V-measure can be computed at once using
homogeneity_completeness_v_measure as follows:
>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
(0.66..., 0.42..., 0.51...)


The following clustering assignment is slightly better, since it is
homogeneous but not complete:
>>> labels_pred = [0, 0, 0, 1, 2, 2]
>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
(1.0, 0.68..., 0.81...)



Note
v_measure_score is symmetric: it can be used to evaluate
the agreement of two independent assignments on the same dataset.
This is not the case for completeness_score and
homogeneity_score: both are bound by the relationship:
homogeneity_score(a, b) == completeness_score(b, a)




Advantages:

Bounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score.
Intuitive interpretation: clustering with bad V-measure can be
qualitatively analyzed in terms of homogeneity and completeness to
better feel what ‘kind’ of mistakes is done by the assignment.
No assumption is made on the cluster structure: can be used to compare
clustering algorithms such as k-means which assumes isotropic blob shapes
with results of spectral clustering algorithms which can find cluster with
“folded” shapes.



Drawbacks:

The previously introduced metrics are not normalized with regards to
random labeling: this means that depending on the number of samples,
clusters and ground truth classes, a completely random labeling will not
always yield the same values for homogeneity, completeness and hence
v-measure. In particular random labeling won’t yield zero scores
especially when the number of clusters is large.
This problem can safely be ignored when the number of samples is more than a
thousand and the number of clusters is less than 10. For smaller sample
sizes or larger number of clusters it is safer to use an adjusted index such
as the Adjusted Rand Index (ARI).







These metrics require the knowledge of the ground truth classes while
almost never available in practice or requires manual assignment by human
annotators (as in the supervised learning setting).


Examples

Adjustment for chance in clustering performance evaluation: Analysis
of the impact of the dataset size on the value of clustering measures for
random assignments.



Mathematical formulation#
Homogeneity and completeness scores are formally given by:

\[h = 1 - \frac{H(C|K)}{H(C)}\]

\[c = 1 - \frac{H(K|C)}{H(K)}\]
where \(H(C|K)\) is the conditional entropy of the classes given the
cluster assignments and is given by:

\[H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n}
\cdot \log\left(\frac{n_{c,k}}{n_k}\right)\]
and \(H(C)\) is the entropy of the classes and is given by:

\[H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)\]
with \(n\) the total number of samples, \(n_c\) and \(n_k\) the
number of samples respectively belonging to class \(c\) and cluster
\(k\), and finally \(n_{c,k}\) the number of samples from class
\(c\) assigned to cluster \(k\).
The conditional entropy of clusters given class \(H(K|C)\) and the
entropy of clusters \(H(K)\) are defined in a symmetric manner.
Rosenberg and Hirschberg further define V-measure as the harmonic mean of
homogeneity and completeness:

\[v = 2 \cdot \frac{h \cdot c}{h + c}\]

References

V-Measure: A conditional entropy-based external cluster evaluation measure Andrew Rosenberg and Julia
Hirschberg, 2007



[B2011]
Identification and Characterization of Events in Social Media, Hila
Becker, PhD Thesis.




2.3.11.4. Fowlkes-Mallows scores#
The original Fowlkes-Mallows index (FMI) was intended to measure the similarity
between two clustering results, which is inherently an unsupervised comparison.
The supervised adaptation of the Fowlkes-Mallows index
(as implemented in sklearn.metrics.fowlkes_mallows_score) can be used
when the ground truth class assignments of the samples are known.
The FMI is defined as the geometric mean of the pairwise precision and recall:

\[\text{FMI} = \frac{\text{TP}}{\sqrt{(\text{TP} + \text{FP}) (\text{TP} + \text{FN})}}\]
In the above formula:

TP (True Positive): The number of pairs of points that are clustered together
both in the true labels and in the predicted labels.
FP (False Positive): The number of pairs of points that are clustered together
in the predicted labels but not in the true labels.
FN (False Negative): The number of pairs of points that are clustered together
in the true labels but not in the predicted labels.

The score ranges from 0 to 1. A high value indicates a good similarity
between two clusters.
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]


>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.47140...


One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get
the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]

>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.47140...


Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
1.0


Bad (e.g. independent labelings) have zero scores:
>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.0



Advantages:

Random (uniform) label assignments have a FMI score close to 0.0 for any
value of n_clusters and n_samples (which is not the case for raw
Mutual Information or the V-measure for instance).
Upper-bounded at 1:  Values close to zero indicate two label assignments
that are largely independent, while values close to one indicate significant
agreement. Further, values of exactly 0 indicate purely independent
label assignments and a FMI of exactly 1 indicates that the two label
assignments are equal (with or without permutation).
No assumption is made on the cluster structure: can be used to compare
clustering algorithms such as k-means which assumes isotropic blob shapes
with results of spectral clustering algorithms which can find cluster with
“folded” shapes.



Drawbacks:

Contrary to inertia, FMI-based measures require the knowledge of the
ground truth classes while almost never available in practice or requires
manual assignment by human annotators (as in the supervised learning
setting).




References#

E. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two
hierarchical clusterings”. Journal of the American Statistical Association.
https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008
Wikipedia entry for the Fowlkes-Mallows Index




2.3.11.5. Silhouette Coefficient#
If the ground truth labels are not known, evaluation must be performed using
the model itself. The Silhouette Coefficient
(sklearn.metrics.silhouette_score)
is an example of such an evaluation, where a
higher Silhouette Coefficient score relates to a model with better defined
clusters. The Silhouette Coefficient is defined for each sample and is composed
of two scores:

a: The mean distance between a sample and all other points in the same
class.
b: The mean distance between a sample and all other points in the next
nearest cluster.

The Silhouette Coefficient s for a single sample is then given as:

\[s = \frac{b - a}{max(a, b)}\]
The Silhouette Coefficient for a set of samples is given as the mean of the
Silhouette Coefficient for each sample.
>>> from sklearn import metrics
>>> from sklearn.metrics import pairwise_distances
>>> from sklearn import datasets
>>> X, y = datasets.load_iris(return_X_y=True)


In normal usage, the Silhouette Coefficient is applied to the results of a
cluster analysis.
>>> import numpy as np
>>> from sklearn.cluster import KMeans
>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans_model.labels_
>>> metrics.silhouette_score(X, labels, metric='euclidean')
0.55...



Advantages:

The score is bounded between -1 for incorrect clustering and +1 for highly
dense clustering. Scores around zero indicate overlapping clusters.
The score is higher when clusters are dense and well separated, which
relates to a standard concept of a cluster.



Drawbacks:

The Silhouette Coefficient is generally higher for convex clusters than
other concepts of clusters, such as density based clusters like those
obtained through DBSCAN.


Examples

Selecting the number of clusters with silhouette analysis on KMeans clustering : In
this example the silhouette analysis is used to choose an optimal value for
n_clusters.



References#

Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the
Interpretation and Validation of Cluster Analysis”.
Computational and Applied Mathematics 20: 53-65.




2.3.11.6. Calinski-Harabasz Index#
If the ground truth labels are not known, the Calinski-Harabasz index
(sklearn.metrics.calinski_harabasz_score) - also known as the Variance
Ratio Criterion - can be used to evaluate the model, where a higher
Calinski-Harabasz score relates to a model with better defined clusters.
The index is the ratio of the sum of between-clusters dispersion and of
within-cluster dispersion for all clusters (where dispersion is defined as the
sum of distances squared):
>>> from sklearn import metrics
>>> from sklearn.metrics import pairwise_distances
>>> from sklearn import datasets
>>> X, y = datasets.load_iris(return_X_y=True)


In normal usage, the Calinski-Harabasz index is applied to the results of a
cluster analysis:
>>> import numpy as np
>>> from sklearn.cluster import KMeans
>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans_model.labels_
>>> metrics.calinski_harabasz_score(X, labels)
561.59...



Advantages:

The score is higher when clusters are dense and well separated, which
relates to a standard concept of a cluster.
The score is fast to compute.



Drawbacks:

The Calinski-Harabasz index is generally higher for convex clusters than
other concepts of clusters, such as density based clusters like those
obtained through DBSCAN.




Mathematical formulation#
For a set of data \(E\) of size \(n_E\) which has been clustered into
\(k\) clusters, the Calinski-Harabasz score \(s\) is defined as the
ratio of the between-clusters dispersion mean and the within-cluster
dispersion:

\[s = \frac{\mathrm{tr}(B_k)}{\mathrm{tr}(W_k)} \times \frac{n_E - k}{k - 1}\]
where \(\mathrm{tr}(B_k)\) is trace of the between group dispersion matrix
and \(\mathrm{tr}(W_k)\) is the trace of the within-cluster dispersion
matrix defined by:

\[W_k = \sum_{q=1}^k \sum_{x \in C_q} (x - c_q) (x - c_q)^T\]

\[B_k = \sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T\]
with \(C_q\) the set of points in cluster \(q\), \(c_q\) the
center of cluster \(q\), \(c_E\) the center of \(E\), and
\(n_q\) the number of points in cluster \(q\).



References#

Caliński, T., & Harabasz, J. (1974). “A Dendrite Method for Cluster Analysis”.
Communications in Statistics-theory and Methods 3: 1-27.




2.3.11.7. Davies-Bouldin Index#
If the ground truth labels are not known, the Davies-Bouldin index
(sklearn.metrics.davies_bouldin_score) can be used to evaluate the
model, where a lower Davies-Bouldin index relates to a model with better
separation between the clusters.
This index signifies the average ‘similarity’ between clusters, where the
similarity is a measure that compares the distance between clusters with the
size of the clusters themselves.
Zero is the lowest possible score. Values closer to zero indicate a better
partition.
In normal usage, the Davies-Bouldin index is applied to the results of a
cluster analysis as follows:
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> X = iris.data
>>> from sklearn.cluster import KMeans
>>> from sklearn.metrics import davies_bouldin_score
>>> kmeans = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans.labels_
>>> davies_bouldin_score(X, labels)
0.666...



Advantages:

The computation of Davies-Bouldin is simpler than that of Silhouette scores.
The index is solely based on quantities and features inherent to the dataset
as its computation only uses point-wise distances.



Drawbacks:

The Davies-Boulding index is generally higher for convex clusters than other
concepts of clusters, such as density based clusters like those obtained
from DBSCAN.
The usage of centroid distance limits the distance metric to Euclidean
space.




Mathematical formulation#
The index is defined as the average similarity between each cluster \(C_i\)
for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of
this index, similarity is defined as a measure \(R_{ij}\) that trades off:

\(s_i\), the average distance between each point of cluster \(i\) and
the centroid of that cluster – also know as cluster diameter.
\(d_{ij}\), the distance between cluster centroids \(i\) and
\(j\).

A simple choice to construct \(R_{ij}\) so that it is nonnegative and
symmetric is:

\[R_{ij} = \frac{s_i + s_j}{d_{ij}}\]
Then the Davies-Bouldin index is defined as:

\[DB = \frac{1}{k} \sum_{i=1}^k \max_{i \neq j} R_{ij}\]



References#

Davies, David L.; Bouldin, Donald W. (1979). “A Cluster Separation
Measure” IEEE Transactions on Pattern Analysis
and Machine Intelligence. PAMI-1 (2): 224-227.
Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). “On
Clustering Validation Techniques” Journal of
Intelligent Information Systems, 17(2-3), 107-145.
Wikipedia entry for Davies-Bouldin index.




2.3.11.8. Contingency Matrix#
Contingency matrix (sklearn.metrics.cluster.contingency_matrix)
reports the intersection cardinality for every true/predicted cluster pair.
The contingency matrix provides sufficient statistics for all clustering
metrics where the samples are independent and identically distributed and
one doesn’t need to account for some instances not being clustered.
Here is an example:
>>> from sklearn.metrics.cluster import contingency_matrix
>>> x = [""a"", ""a"", ""a"", ""b"", ""b"", ""b""]
>>> y = [0, 0, 1, 1, 2, 2]
>>> contingency_matrix(x, y)
array([[2, 1, 0],
       [0, 1, 2]])


The first row of output array indicates that there are three samples whose
true cluster is “a”. Of them, two are in predicted cluster 0, one is in 1,
and none is in 2. And the second row indicates that there are three samples
whose true cluster is “b”. Of them, none is in predicted cluster 0, one is in
1 and two are in 2.
A confusion matrix for classification is a square
contingency matrix where the order of rows and columns correspond to a list
of classes.

Advantages:

Allows to examine the spread of each true cluster across predicted clusters
and vice versa.
The contingency table calculated is typically utilized in the calculation of
a similarity statistic (like the others listed in this document) between the
two clusterings.



Drawbacks:

Contingency matrix is easy to interpret for a small number of clusters, but
becomes very hard to interpret for a large number of clusters.
It doesn’t give a single metric to use as an objective for clustering
optimisation.




References#

Wikipedia entry for contingency matrix




2.3.11.9. Pair Confusion Matrix#
The pair confusion matrix
(sklearn.metrics.cluster.pair_confusion_matrix) is a 2x2
similarity matrix

\[\begin{split}C = \left[\begin{matrix}
C_{00} & C_{01} \\
C_{10} & C_{11}
\end{matrix}\right]\end{split}\]
between two clusterings computed by considering all pairs of samples and
counting pairs that are assigned into the same or into different clusters
under the true and predicted clusterings.
It has the following entries:
\(C_{00}\) : number of pairs with both clusterings having the samples
not clustered together
\(C_{10}\) : number of pairs with the true label clustering having the
samples clustered together but the other clustering not having the samples
clustered together
\(C_{01}\) : number of pairs with the true label clustering not having
the samples clustered together but the other clustering having the samples
clustered together
\(C_{11}\) : number of pairs with both clusterings having the samples
clustered together
Considering a pair of samples that is clustered together a positive pair,
then as in binary classification the count of true negatives is
\(C_{00}\), false negatives is \(C_{10}\), true positives is
\(C_{11}\) and false positives is \(C_{01}\).
Perfectly matching labelings have all non-zero entries on the
diagonal regardless of actual label values:
>>> from sklearn.metrics.cluster import pair_confusion_matrix
>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])
array([[8, 0],
       [0, 4]])


>>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])
array([[8, 0],
       [0, 4]])


Labelings that assign all classes members to the same clusters
are complete but may not always be pure, hence penalized, and
have some off-diagonal non-zero entries:
>>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])
array([[8, 2],
       [0, 2]])


The matrix is not symmetric:
>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])
array([[8, 0],
       [2, 2]])


If classes members are completely split across different clusters, the
assignment is totally incomplete, hence the matrix has all zero
diagonal entries:
>>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])
array([[ 0,  0],
       [12,  0]])




References#

“Comparing Partitions” L. Hubert and P. Arabie,
Journal of Classification 1985













previous
2.2. Manifold learning




next
2.4. Biclustering










 On this page
  


2.3.1. Overview of clustering methods
2.3.2. K-means
2.3.2.1. Low-level parallelism
2.3.2.2. Mini Batch K-Means


2.3.3. Affinity Propagation
2.3.4. Mean Shift
2.3.5. Spectral clustering
2.3.5.1. Different label assignment strategies
2.3.5.2. Spectral Clustering Graphs


2.3.6. Hierarchical clustering
2.3.6.1. Different linkage type: Ward, complete, average, and single linkage
2.3.6.2. Visualization of cluster hierarchy
2.3.6.3. Adding connectivity constraints
2.3.6.4. Varying the metric
2.3.6.5. Bisecting K-Means


2.3.7. DBSCAN
2.3.8. HDBSCAN
2.3.8.1. Mutual Reachability Graph
2.3.8.2. Hierarchical Clustering


2.3.9. OPTICS
2.3.10. BIRCH
2.3.11. Clustering performance evaluation
2.3.11.1. Rand index
2.3.11.2. Mutual Information based scores
2.3.11.3. Homogeneity, completeness and V-measure
2.3.11.4. Fowlkes-Mallows scores
2.3.11.5. Silhouette Coefficient
2.3.11.6. Calinski-Harabasz Index
2.3.11.7. Davies-Bouldin Index
2.3.11.8. Contingency Matrix
2.3.11.9. Pair Confusion Matrix







 Show Source
    

















    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      






",NUM,K-Means Clustering,K-Means Clustering.
Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) (version 0.8.14),https://hdbscan.readthedocs.io/en/0.8.14/,Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) (version 0,HDBSCAN_CL,Clustering,"


  



The hdbscan Clustering Library — hdbscan 0.8.1 documentation





















 hdbscan
          

          
          

                0.8.14
              










Basic Usage of HDBSCAN* for Clustering
Getting More Information About a Clustering
Parameter Selection for HDBSCAN*
Outlier Detection
Predicting clusters for new points
Soft Clustering for HDBSCAN*
Frequently Asked Questions


How HDBSCAN Works
Comparing Python Clustering Algorithms
Benchmarking Performance and Scaling of Python Clustering Algorithms
How Soft Clustering for HDBSCAN Works


API Reference







hdbscan





Docs »
The hdbscan Clustering Library

 Edit on GitHub







The hdbscan Clustering Library¶
The hdbscan library is a suite of tools to use unsupervised learning to find clusters, or
dense regions, of a dataset. The primary algorithm is HDBSCAN* as proposed by Campello,
Moulavi, and Sander. The library provides a high performance implementation of this algorithm,
along with tools for analysing the resulting clustering.

User Guide / Tutorial¶


Basic Usage of HDBSCAN* for Clustering
The Simple Case
What about different metrics?
Distance matrices


Getting More Information About a Clustering
Condensed Trees
Single Linkage Trees


Parameter Selection for HDBSCAN*
Selecting min_cluster_size
Selecting min_samples
Selecting alpha
Leaf clustering
Allowing a single cluster


Outlier Detection
Predicting clusters for new points
Soft Clustering for HDBSCAN*
Frequently Asked Questions
Q: Most of data is classified as noise; why?
Q: I mostly just get one large cluster; I want smaller clusters.
Q: HDBSCAN is failing to separate the clusters I think it should.
Q: I am not getting the claimed performance. Why not?
Q: I want to predict the cluster of a new unseen point. How do I do this?
Q: Haversine metric is not clustering my Lat-Lon data correctly.
Q: I want to cite this software in my journal publication. How do I do that?






Background on Clustering with HDBSCAN¶


How HDBSCAN Works
Transform the space
Build the minimum spanning tree
Build the cluster hierarchy
Condense the cluster tree
Extract the clusters


Comparing Python Clustering Algorithms
Some rules for EDA clustering
Getting set up
Testing Clustering Algorithms
K-Means
Affinity Propagation
Mean Shift
Spectral Clustering
Agglomerative Clustering
DBSCAN
HDBSCAN


Benchmarking Performance and Scaling of Python Clustering Algorithms
Comparison of all ten implementations
Comparison of fast implementations
Comparison of high performance implementations
But should I get a coffee?
Conclusions


How Soft Clustering for HDBSCAN Works
What is Soft Clustering?
Soft Clustering for HDBSCAN
Distance Based Membership
Outlier Based Membership
The Middle Way
Converting a Conditional Probability






API Reference¶


API Reference
HDBSCAN
RobustSingleLinkage
Utilities







Indices and tables¶

Index
Module Index
Search Page






Next 




        © Copyright 2016, Leland McInnes, John Healy, Steve Astels.
      
        
          Revision 54e8a19f.
        


  Built with Sphinx using a theme provided by Read the Docs. 















",NUM,Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN),Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) (version 0.8.14)
Fit High Cardinality and Text first. A ElasticNet Regressor is trained on high cardinality features and text features. Raw predictions are used as offset of the main model,no url,Fit High Cardinality and Text first,XLF_ENETCD,Calibration,no documentation retrieved,NUM,Fit High Cardinality Categorical and Text Regressor,Fit High Cardinality and Text first. A ElasticNet Regressor is trained on high cardinality features and text features. Raw predictions are used as offset of the main model
Calibrate predictions using Generalized Linear Model to scale them.,no url,Calibrate predictions using Generalized Linear Model to scale them,CALIB2,Calibration,no documentation retrieved,NUM,Calibrate predictions,Calibrate predictions using Generalized Linear Model to scale them.
Fit High Cardinality and Text first. A ElasticNet Regressor is trained on high cardinality features and text features. Raw predictions are used as offset of the main model,no url,Fit High Cardinality and Text first,XLF_LENETCD,Calibration,no documentation retrieved,NUM,Fit High Cardinality and Text Classifier,Fit High Cardinality and Text first. A ElasticNet Regressor is trained on high cardinality features and text features. Raw predictions are used as offset of the main model
Calibrate predictions using Generalized Linear Model to scale them.,no url,Calibrate predictions using Generalized Linear Model to scale them,CALIB,Calibration,no documentation retrieved,NUM,Calibrate predictions,Calibrate predictions using Generalized Linear Model to scale them.
Calibrate predictions using Random Forest Classifier.,no url,Calibrate predictions using Random Forest Classifier,CALIB_V2_RFC,Calibration,no documentation retrieved,NUM,Calibrate predictions with RF,Calibrate predictions using Random Forest Classifier.
Calibration of probabilities via Platt's method,no url,Calibration of probabilities via Platt's method,PLACAL2,Calibration,no documentation retrieved,NUM,Calibrate predictions: Platt,Calibration of probabilities via Platt's method
Calibration of probabilities via weighted scaling,no url,Calibration of probabilities via weighted scaling,SWCAL,Calibration,no documentation retrieved,NUM,Calibrate predictions: Weighted Calibration,Calibration of probabilities via weighted scaling
Selects a single column by name,no url,Selects a single column by name,SCPICK,Column Selection,no documentation retrieved,NUM,Single Column Converter,Selects a single column by name
Selects a single column by name,no url,Selects a single column by name,SCTXT2,Column Selection,no documentation retrieved,NUM,Converter for Text Mining,Selects a single column by name
Selects a single column by name,no url,Selects a single column by name,SCBAGOFCAT,Column Selection,no documentation retrieved,NUM,Single Column Converter for Summarized Categorical,Selects a single column by name
Selects a single column by name,no url,Selects a single column by name,SCPICK2,Column Selection,no documentation retrieved,NUM,Single Column Converter,Selects a single column by name
Selects a single column by name,no url,Selects a single column by name,SCTXT4,Column Selection,no documentation retrieved,NUM,Converter for Text Mining,Selects a single column by name
"Multiple Column Selector
    
    *Note: The terms ""Column"" and ""feature"" are used interchangeably to specify a feature specific
    column (i.e., feature) passed to the MCPICK task.*
    
    This task selects specific features in a dataset such that downstream transformations are only
    applied to a subset of columns. To use this task, add it directly after a data type (for example,
    directly after ""Categorical Variables""), then use the task's parameters to specify which features
    should or should not be passed to the next task.
    
    **Example:**
    
    Suppose an MCPICK task takes ""Categorical Variables"" as an input. If the model's feature list
    contains four categorical variables--*a*, *b*, *c*, and *d*--then MCPICK will receive only
    these four features from the dataset as input.
    
    If the MCPICK task is configured with parameters *method* = *include* and *column_names* = *list(
    a, c, f)*, where column *f* is present in the dataset, but not in the model's feature list, the task
    following MCPICK will only receive features *a* and *c*.
    
    Parameters
    ----------
    column_names (cns): list of strings (default='list()')
        A list of column names to include or exclude, taken from the features in the dataset of the
        project under which the MCPICK is used.
    
    method (method): string (default='include')
        Method of feature selection, either 'include' or 'exclude'.
    
        If 'include', features listed in column_names and included in the model's feature list will be
        passed to the next task. If 'exclude', features listed in column_names will be excluded,
        while other features will be passed to the next task.
    
    References
    ----------
    
    See Also
    --------
    ",no url,Multiple Column Selector,MCPICK,Column Selection,no documentation retrieved,NUM,Multiple Column Selector,Selects multiple columns by names
Extracts features for trend modeling,no url,Extracts features for trend modeling,TRENDFEATS,Other,no documentation retrieved,NUM,Extract Trend Features,Extracts features for trend modeling
"Bind data processing workflows within a type of data.  For example, bind 2 kinds of numeric preprocessing or 2 kinds of text preprocessing",no url,Bind data processing workflows within a type of data,BIND,Other,no documentation retrieved,NUM,Bind branches,"Bind data processing workflows within a type of data.  For example, bind 2 kinds of numeric preprocessing or 2 kinds of text preprocessing"
Feature Selection For Dimensionality Reduction,no url,Feature Selection For Dimensionality Reduction,FS_RFRDR_LASSO,Numeric Preprocessing,no documentation retrieved,NUM,Feature Selection with Lasso Pre-selection (regression),Feature Selection For Dimensionality Reduction
Feature Selection For Dimensionality Reduction,no url,Feature Selection For Dimensionality Reduction,FS_RFRDR2,Numeric Preprocessing,no documentation retrieved,NUM,Feature Selection For Dimensionality Reduction,Feature Selection For Dimensionality Reduction
Feature Selection using L1 Regularization,no url,Feature Selection using L1 Regularization,FS_XL_LASSO2,Numeric Preprocessing,no documentation retrieved,NUM,Feature Selection using L1 Regularization and Predictions as Offset,Feature Selection using L1 Regularization
Rare feature masking. Removes infrequence columns from sparse array,null},Rare feature masking,RFMASK,Numeric Preprocessing,,NUM,Rare Feature Masking,Rare feature masking. Removes infrequence columns from sparse array
Feature Selection For Dimensionality Reduction,no url,Feature Selection For Dimensionality Reduction,FS_RFCDR2,Numeric Preprocessing,no documentation retrieved,NUM,Feature Selection For Dimensionality Reduction,Feature Selection For Dimensionality Reduction
Feature Selection for Ratios/Differences,no url,Feature Selection for Ratios/Differences,FS_RFC2,Numeric Preprocessing,no documentation retrieved,NUM,Feature Selection for Ratios/Differences,Feature Selection for Ratios/Differences
Feature Selection For Dimensionality Reduction,no url,Feature Selection For Dimensionality Reduction,FS_RFCDR_LASSO,Numeric Preprocessing,no documentation retrieved,NUM,Feature Selection with Lasso Pre-selection (classification),Feature Selection For Dimensionality Reduction
Feature Selection for Ratios/Differences,no url,Feature Selection for Ratios/Differences,FS_RFR2,Numeric Preprocessing,no documentation retrieved,NUM,Feature Selection for Ratios/Differences,Feature Selection for Ratios/Differences
